# Resume â€“ Samar (MLOps Engineer)

## Details
- **Name**: Samar  
- **Target Role**: MLOps Engineer / GenAI Operations Engineer  
- **Experience**: 3 years  
- **Primary Focus**: ML Pipelines, Model Deployment, Monitoring, CI/CD for ML  

## Career Objective
MLOps Engineer with 3 years of experience in building ML pipelines, deploying models, and monitoring them in production. Focused on scaling ML and GenAI systems reliably by combining strong CI/CD practices, observability, and agentic AI-driven automation. Seeking to own the end-to-end ML operations lifecycle for high-impact AI products.

## Roles & Responsibilities
1. Designed and implemented ML pipelines from data ingestion to model training, evaluation, and deployment.
2. Managed model packaging and deployment as APIs, batch jobs, or streaming components.
3. Built CI/CD workflows that run tests, validate data, and automatically promote models based on performance criteria.
4. Set up monitoring for model performance (accuracy, drift, fairness) and system metrics (latency, throughput, errors).
5. Integrated GenAI workflows into existing ML stacks, including RAG components and LLM-based microservices.
6. Implemented agentic AI mechanisms to autonomously trigger model retraining, rollback, or human review under specific conditions.
7. Collaborated closely with data scientists to design production-ready feature pipelines and deployment strategies.
8. Managed model versioning, governance, and documentation in model registries.
9. Participated in incident response, RCA, and continuous improvement for ML/GenAI systems.
10. Advocated for best practices in reproducibility, testing, and security within MLOps workflows.

## Skills
- **MLOps & DevOps**: CI/CD (GitHub Actions/Jenkins), Docker, Kubernetes, model registries (MLflow/SageMaker/Vertex), feature stores
- **Data & ML**: Python, SQL, Spark (optional), training pipelines, evaluation, A/B testing
- **GenAI & Agentic AI**: LLM integration, RAG, LLMOps, evaluation pipelines, agents for retraining/drift response
- **Observability**: Monitoring dashboards, logs, alerts, SLO/SLA design
- **Other**: Git, documentation, cross-functional collaboration

## Projects

### Project 1: Production ML Pipeline for Demand Forecasting
- **Title**: Production ML Pipeline for Demand Forecasting
- **Description**: Built and operated an automated ML pipeline to forecast product demand across regions and channels.
- **Skills**: Python, CI/CD, feature store, MLflow, Kubernetes
- **Roles & Responsibilities**:
  1. Implemented data ingestion and feature engineering jobs with robust validation checks.
  2. Automated training runs, model selection, and registration based on performance metrics.
  3. Deployed top-performing models to production APIs and batch scoring jobs.
  4. Monitored forecast accuracy and resource usage, optimizing where needed.

### Project 2: GenAI Content Generation & Review Flow
- **Title**: GenAI Content Generation & Review Flow
- **Description**: Operationalized GenAI models for generating marketing content and automated quality checks.
- **Skills**: LLMs, prompt engineering, CI/CD, dashboards, human-in-the-loop tools
- **Roles & Responsibilities**:
  1. Set up environments and pipelines for prompt templates, A/B testing of models, and safety filters.
  2. Integrated human review steps where content above certain risk thresholds required approval.
  3. Collected feedback and metrics to improve prompts and model configurations.
  4. Ensured compliance with brand and regulatory guidelines.

### Project 3: Agentic Model Drift Response System
- **Title**: Agentic Model Drift Response System
- **Description**: Created an agent-based system that monitors model drift and orchestrates retraining or fallback behavior.
- **Skills**: Python, monitoring APIs, scheduling, LLM/agent frameworks
- **Roles & Responsibilities**:
  1. Defined drift indicators and thresholds for key models (distribution shift, performance drop).
  2. Implemented agents that check indicators, decide on actions (retrain, rollback, alert), and execute workflows.
  3. Logged decisions and outcomes for auditability and improvement.
  4. Reduced mean time to detection and resolution for model performance issues.
