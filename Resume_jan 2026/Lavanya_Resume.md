# Resume – Lavanya (Data Engineer → Data Scientist / GenAI)

## Details
- **Name**: Lavanya  
- **Target Role**: Data Scientist / Data Engineer (ETL & GenAI)  
- **Experience**: Not specified (hands-on in data engineering and analytics)  
- **Primary Stack**: SQL, Python, Data Warehousing, ETL  

## Career Objective
Data Engineer skilled in SQL, Python, and data warehousing, seeking to evolve into a Data Scientist role with a focus on GenAI applications. Aiming to leverage strong ETL and modeling capabilities to build robust datasets, predictive analytics, and AI agents that streamline business decision-making.

## Roles & Responsibilities
1. Designed and implemented ETL workflows to consolidate data from transactional systems into data warehouses.
2. Created well-modeled tables and views (star/snowflake schemas) to support reporting and analytics.
3. Wrote optimized SQL queries for aggregations, cohort analyses, and experimentation evaluation.
4. Developed Python scripts to perform data cleaning, transformation, and automated checks.
5. Partnered with analysts and business stakeholders to understand data requirements and deliver reliable datasets.
6. Assisted in building ML-ready datasets and baseline models for key use cases (churn, segmentation, demand prediction).
7. Explored GenAI techniques to automatically document data models and generate data dictionaries from schemas and samples.
8. Implemented simple agentic workflows to schedule and monitor pipeline tasks, notifying owners on failures/anomalies.
9. Maintained version control for SQL and Python code, following code review practices.
10. Documented pipelines, tables, and key business metrics in shared knowledge bases.

## Skills
- **Data Engineering**: ETL/ELT, data warehousing, dimensional modeling, SQL performance tuning
- **Programming**: Python (Pandas, data utilities), shell scripting
- **Data Science**: EDA, feature engineering, basic ML models in scikit-learn
- **GenAI & Agentic AI**: LLM-based documentation tools, schema understanding, agents for pipeline monitoring & notifications
- **Other**: Git, BI tool collaboration (Power BI/Tableau basics), Agile teamwork

## Projects

### Project 1: Sales & Revenue Data Mart
- **Title**: Sales & Revenue Data Mart
- **Description**: Built a sales and revenue data mart serving finance and business analytics teams.
- **Skills**: SQL, ETL, dimensional modeling, data warehousing
- **Roles & Responsibilities**:
  1. Designed fact and dimension tables for orders, invoices, customers, and products.
  2. Implemented incremental ETL jobs to keep data fresh while minimizing load on source systems.
  3. Validated metrics against finance reports to ensure consistency and trust.
  4. Provided documentation and usage examples for analysts and self-service users.

### Project 2: GenAI Data Documentation Assistant
- **Title**: GenAI Data Documentation Assistant
- **Description**: Created a GenAI-based tool that automatically generates human-readable documentation for new tables and columns.
- **Skills**: Python, LLM APIs, SQL introspection, prompt engineering
- **Roles & Responsibilities**:
  1. Implemented scripts that inspect schemas, sample values, and references to infer column meanings.
  2. Fed inferred information into an LLM to generate descriptions, example queries, and usage notes.
  3. Integrated output into existing documentation portals and kept docs updated with schema changes.
  4. Collected user feedback to refine the assistant's descriptions and ensure accuracy.

### Project 3: Agentic Pipeline Monitor
- **Title**: Agentic Pipeline Monitor
- **Description**: Built an agent that monitors ETL jobs and automatically summarizes their status, issues, and impacts for stakeholders.
- **Skills**: Python, scheduling, monitoring APIs, LLM summarization
- **Roles & Responsibilities**:
  1. Aggregated job logs and metrics from ETL tools into a central status table.
  2. Used an LLM to generate human-readable summaries of daily pipeline health and notable incidents.
  3. Sent summaries to relevant channels (email/Slack) with links to detailed dashboards.
  4. Iterated on rules and prompts based on stakeholder feedback to improve clarity and usefulness.
