# Resume – Deekshith (Data Engineer → Data Scientist / GenAI)

## Details
- **Name**: Deekshith  
- **Target Role**: Data Scientist / Senior Data Engineer (Azure & GenAI)  
- **Experience**: 3+ years  
- **Primary Stack**: Azure Data Engineering, SQL, PySpark, Data Pipelines  

## Career Objective
Azure-focused Data Engineer with 3+ years of experience building scalable data pipelines and analytics platforms, transitioning into a Data Scientist role with emphasis on GenAI and intelligent analytics. Seeking to leverage strong SQL/PySpark skills and cloud experience to design data products, predictive models, and agentic AI systems that unlock value from large datasets.

## Roles & Responsibilities (Data Science–Oriented)
1. Designed and implemented end-to-end data pipelines on Azure (ADF, Databricks, Data Lake) to serve analytics and ML workloads.
2. Performed in-depth data profiling, cleansing, and transformation to create high-quality feature stores and analytical models.
3. Collaborated with data scientists to define features, build training datasets, and schedule model training/inference jobs.
4. Implemented business logic in PySpark/SQL for segmentation, scoring, and KPI computation.
5. Optimized data processing jobs for performance and cost, tuning cluster configurations and job parallelism.
6. Applied statistical techniques and basic ML algorithms to build POCs for forecasting, churn prediction, and anomaly detection.
7. Built GenAI-enhanced tools that allow users to explore data via natural language questions and receive LLM-generated insights.
8. Designed agentic AI workflows that automatically trigger data refreshes, re-train models, or notify owners when data quality or KPI thresholds are breached.
9. Established data quality frameworks, including validation rules, reconciliation checks, and issue tracking.
10. Created technical documentation, data dictionaries, and onboarding material for analytics and AI users.

## Skills
- **Programming**: Python (Pandas, PySpark), SQL, Scala (basic)
- **Data Engineering**: Azure Data Factory, Azure Databricks, Delta Lake, Data Lake Storage, ETL/ELT
- **Data Science & Analytics**: Feature engineering, exploratory data analysis, supervised learning (regression/classification), time-series forecasting
- **GenAI & Agentic AI**: LLM APIs, RAG on enterprise data, semantic search, prompt design, event-driven agents for data/ML operations
- **Other**: Git, CI/CD basics, monitoring, stakeholder communication, Agile

## Projects

### Project 1: Marketing Attribution & Uplift Modeling Platform
- **Title**: Marketing Attribution & Uplift Modeling Platform
- **Description**: Built a data platform to unify marketing channel data and support uplift modeling for campaign optimization.
- **Skills**: Azure Data Factory, Databricks (PySpark), SQL, ML (uplift modeling), visualization
- **Roles & Responsibilities**:
  1. Ingested data from ad platforms, CRM, and web analytics into a centralized data lake with incremental loads.
  2. Engineered features representing user touchpoints and engagement across channels.
  3. Collaborated with data scientists to train uplift models and publish uplift scores for targeting.
  4. Exposed results through curated tables and dashboards, supporting A/B test analysis.

### Project 2: GenAI-Driven Data Exploration Assistant
- **Title**: GenAI-Driven Data Exploration Assistant
- **Description**: Implemented a GenAI assistant on top of Azure data assets to help analysts and business users query data via natural language.
- **Skills**: Python, LLM APIs, Azure Databricks, SQL, embeddings, RAG
- **Roles & Responsibilities**:
  1. Built metadata catalogs and semantic layers that map business terms to technical tables/columns.
  2. Developed a query generation module that converts user questions into validated SQL queries over governed datasets.
  3. Used RAG to ground LLM responses in actual data and documentation, returning charts and narratives.
  4. Monitored usage and refined prompts based on feedback and error analysis.

### Project 3: Agentic AI for Data Reliability
- **Title**: Agentic AI for Data Reliability
- **Description**: Created agent workflows that monitor data pipelines and proactively handle failures or anomalies.
- **Skills**: Python, Azure Functions/Logic Apps, monitoring APIs, anomaly detection, agent patterns
- **Roles & Responsibilities**:
  1. Implemented anomaly detection on critical KPIs and data volumes to detect issues early.
  2. Designed agents that propose root causes (e.g., schema change, source delay) and suggest remediation steps.
  3. Automated creation of incident tickets and notifications to responsible teams, including attachments with diagnostic logs.
  4. Tracked incident metrics to continuously improve pipeline robustness.
