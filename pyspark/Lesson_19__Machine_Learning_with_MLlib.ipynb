{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c3878c",
   "metadata": {},
   "source": [
    "# Lesson 19 - Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99590d",
   "metadata": {},
   "source": [
    "Okay, here are the detailed technical notes on PySpark's MLlib library, focusing on the requested areas and adhering to the specified format and tone.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark MLlib: Scalable Machine Learning\n",
    "\n",
    "Apache Spark's MLlib is a powerful, scalable machine learning library designed to run in distributed environments. It integrates seamlessly with Spark's core data processing capabilities (DataFrames), allowing ML pipelines to leverage Spark's speed and fault tolerance. MLlib provides implementations of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs.\n",
    "\n",
    "We will focus on the `spark.ml` package, which is the primary API built around DataFrames, offering a higher-level, more user-friendly interface compared to the older RDD-based `spark.mllib` package.\n",
    "\n",
    "**Prerequisites:** A running SparkSession instance.\n",
    "\n",
    "```python\n",
    "# Standard PySpark Session Initialization\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkMLlibNotes\") \\\n",
    "    .master(\"local[*]\") # Use local machine with all available cores\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session Initialized. Spark version: {spark.version}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Engineering with VectorAssembler\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Machine learning algorithms typically operate on numerical data, often represented as vectors. Raw datasets, however, usually contain features spread across multiple columns, potentially including categorical or text data alongside numerical values. Feature engineering is the crucial process of transforming raw data into a format suitable for ML algorithms.\n",
    "\n",
    "`VectorAssembler` is a fundamental *Transformer* in `spark.ml`. Its purpose is to combine a given list of columns into a single vector column. This vector column is typically named \"features\" by convention and serves as the input for most MLlib estimators (models).\n",
    "\n",
    "-   **Input:** A DataFrame with multiple numerical columns (or columns that have already been converted to numerical representations, e.g., through one-hot encoding or string indexing).\n",
    "-   **Output:** A new DataFrame with an additional column containing `Vector` objects (usually `DenseVector` or `SparseVector` depending on the data).\n",
    "-   **Key Parameters:**\n",
    "    -   `inputCols`: A list of column names to be combined.\n",
    "    -   `outputCol`: The name of the new vector column to be created.\n",
    "    -   `handleInvalid`: How to handle invalid data (e.g., Null values). Options include 'error' (default), 'skip', or 'keep'.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors # Used for creating sample vectors if needed\n",
    "\n",
    "# 1. Create Sample Data\n",
    "# Imagine data with age, income (in thousands), and years of experience\n",
    "data = [(1, 35, 60.0, 10),\n",
    "        (2, 42, 85.5, 15),\n",
    "        (3, 28, 45.0, 5),\n",
    "        (4, 55, 120.0, 25)]\n",
    "columns = [\"id\", \"age\", \"income_k\", \"experience_years\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# 2. Instantiate VectorAssembler\n",
    "# We want to combine 'age', 'income_k', and 'experience_years' into a single feature vector\n",
    "feature_columns = [\"age\", \"income_k\", \"experience_years\"]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\" # Standard output column name\n",
    ")\n",
    "\n",
    "# 3. Transform the DataFrame\n",
    "# VectorAssembler is a Transformer, so we use the .transform() method\n",
    "output_df = assembler.transform(df)\n",
    "\n",
    "print(\"DataFrame after VectorAssembler:\")\n",
    "output_df.show(truncate=False)\n",
    "\n",
    "# 4. Inspect the output column type\n",
    "print(\"Schema of the transformed DataFrame:\")\n",
    "output_df.printSchema()\n",
    "# Note the 'features' column type: vector\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Create Sample Data:** We create a sample PySpark DataFrame `df` with columns representing potential features (`age`, `income_k`, `experience_years`) and an identifier (`id`).\n",
    "2.  **Instantiate VectorAssembler:** An instance of `VectorAssembler` is created.\n",
    "    *   `inputCols`: Specifies the list of columns (`feature_columns`) that we want to merge into a vector.\n",
    "    *   `outputCol`: Defines the name of the new column (\"features\") that will hold the resulting vectors.\n",
    "3.  **Transform the DataFrame:** The `transform()` method is called on the `assembler` instance, passing the original DataFrame `df`. This applies the assembly logic and returns a new DataFrame `output_df` containing the original columns plus the new \"features\" column.\n",
    "4.  **Inspect the output:** We display the transformed DataFrame. Notice the new \"features\" column contains vector representations (e.g., `[35.0, 60.0, 10.0]`). We also print the schema to confirm the data type of the \"features\" column is `vector`.\n",
    "\n",
    "**Practical Use Case:** `VectorAssembler` is almost always used as one of the first steps in an ML pipeline to prepare the feature set required by downstream learning algorithms. It standardizes the input format for models.\n",
    "\n",
    "---\n",
    "\n",
    "### Classification & Regression Models\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Supervised learning involves training a model on labeled data, where both the input features and the corresponding correct output (label) are provided. The goal is to learn a mapping function that can predict the output for new, unseen input features.\n",
    "\n",
    "-   **Classification:** Predicts a discrete category or class label.\n",
    "    -   Examples: Spam detection (spam/not spam), image recognition (cat/dog/bird), customer churn prediction (churn/no churn).\n",
    "    -   Common Algorithms: Logistic Regression, Decision Trees, Random Forests, Gradient Boosted Trees, Naive Bayes, Support Vector Machines (SVM).\n",
    "-   **Regression:** Predicts a continuous numerical value.\n",
    "    -   Examples: Predicting house prices, forecasting sales, estimating temperature.\n",
    "    *   Common Algorithms: Linear Regression, Generalized Linear Regression, Decision Trees, Random Forests, Gradient Boosted Trees.\n",
    "\n",
    "In `spark.ml`, both classification and regression models are typically *Estimators*. An Estimator implements a `fit()` method, which takes a DataFrame (containing features and labels) and learns the model parameters, returning a trained *Model* (which is a *Transformer*). The trained Model then has a `transform()` method to make predictions on new data.\n",
    "\n",
    "-   **Key Parameters (Common):**\n",
    "    -   `featuresCol`: Name of the input column containing feature vectors (usually \"features\").\n",
    "    -   `labelCol`: Name of the input column containing the true labels.\n",
    "    -   `predictionCol`: Name of the output column where predictions will be stored (usually \"prediction\").\n",
    "\n",
    "**Classification Example: Logistic Regression**\n",
    "\n",
    "Logistic Regression is a widely used algorithm for binary classification problems (predicting one of two outcomes). Despite its name, it's a classification algorithm that models the probability of the default class.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# 1. Prepare Labeled Data (using the previous 'output_df' and adding a label)\n",
    "# Let's create a binary label (e.g., 1 if income > 70k, else 0)\n",
    "# NOTE: This is a synthetic example for demonstration. Real labels come from the data source.\n",
    "labeled_df = output_df.withColumn(\"label\", (output_df[\"income_k\"] > 70).cast(\"double\"))\n",
    "print(\"DataFrame with Features and Labels:\")\n",
    "labeled_df.select(\"features\", \"label\").show(truncate=False)\n",
    "\n",
    "# 2. Split Data into Training and Test Sets\n",
    "# Use a random split (e.g., 80% training, 20% testing)\n",
    "(trainingData, testData) = labeled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training data count: {trainingData.count()}\")\n",
    "print(f\"Test data count: {testData.count()}\")\n",
    "\n",
    "# 3. Instantiate the Estimator (Logistic Regression)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# 4. Train the Model (Fit the Estimator)\n",
    "# The fit() method takes the training data and returns a trained LogisticRegressionModel\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print learned coefficients and intercept (Optional)\n",
    "print(f\"Coefficients: {lrModel.coefficients}\")\n",
    "print(f\"Intercept: {lrModel.intercept}\")\n",
    "\n",
    "# 5. Make Predictions on Test Data (Transform)\n",
    "# The trained model (lrModel) is a Transformer\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "print(\"Predictions on Test Data:\")\n",
    "# Show features, label, raw prediction, probability, and final prediction\n",
    "predictions.select(\"features\", \"label\", \"rawPrediction\", \"probability\", \"prediction\").show(truncate=False)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "# Use BinaryClassificationEvaluator for binary tasks\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area Under ROC (AUC) on Test Data: {auc}\")\n",
    "# Other metrics like 'areaUnderPR' are also available.\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Prepare Labeled Data:** We take the `output_df` (which already has the \"features\" vector column) and add a \"label\" column. Here, we derive it synthetically based on income for demonstration. In real scenarios, labels are part of the input dataset. The label column must be numerical (usually double type).\n",
    "2.  **Split Data:** The labeled data is split into `trainingData` and `testData` using `randomSplit`. This is crucial to evaluate the model's performance on unseen data. A `seed` ensures reproducibility.\n",
    "3.  **Instantiate Estimator:** An instance of `LogisticRegression` is created, specifying the names of the feature and label columns.\n",
    "4.  **Train Model:** The `fit()` method is called on the `lr` estimator with the `trainingData`. Spark executes the training algorithm distributedly, learning the model parameters (coefficients and intercept). The result is a `LogisticRegressionModel` (`lrModel`).\n",
    "5.  **Make Predictions:** The `transform()` method of the *trained model* (`lrModel`) is used on the `testData`. This applies the learned model to generate predictions. The output DataFrame `predictions` includes new columns like `rawPrediction` (logit value), `probability` (probability of each class, usually a vector `[prob_0, prob_1]`), and `prediction` (the final predicted class label, 0.0 or 1.0).\n",
    "6.  **Evaluate Model:** A `BinaryClassificationEvaluator` is used to assess the model's quality. We configure it with the label and prediction columns (using `rawPrediction` often gives more stable AUC results) and specify the metric (`areaUnderROC`). The `evaluate()` method calculates the AUC score on the `predictions` DataFrame.\n",
    "\n",
    "**Regression Example: Linear Regression**\n",
    "\n",
    "Linear Regression predicts a continuous value based on a linear combination of input features.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Prepare Labeled Data (Assume we want to predict 'income_k')\n",
    "# We'll use 'age' and 'experience_years' as features to predict 'income_k'\n",
    "# Re-assemble features without income\n",
    "assembler_reg = VectorAssembler(inputCols=[\"age\", \"experience_years\"], outputCol=\"reg_features\")\n",
    "df_reg_input = assembler_reg.transform(df)\n",
    "\n",
    "# Select features and the target variable as the label\n",
    "labeled_df_reg = df_reg_input.selectExpr(\"reg_features as features\", \"income_k as label\")\n",
    "print(\"DataFrame for Regression:\")\n",
    "labeled_df_reg.show(truncate=False)\n",
    "\n",
    "# 2. Split Data\n",
    "(trainingData_reg, testData_reg) = labeled_df_reg.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. Instantiate the Estimator (Linear Regression)\n",
    "lin_reg = LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"predicted_income\")\n",
    "\n",
    "# 4. Train the Model\n",
    "linRegModel = lin_reg.fit(trainingData_reg)\n",
    "\n",
    "# Print learned coefficients and intercept (Optional)\n",
    "print(f\"Coefficients: {linRegModel.coefficients}\")\n",
    "print(f\"Intercept: {linRegModel.intercept}\")\n",
    "# Print some summary statistics of the training (Optional)\n",
    "trainingSummary = linRegModel.summary\n",
    "print(f\"RMSE on training data: {trainingSummary.rootMeanSquaredError}\")\n",
    "print(f\"R^2 on training data: {trainingSummary.r2}\")\n",
    "\n",
    "# 5. Make Predictions on Test Data\n",
    "predictions_reg = linRegModel.transform(testData_reg)\n",
    "\n",
    "print(\"Regression Predictions on Test Data:\")\n",
    "predictions_reg.select(\"features\", \"label\", \"predicted_income\").show(truncate=False)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "# Use RegressionEvaluator for regression tasks\n",
    "evaluator_reg = RegressionEvaluator(labelCol=\"label\", predictionCol=\"predicted_income\", metricName=\"rmse\") # Other metrics: \"mse\", \"r2\", \"mae\"\n",
    "rmse = evaluator_reg.evaluate(predictions_reg)\n",
    "print(f\"Root Mean Squared Error (RMSE) on Test Data: {rmse}\")\n",
    "\n",
    "r2 = evaluator_reg.setMetricName(\"r2\").evaluate(predictions_reg)\n",
    "print(f\"R-squared (R2) on Test Data: {r2}\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Prepare Labeled Data:** We use `VectorAssembler` again, but this time only include `age` and `experience_years` in the feature vector (`reg_features`). We then select this new feature column (renaming it to the standard \"features\") and the target variable `income_k` (renaming it to the standard \"label\").\n",
    "2.  **Split Data:** Similar to classification, we split into training and test sets.\n",
    "3.  **Instantiate Estimator:** An instance of `LinearRegression` is created, specifying feature, label, and the desired prediction column name (`predicted_income`).\n",
    "4.  **Train Model:** The `fit()` method trains the model on `trainingData_reg`. The resulting `LinearRegressionModel` (`linRegModel`) contains the learned coefficients and intercept. We can also access a `summary` object for metrics calculated on the training data.\n",
    "5.  **Make Predictions:** The `transform()` method of `linRegModel` is called on `testData_reg` to generate predicted income values in the `predicted_income` column.\n",
    "6.  **Evaluate Model:** A `RegressionEvaluator` is used. We specify the label and prediction columns and choose a metric (`rmse`, `r2`, `mae`, `mse`). We calculate and print RMSE and R-squared on the test set predictions.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "-   **Classification:** Customer churn prediction, sentiment analysis, fraud detection, medical diagnosis.\n",
    "-   **Regression:** House price prediction, demand forecasting, stock price analysis, predicting customer lifetime value.\n",
    "\n",
    "---\n",
    "\n",
    "### Clustering with KMeans\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Clustering is an unsupervised learning technique used to group data points into clusters based on their similarity. The goal is that points within the same cluster are more similar to each other than to points in other clusters. \"Unsupervised\" means the algorithm learns patterns from the data without predefined labels.\n",
    "\n",
    "KMeans is one of the most popular and simplest clustering algorithms.\n",
    "-   **Objective:** Partition *n* observations into *k* clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid).\n",
    "-   **Algorithm Steps (Iterative):**\n",
    "    1.  **Initialization:** Randomly select *k* data points as the initial centroids.\n",
    "    2.  **Assignment Step:** Assign each data point to the cluster whose centroid is the nearest (typically using Euclidean distance).\n",
    "    3.  **Update Step:** Recalculate the position of the *k* centroids based on the mean of the data points assigned to each cluster.\n",
    "    4.  **Repeat:** Repeat steps 2 and 3 until the centroids no longer move significantly or a maximum number of iterations is reached.\n",
    "-   **Key Parameters:**\n",
    "    -   `k`: The desired number of clusters (must be specified beforehand). Choosing the right `k` is often done using methods like the Elbow method or Silhouette analysis.\n",
    "    -   `featuresCol`: Name of the input column containing feature vectors.\n",
    "    -   `predictionCol`: Name of the output column where the assigned cluster index (0 to k-1) will be stored.\n",
    "    -   `seed`: For reproducible initialization.\n",
    "    -   `maxIter`: Maximum number of iterations.\n",
    "    -   `initMode`: Initialization method ('random' or 'k-means||'). 'k-means||' is generally preferred.\n",
    "\n",
    "KMeans in `spark.ml` is an *Estimator*. It `fit()`s on the data to find the cluster centroids and returns a `KMeansModel` (*Transformer*), which can then `transform()` the data to assign cluster memberships.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# 1. Prepare Data (Requires only features)\n",
    "# We'll use the 'output_df' which already has the 'features' column from VectorAssembler.\n",
    "# No labels are needed for unsupervised learning.\n",
    "data_for_clustering = output_df.select(\"id\", \"features\")\n",
    "print(\"Data for Clustering:\")\n",
    "data_for_clustering.show(truncate=False)\n",
    "\n",
    "# 2. Instantiate the Estimator (KMeans)\n",
    "# Let's assume we want to find 2 clusters (k=2)\n",
    "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster_id\", k=2, seed=42)\n",
    "\n",
    "# 3. Train the Model (Find Cluster Centroids)\n",
    "# fit() finds the centroids based on the feature vectors\n",
    "model = kmeans.fit(data_for_clustering)\n",
    "\n",
    "# 4. Assign Clusters to Data (Transform)\n",
    "# transform() adds the 'cluster_id' column to the DataFrame\n",
    "predictions_cluster = model.transform(data_for_clustering)\n",
    "\n",
    "print(\"Data with Assigned Cluster IDs:\")\n",
    "predictions_cluster.show(truncate=False)\n",
    "\n",
    "# 5. Evaluate Clustering (Optional but Recommended)\n",
    "# The Silhouette score measures how similar an object is to its own cluster\n",
    "# compared to other clusters. Ranges from -1 to 1. Higher is better.\n",
    "evaluator_cluster = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster_id\", metricName=\"silhouette\")\n",
    "silhouette = evaluator_cluster.evaluate(predictions_cluster)\n",
    "print(f\"Silhouette Score: {silhouette}\") # Note: Silhouette is computationally intensive on large datasets\n",
    "\n",
    "# 6. Get Cluster Centers (Optional)\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Center {i}: {center}\")\n",
    "\n",
    "# 7. Compute Within Set Sum of Squared Errors (WSSSE) - Used for Elbow Method\n",
    "# This is calculated on the training data by the model object\n",
    "wssse = model.summary.trainingCost\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n",
    "# You would typically run KMeans for different 'k' values and plot WSSSE vs k\n",
    "# to find the 'elbow' point, suggesting an optimal k.\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Prepare Data:** We select only the necessary columns (`id` for reference and `features`) from our previously assembled DataFrame. No labels are used.\n",
    "2.  **Instantiate Estimator:** An instance of `KMeans` is created. We must specify `k` (the number of clusters), along with `featuresCol` and `predictionCol`. A `seed` is used for reproducibility.\n",
    "3.  **Train Model:** The `fit()` method is called on the `kmeans` estimator with the feature data. This runs the iterative KMeans algorithm to determine the optimal positions of the `k` cluster centroids. It returns a `KMeansModel`.\n",
    "4.  **Assign Clusters:** The `transform()` method of the trained `model` is used on the same (or new) data. It calculates the distance from each data point to each centroid and assigns the point to the nearest cluster, adding the `cluster_id` column.\n",
    "5.  **Evaluate Clustering:** A `ClusteringEvaluator` is instantiated, configured for the `silhouette` metric. The `evaluate()` method calculates the score based on the features and the assigned cluster IDs. Higher Silhouette scores generally indicate better-defined clusters.\n",
    "6.  **Get Cluster Centers:** The `clusterCenters()` attribute of the trained `model` provides the coordinates of the final centroids found during training.\n",
    "7.  **Compute WSSSE:** The `trainingCost` attribute (accessible via `model.summary.trainingCost`) gives the WSSSE for the fitted model. This value is useful when applying the Elbow method to determine an appropriate value for `k`.\n",
    "\n",
    "**Practical Use Cases:** Customer segmentation (grouping customers with similar behavior), anomaly detection (points far from any cluster centroid), image compression (grouping similar pixel colors), grouping documents by topic.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Persistence and Pipelines\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "**Model Persistence:**\n",
    "Once a model (or any Transformer/Estimator) is trained or configured, you often need to save it for later use, such as deploying it in a production application or sharing it. `spark.ml` provides built-in methods for saving and loading MLlib objects.\n",
    "\n",
    "-   `save(path)`: Method available on Estimators, Transformers (including trained Models), and Pipelines to save their state to a distributed filesystem (like HDFS) or a local path. This typically saves metadata (parameters, stages) and potentially learned model data.\n",
    "-   `load(path)`: Static method available on the corresponding class (e.g., `LogisticRegressionModel.load(path)`, `PipelineModel.load(path)`) to load a previously saved object.\n",
    "\n",
    "**Pipelines:**\n",
    "Real-world ML workflows often involve multiple stages: data cleaning, feature extraction, feature transformation, model training, and prediction. Managing these stages individually can be cumbersome and error-prone. `spark.ml` Pipelines provide a way to chain multiple stages together into a single workflow.\n",
    "\n",
    "-   **Stage:** A Pipeline stage can be either a *Transformer* (like `VectorAssembler`, a trained model) or an *Estimator* (like `LogisticRegression`, `KMeans`).\n",
    "-   **Pipeline:** An Estimator that chains multiple stages. When a `Pipeline`'s `fit()` method is called on data:\n",
    "    -   It processes the data sequentially through the Transformer stages.\n",
    "    -   It calls `fit()` on each Estimator stage in order, transforming the data with the newly fitted model before passing it to the next stage.\n",
    "    -   It returns a `PipelineModel`.\n",
    "-   **PipelineModel:** A Transformer representing the *fitted* Pipeline. It contains all the Transformers and *fitted* Models from the original Pipeline stages. When its `transform()` method is called:\n",
    "    -   It applies all the stages (Transformers and fitted Models) in sequence to the input data.\n",
    "-   **Benefits:**\n",
    "    -   **Code Simplicity:** Encapsulates the entire workflow.\n",
    "    -   **Consistency:** Ensures the same steps are applied during training and prediction/evaluation.\n",
    "    -   **Parameter Tuning:** Allows tuning parameters across all stages simultaneously using tools like `CrossValidator` or `TrainValidationSplit`.\n",
    "    -   **Prevents Data Leakage:** When used with cross-validation, ensures that fitting transformations (like scaling parameters) happens only on the training fold within each split, preventing information from the validation fold leaking into the training process.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import os\n",
    "import shutil # For managing local directories\n",
    "\n",
    "# Define paths for saving/loading\n",
    "pipeline_path = \"/tmp/spark_pipeline_example\"\n",
    "model_path = \"/tmp/spark_logistic_model_example\"\n",
    "\n",
    "# --- Model Persistence Example (using Logistic Regression model from earlier) ---\n",
    "\n",
    "# 1. Save the trained Logistic Regression Model\n",
    "# First, remove the directory if it exists (for local filesystem)\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "print(f\"Saving LogisticRegressionModel to: {model_path}\")\n",
    "lrModel.save(model_path)\n",
    "\n",
    "# 2. Load the saved model\n",
    "print(f\"Loading LogisticRegressionModel from: {model_path}\")\n",
    "loadedLrModel = LogisticRegressionModel.load(model_path)\n",
    "\n",
    "# 3. Verify loaded model works (make predictions)\n",
    "print(\"Predictions using loaded LogisticRegressionModel:\")\n",
    "loaded_predictions = loadedLrModel.transform(testData)\n",
    "loaded_predictions.select(\"features\", \"label\", \"prediction\").show(truncate=False, n=5)\n",
    "\n",
    "# --- Pipeline Example (Combining VectorAssembler and LogisticRegression) ---\n",
    "\n",
    "# 1. Define Pipeline Stages\n",
    "# Use the original DataFrame 'df' and create label again\n",
    "df_pipeline_input = df.withColumn(\"label\", (df[\"income_k\"] > 70).cast(\"double\"))\n",
    "(trainingDataPipe, testDataPipe) = df_pipeline_input.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Stage 1: VectorAssembler (Estimator-like, but technically a Transformer here as parameters are fixed)\n",
    "assembler_pipe = VectorAssembler(\n",
    "    inputCols=[\"age\", \"income_k\", \"experience_years\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Stage 2: Logistic Regression (Estimator)\n",
    "lr_pipe = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# 2. Create the Pipeline\n",
    "pipeline = Pipeline(stages=[assembler_pipe, lr_pipe])\n",
    "\n",
    "# 3. Train the Pipeline (Fit the Estimators)\n",
    "print(\"Fitting the Pipeline...\")\n",
    "# fit() runs assembler.transform() then lr_pipe.fit() on the transformed data\n",
    "pipelineModel = pipeline.fit(trainingDataPipe)\n",
    "print(\"Pipeline fitting complete.\")\n",
    "\n",
    "# 4. Make Predictions using the Fitted PipelineModel\n",
    "print(\"Making predictions using the PipelineModel...\")\n",
    "# transform() runs assembler.transform() then the fitted lrModel.transform()\n",
    "predictions_pipe = pipelineModel.transform(testDataPipe)\n",
    "\n",
    "print(\"Pipeline Predictions:\")\n",
    "predictions_pipe.select(\"age\", \"income_k\", \"features\", \"label\", \"prediction\").show(truncate=False)\n",
    "\n",
    "# 5. Save the entire PipelineModel\n",
    "# First, remove the directory if it exists (for local filesystem)\n",
    "if os.path.exists(pipeline_path):\n",
    "    shutil.rmtree(pipeline_path)\n",
    "print(f\"Saving PipelineModel to: {pipeline_path}\")\n",
    "pipelineModel.save(pipeline_path)\n",
    "\n",
    "# 6. Load the PipelineModel\n",
    "print(f\"Loading PipelineModel from: {pipeline_path}\")\n",
    "loadedPipelineModel = PipelineModel.load(pipeline_path)\n",
    "\n",
    "# 7. Verify loaded PipelineModel works\n",
    "print(\"Predictions using loaded PipelineModel:\")\n",
    "loaded_pipe_predictions = loadedPipelineModel.transform(testDataPipe)\n",
    "loaded_pipe_predictions.select(\"age\", \"income_k\", \"features\", \"label\", \"prediction\").show(truncate=False, n=5)\n",
    "\n",
    "# Clean up saved directories\n",
    "shutil.rmtree(model_path)\n",
    "shutil.rmtree(pipeline_path)\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Save Model:** The `lrModel.save(model_path)` command saves the state of the trained `LogisticRegressionModel` to the specified directory `model_path`. Spark creates this directory and stores metadata (JSON format) and model data (often in Parquet format) inside.\n",
    "2.  **Load Model:** `LogisticRegressionModel.load(model_path)` reads the saved files and reconstructs the identical trained model object.\n",
    "3.  **Verify Loaded Model:** We use the `loadedLrModel` to make predictions, demonstrating it functions just like the original `lrModel`.\n",
    "4.  **Define Pipeline Stages:** We create instances of our desired stages: `VectorAssembler` and `LogisticRegression`. These are configured but not yet fitted (in the case of `lr_pipe`).\n",
    "5.  **Create Pipeline:** A `Pipeline` object is instantiated, passing the list of stages in the desired order of execution.\n",
    "6.  **Train Pipeline:** Calling `pipeline.fit(trainingDataPipe)` executes the pipeline:\n",
    "    *   `assembler_pipe` transforms `trainingDataPipe` to add the \"features\" column.\n",
    "    *   `lr_pipe` is fitted using the transformed data (with \"features\" and \"label\").\n",
    "    *   A `PipelineModel` is returned, containing the `assembler_pipe` and the *fitted* `LogisticRegressionModel`.\n",
    "7.  **Make Predictions (PipelineModel):** Calling `pipelineModel.transform(testDataPipe)` executes the fitted pipeline:\n",
    "    *   `assembler_pipe` transforms `testDataPipe`.\n",
    "    *   The *fitted* `LogisticRegressionModel` (inside `pipelineModel`) transforms the result to add predictions.\n",
    "8.  **Save PipelineModel:** The entire fitted pipeline (`pipelineModel`) can be saved using `pipelineModel.save()`. This saves all stages, including the fitted model parameters.\n",
    "9.  **Load PipelineModel:** `PipelineModel.load()` reconstructs the entire fitted pipeline.\n",
    "10. **Verify Loaded PipelineModel:** We use the `loadedPipelineModel` to make predictions on the test data, showing it encapsulates the full feature engineering + prediction workflow.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "-   **Persistence:** Deploying trained models into production scoring pipelines, sharing models between teams, checkpointing long training processes.\n",
    "-   **Pipelines:** Standardizing ML workflows, simplifying model deployment, enabling robust hyperparameter tuning (with `CrossValidator`), ensuring consistency between training and inference environments. Pipelines are the standard way to build production-grade ML applications in Spark.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Considerations & Performance Tuning\n",
    "\n",
    "While MLlib abstracts away much of the complexity of distributed computation, understanding certain aspects can help optimize performance:\n",
    "\n",
    "1.  **Data Partitioning:**\n",
    "    -   **Impact:** The way data is partitioned across the Spark cluster can significantly impact ML algorithm performance, especially for iterative algorithms (like KMeans, gradient descent). Poor partitioning can lead to data skew and excessive data shuffling between nodes.\n",
    "    -   **Action:** If performance is poor, consider repartitioning the input DataFrame (`df.repartition(numPartitions)` or `df.coalesce(numPartitions)`) before feeding it into the `fit()` method. The ideal number of partitions often depends on the number of cores available and the dataset size. Experimentation might be needed. Ensure partitions are not too small (causes overhead) or too large (reduces parallelism).\n",
    "\n",
    "2.  **Caching:**\n",
    "    -   **Impact:** Iterative algorithms repeatedly access the training data. Reading data from disk or recomputing transformations in each iteration is inefficient.\n",
    "    -   **Action:** Cache the training DataFrame in memory *before* calling `fit()` on an iterative estimator.\n",
    "        ```python\n",
    "        trainingData.cache()\n",
    "        # Optionally force caching by triggering an action\n",
    "        # trainingData.count()\n",
    "        model = estimator.fit(trainingData)\n",
    "        trainingData.unpersist() # Release memory after training\n",
    "        ```\n",
    "    -   **Use Case:** Crucial for algorithms like KMeans, Logistic Regression, GBTs, ALS, etc., especially on large datasets. Choose the appropriate storage level (`MEMORY_ONLY`, `MEMORY_AND_DISK`, etc.) based on available resources.\n",
    "\n",
    "3.  **Feature Vector Format (Dense vs. Sparse):**\n",
    "    -   **Impact:** `VectorAssembler` produces `DenseVector` by default. If your data is very high-dimensional and most features are zero (e.g., after one-hot encoding text features), using `SparseVector` can save significant memory and potentially speed up computations for algorithms optimized for sparsity.\n",
    "    -   **Action:** While `VectorAssembler` doesn't directly create sparse vectors based on input values, subsequent steps (like `HashingTF` or `CountVectorizer` for text) often produce sparse vectors naturally. Be aware of the vector types being used. Some algorithms might perform differently depending on vector density.\n",
    "\n",
    "4.  **Hyperparameter Tuning:**\n",
    "    -   **Impact:** Model performance heavily depends on choosing the right hyperparameters (e.g., `k` in KMeans, regularization parameters in regression, tree depth).\n",
    "    -   **Action:** Use MLlib's tuning utilities:\n",
    "        -   `ParamGridBuilder`: Define a grid of hyperparameters to search.\n",
    "        -   `CrossValidator`: Performs k-fold cross-validation to find the best hyperparameters from the grid. More robust but computationally expensive.\n",
    "        -   `TrainValidationSplit`: Performs a single split into training/validation sets. Faster than `CrossValidator` but potentially less robust.\n",
    "    -   **Use Case:** Essential for maximizing model accuracy and generalization. Often used in conjunction with Pipelines to tune parameters of multiple stages simultaneously.\n",
    "\n",
    "5.  **Algorithm Choice:**\n",
    "    -   **Impact:** Different algorithms have different scalability characteristics and performance trade-offs. For instance, Linear Regression is generally faster to train than Gradient Boosted Trees.\n",
    "    -   **Action:** Understand the computational complexity and assumptions of the algorithms. Choose an algorithm appropriate for the data size, dimensionality, and desired accuracy. Sometimes a simpler, faster algorithm might be sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "PySpark's MLlib provides a comprehensive and scalable toolkit for building end-to-end machine learning workflows. By leveraging DataFrames, feature transformers like `VectorAssembler`, a rich set of classification, regression, and clustering algorithms, and powerful abstractions like `Pipeline`s, developers can effectively tackle complex ML problems on large datasets. Understanding concepts like model persistence, evaluation, and performance tuning techniques such as caching and partitioning is key to building robust and efficient ML applications with Spark.\n",
    "\n",
    "---\n",
    "```python\n",
    "# Stop the SparkSession at the end\n",
    "spark.stop()\n",
    "print(\"Spark Session Stopped.\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
