{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929bb9f8",
   "metadata": {},
   "source": [
    "# Lesson 22 - Interview Questions and Coding Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07120a9a",
   "metadata": {},
   "source": [
    "Okay, let's build comprehensive technical notes on PySpark suitable for professional learners, followed by sections on interview preparation, debugging, and resume tips.\n",
    "\n",
    "## PySpark Technical Notes for Professional Learners\n",
    "\n",
    "These notes provide a deep dive into PySpark, focusing on its core concepts, architecture, APIs, and best practices for building scalable data processing applications.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to Apache Spark and PySpark\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python (PySpark), and R, along with an optimized engine that supports general execution graphs. Key features include:\n",
    "\n",
    "*   **Speed:** Spark can be significantly faster than Hadoop MapReduce due to in-memory processing and optimized execution DAGs (Directed Acyclic Graphs).\n",
    "*   **Ease of Use:** Offers rich APIs for common data transformations and actions.\n",
    "*   **Generality:** Combines SQL, streaming, machine learning, and graph processing in one platform.\n",
    "*   **Runs Everywhere:** Can run on Hadoop YARN, Apache Mesos, Kubernetes, standalone, or in the cloud.\n",
    "\n",
    "**PySpark** is the Python API for Spark. It allows data scientists and engineers to leverage the simplicity and richness of Python with the power of Spark's distributed computing engine. PySpark achieves this by:\n",
    "\n",
    "1.  Python code running in the Driver program launches Spark jobs.\n",
    "2.  PySpark internally uses Py4J to launch a JVM (Java Virtual Machine) and communicate with the Spark execution environment.\n",
    "3.  Code and data dependencies are shipped to executor nodes.\n",
    "4.  Python processes are spawned on executor nodes to execute tasks (like running UDFs or processing RDDs of Python objects). Data serialization/deserialization happens between JVM and Python processes.\n",
    "\n",
    "**Use Cases:** ETL (Extract, Transform, Load), interactive data analysis, machine learning pipelines, real-time stream processing, graph analytics.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Spark Architecture Fundamentals\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Understanding Spark's architecture is crucial for writing efficient code and debugging performance issues.\n",
    "\n",
    "*   **Cluster Manager:** An external service for acquiring resources on the cluster (e.g., YARN, Mesos, Kubernetes, Spark Standalone). Spark is agnostic to the underlying cluster manager.\n",
    "*   **Driver Program:** The process running the `main()` function of your application and creating the `SparkContext` (or `SparkSession`).\n",
    "    *   It coordinates the job execution.\n",
    "    *   It breaks down the user's code into smaller execution units called **Tasks**.\n",
    "    *   It schedules tasks on Executors.\n",
    "    *   It communicates with the Cluster Manager to request resources.\n",
    "*   **Executor:** A process launched for an application on a worker node.\n",
    "    *   Runs tasks scheduled by the Driver.\n",
    "    *   Holds data partitions in memory or on disk (`cache()`, `persist()`).\n",
    "    *   Communicates results back to the Driver.\n",
    "    *   Each executor has multiple **slots** (cores) for running tasks concurrently.\n",
    "*   **SparkSession:** (Introduced in Spark 2.0) The unified entry point for Spark functionality. It encapsulates `SparkContext`, `SQLContext`, `HiveContext`, and `StreamingContext`.\n",
    "*   **Job:** A parallel computation triggered by a Spark **Action** (e.g., `count()`, `collect()`, `save()`).\n",
    "*   **Stage:** Each job is divided into smaller sets of tasks called Stages. Stages are separated by **shuffle operations** (wide transformations like `groupByKey`, `reduceByKey`, `join`). Tasks within a stage can run in parallel without data shuffling.\n",
    "*   **Task:** A unit of work sent to an Executor to be executed on a specific **Partition** of data.\n",
    "*   **DAG (Directed Acyclic Graph):** Spark creates a DAG of operations (RDD transformations). This DAG represents the computation plan.\n",
    "*   **Lazy Evaluation:** Transformations in Spark are *lazy*. They are not executed immediately. Spark builds up the DAG of transformations. Execution only starts when an **Action** is called. This allows Spark to optimize the overall execution plan (e.g., predicate pushdown, pipelining transformations).\n",
    "\n",
    "**Diagrammatic Representation:**\n",
    "\n",
    "```\n",
    "+-------------------+       Resource Request      +-------------------+\n",
    "|   Driver Program  | -------------------------> |  Cluster Manager  |\n",
    "| (SparkSession,    | <------------------------- | (YARN, Mesos, etc)|\n",
    "|  DAG Scheduler,   |      Executor Allocation    +-------------------+\n",
    "|  Task Scheduler)  |        /|\\           /|\\\n",
    "+--------|----------+         |             |\n",
    "         | Task                |             | Task Resources\n",
    "         | Scheduling          |             | Allocated\n",
    "         V                     V             V\n",
    "+-------------------+       +-------------------+\n",
    "| Executor (Node 1) |       | Executor (Node N) |\n",
    "|-------------------|       |-------------------|\n",
    "| Cache | Task |Task|       | Cache | Task |Task|\n",
    "+-------------------+       +-------------------+\n",
    "      |     |                   |     |\n",
    "      +-----> Results ----------> Driver <-------+\n",
    "```\n",
    "\n",
    "**Code Example (Initializing SparkSession):**\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Theory: Create a SparkSession - the entry point to Spark functionality.\n",
    "# .appName(): Sets a name for the application, shown in the Spark UI.\n",
    "# .master(): Specifies the cluster manager. 'local[*]' runs Spark locally using all available cores.\n",
    "#            Other examples: 'yarn', 'spark://host:port' (standalone).\n",
    "# .getOrCreate(): Gets an existing SparkSession or creates a new one if none exists.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkFundamentals\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Theory: The 'spark' object is now your gateway to Spark APIs.\n",
    "# It encapsulates the SparkContext, which can be accessed via spark.sparkContext.\n",
    "print(f\"SparkSession created. Spark version: {spark.version}\")\n",
    "print(f\"SparkContext available: {spark.sparkContext}\")\n",
    "\n",
    "# Practical Use Case: This initialization is the starting point for any PySpark application.\n",
    "# In a cluster environment, 'master' would typically be 'yarn' or omitted\n",
    "# if submitting via spark-submit which handles cluster discovery.\n",
    "\n",
    "# Shutdown the SparkSession when done (important in interactive sessions or scripts)\n",
    "# spark.stop() # Usually at the end of your script/notebook\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `from pyspark.sql import SparkSession`: Imports the necessary class to create a SparkSession.\n",
    "2.  `spark = SparkSession.builder`: Starts the builder pattern to configure the SparkSession.\n",
    "3.  `.appName(\"PySparkFundamentals\")`: Assigns a name to the application for identification in logs and the Spark UI.\n",
    "4.  `.master(\"local[*]\")`: Configures Spark to run locally using as many worker threads as logical cores on the machine. This is ideal for development and testing. For cluster deployment, this would change (e.g., `yarn`).\n",
    "5.  `.getOrCreate()`: Constructs the SparkSession with the specified configuration or returns an existing one.\n",
    "6.  `print(...)`: Displays confirmation and accesses attributes like the Spark version and the underlying SparkContext.\n",
    "7.  `# spark.stop()`: Comments out the stop command, typically placed at the very end of an application's lifecycle to release resources.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Abstractions: RDDs and DataFrames\n",
    "\n",
    "#### 3.1 Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "RDD was the original core abstraction in Spark. It represents an immutable, partitioned collection of elements that can be operated on in parallel.\n",
    "\n",
    "*   **Resilient:** Fault-tolerant. If a partition is lost, Spark can recompute it using the lineage (the DAG of transformations).\n",
    "*   **Distributed:** Data in an RDD is partitioned across nodes in the cluster.\n",
    "*   **Dataset:** A collection of records (can be simple types or complex Python objects).\n",
    "\n",
    "RDDs support two types of operations:\n",
    "\n",
    "*   **Transformations:** Lazy operations that create a new RDD from an existing one (e.g., `map`, `filter`, `flatMap`).\n",
    "*   **Actions:** Operations that trigger computation and return a result to the driver program or write data to storage (e.g., `count`, `collect`, `saveAsTextFile`).\n",
    "\n",
    "While powerful, RDDs lack schema information and optimization opportunities available with DataFrames/Datasets. They are generally used for unstructured data or when fine-grained control over physical execution is needed.\n",
    "\n",
    "**Code Example (RDD):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext # Get the underlying SparkContext\n",
    "\n",
    "# Theory: Create an RDD from a Python list using parallelize.\n",
    "# Data is distributed into partitions (default based on master setting or specified).\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data, 4) # Explicitly request 4 partitions\n",
    "\n",
    "# Transformation: map - applies a function to each element. (Lazy)\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "\n",
    "# Transformation: filter - keeps elements satisfying a condition. (Lazy)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 20)\n",
    "\n",
    "# Action: collect - brings all elements from the RDD back to the Driver.\n",
    "# Use with caution on large datasets!\n",
    "results = filtered_rdd.collect()\n",
    "print(f\"RDD Transformation Results (collect): {results}\")\n",
    "\n",
    "# Action: count - returns the number of elements in the RDD. Triggers computation.\n",
    "count = filtered_rdd.count()\n",
    "print(f\"RDD Transformation Results (count): {count}\")\n",
    "\n",
    "# Action: reduce - aggregates elements using a function. Triggers computation.\n",
    "sum_of_squares = filtered_rdd.reduce(lambda a, b: a + b)\n",
    "print(f\"RDD Transformation Results (reduce): {sum_of_squares}\")\n",
    "\n",
    "# Practical Use Case: RDDs are useful for unstructured text processing or\n",
    "# when dealing directly with serialized Python objects requiring custom logic.\n",
    "# However, for structured data, DataFrames are strongly preferred.\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `sc = spark.sparkContext`: Obtains the SparkContext from the SparkSession, needed for RDD operations.\n",
    "2.  `data = [...]`: Defines a simple Python list.\n",
    "3.  `rdd = sc.parallelize(data, 4)`: Creates an RDD from the list `data`, distributing it into 4 partitions across the available executors (or threads in local mode).\n",
    "4.  `squared_rdd = rdd.map(...)`: Defines a transformation. It applies the lambda function (squaring the number) to each element of `rdd`. This *doesn't* execute yet. It returns a *new* RDD definition.\n",
    "5.  `filtered_rdd = squared_rdd.filter(...)`: Defines another transformation based on `squared_rdd`. It keeps only elements greater than 20. Still lazy.\n",
    "6.  `results = filtered_rdd.collect()`: This is an **action**. Spark now looks at the DAG (`parallelize` -> `map` -> `filter`), optimizes it, breaks it into stages/tasks, and executes them on the executors. The final results from all partitions are gathered back to the Driver program and stored in the `results` list. **Warning:** `collect()` can cause OutOfMemory errors on the Driver if the dataset is large.\n",
    "7.  `count = filtered_rdd.count()`: Another **action**. Triggers execution of the DAG again (unless `filtered_rdd` was cached). Returns the total number of elements matching the filter.\n",
    "8.  `sum_of_squares = filtered_rdd.reduce(...)`: An **action** that aggregates the RDD elements using the provided commutative and associative function (summation here).\n",
    "\n",
    "#### 3.2 DataFrames\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Introduced in Spark 1.3, the DataFrame API is the primary interface for working with structured and semi-structured data in modern Spark. A DataFrame is conceptually equivalent to a table in a relational database or a data frame in R/Python (Pandas), but distributed across the cluster.\n",
    "\n",
    "*   **Distributed:** Data is partitioned across nodes.\n",
    "*   **Schema:** DataFrames have a named schema (column names and types), allowing for better optimization.\n",
    "*   **Optimization:** Leverages the **Catalyst Optimizer** and **Tungsten execution engine**.\n",
    "    *   **Catalyst Optimizer:** Performs rule-based and cost-based optimization. It analyzes the logical plan (derived from DataFrame operations or SQL queries), applies optimization rules (e.g., predicate pushdown, column pruning), and generates multiple physical plans, choosing the most efficient one.\n",
    "    *   **Tungsten:** Focuses on optimizing Spark jobs for CPU and memory efficiency. It uses techniques like whole-stage code generation (compiling Spark operations into JVM bytecode directly, reducing virtual function calls) and optimized memory management (operating directly on binary data, avoiding Java object overhead and reducing Garbage Collection pressure).\n",
    "\n",
    "DataFrames can be created from various sources: structured data files (CSV, JSON, Parquet, ORC), Hive tables, external databases, or existing RDDs.\n",
    "\n",
    "**Code Example (DataFrame Basics):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, expr, avg, count, when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# === Creating DataFrames ===\n",
    "\n",
    "# 1. From a list of tuples/rows with schema inference (generally discouraged for production)\n",
    "data = [(\"Alice\", 34, 55000.0), (\"Bob\", 45, 72000.0), (\"Charlie\", 29, 48000.0), (\"David\", 34, 65000.0)]\n",
    "# Theory: When no schema is provided, Spark infers types, which can be slow and sometimes incorrect.\n",
    "inferred_df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
    "print(\"Inferred Schema:\")\n",
    "inferred_df.printSchema()\n",
    "inferred_df.show()\n",
    "\n",
    "# 2. From a list of tuples/rows with an explicit schema (Recommended)\n",
    "# Theory: Defining an explicit schema is faster, safer (prevents incorrect type assumptions),\n",
    "# and allows for early error detection if data doesn't match.\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True), # columnName, dataType, nullable\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "explicit_df = spark.createDataFrame(data, schema=schema)\n",
    "print(\"Explicit Schema:\")\n",
    "explicit_df.printSchema()\n",
    "explicit_df.show()\n",
    "\n",
    "# 3. Reading from a file (e.g., CSV)\n",
    "# Create a dummy CSV file for the example\n",
    "with open(\"employees.csv\", \"w\") as f:\n",
    "    f.write(\"name,age,salary,department\\n\")\n",
    "    f.write(\"Alice,34,55000.0,HR\\n\")\n",
    "    f.write(\"Bob,45,72000.0,Engineering\\n\")\n",
    "    f.write(\"Charlie,29,48000.0,Sales\\n\")\n",
    "    f.write(\"David,34,65000.0,Engineering\\n\")\n",
    "    f.write(\"Eve,29,52000.0,HR\\n\")\n",
    "    f.write(\"Frank,,60000.0,Sales\\n\") # Example with missing age\n",
    "\n",
    "# Theory: spark.read provides methods to read various formats.\n",
    "# 'header=True' uses the first line as column names.\n",
    "# 'inferSchema=True' scans the data to infer types (can be slow for large files).\n",
    "# Better practice: Provide the schema explicitly using .schema()\n",
    "csv_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "# Or with explicit schema (better performance & safety)\n",
    "# csv_schema = StructType([...]) # Define schema similar to above\n",
    "# csv_df = spark.read.csv(\"employees.csv\", header=True, schema=csv_schema)\n",
    "\n",
    "print(\"DataFrame from CSV:\")\n",
    "csv_df.printSchema()\n",
    "csv_df.show()\n",
    "\n",
    "# === Basic DataFrame Operations ===\n",
    "\n",
    "# Theory: Select specific columns. Use col() function or string names.\n",
    "name_age_df = csv_df.select(\"name\", \"age\")\n",
    "name_age_df.show()\n",
    "\n",
    "# Theory: Filter rows based on a condition.\n",
    "eng_df = csv_df.filter(col(\"department\") == \"Engineering\")\n",
    "# Alternative filter syntax: eng_df = csv_df.filter(\"department = 'Engineering'\")\n",
    "eng_df.show()\n",
    "\n",
    "# Theory: Add a new column or modify an existing one using withColumn.\n",
    "# expr() allows using SQL-like expressions.\n",
    "bonus_df = csv_df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "# Using expr for more complex logic\n",
    "salary_grade_df = bonus_df.withColumn(\"salary_grade\", expr(\"CASE WHEN salary >= 70000 THEN 'High' WHEN salary >= 50000 THEN 'Medium' ELSE 'Low' END\"))\n",
    "salary_grade_df.show()\n",
    "\n",
    "# Theory: Group by one or more columns and perform aggregations (agg).\n",
    "# Common aggregations: count, sum, avg, min, max.\n",
    "dept_salary_df = salary_grade_df.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        avg(\"salary\").alias(\"avg_salary\"), # Rename aggregated column using alias()\n",
    "        count(\"*\").alias(\"num_employees\")  # Count all rows in each group\n",
    "    )\n",
    "dept_salary_df.show()\n",
    "\n",
    "# === Handling Missing Data ===\n",
    "\n",
    "# Theory: dropna() removes rows with null values.\n",
    "# 'any' drops row if any column is null (default). 'all' drops if all columns are null.\n",
    "# 'subset' specifies columns to check for nulls.\n",
    "print(\"DataFrame with nulls:\")\n",
    "csv_df.filter(col(\"age\").isNull()).show()\n",
    "\n",
    "no_null_age_df = csv_df.dropna(subset=[\"age\"])\n",
    "print(\"DataFrame after dropping rows with null age:\")\n",
    "no_null_age_df.show()\n",
    "\n",
    "# Theory: fillna() fills null values with a specified value.\n",
    "# Can provide a single value for all columns of compatible types,\n",
    "# or a dictionary to specify values per column.\n",
    "filled_df = csv_df.fillna({\"age\": 0, \"name\": \"Unknown\"}) # Fill null age with 0, null name with \"Unknown\"\n",
    "print(\"DataFrame after filling nulls:\")\n",
    "filled_df.show()\n",
    "\n",
    "\n",
    "# Practical Use Cases: DataFrames are the workhorse for most ETL, data cleaning,\n",
    "# feature engineering, and analysis tasks on structured or semi-structured data in Spark.\n",
    "# Their performance benefits due to Catalyst and Tungsten are significant.\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation (Selected Parts):**\n",
    "\n",
    "1.  `spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])`: Creates a DataFrame, letting Spark *infer* the schema (`StringType`, `LongType`, `DoubleType` usually).\n",
    "2.  `StructType([...])`: Defines the schema explicitly using `StructField` for each column, specifying name, data type (`StringType`, `IntegerType`, etc.), and nullability.\n",
    "3.  `spark.createDataFrame(data, schema=schema)`: Creates a DataFrame using the *explicitly defined* schema. This is preferred.\n",
    "4.  `spark.read.csv(...)`: Reads data from a CSV file. `header=True` treats the first row as column names. `inferSchema=True` makes Spark read a portion of the data to guess column types (convenient but potentially slow/inaccurate). Providing `.schema(csv_schema)` is better.\n",
    "5.  `.select(\"name\", \"age\")`: Transformation to select only the \"name\" and \"age\" columns.\n",
    "6.  `.filter(col(\"department\") == \"Engineering\")`: Transformation to keep only rows where the \"department\" column equals \"Engineering\". `col()` refers to a column object.\n",
    "7.  `.withColumn(\"bonus\", col(\"salary\") * 0.1)`: Transformation to add a new column named \"bonus\", calculated as 10% of the \"salary\" column.\n",
    "8.  `.withColumn(\"salary_grade\", expr(\"...\"))`: Uses `expr` to add a column based on a SQL-like CASE WHEN expression.\n",
    "9.  `.groupBy(\"department\")`: Transformation that groups rows based on the values in the \"department\" column. This typically involves a shuffle operation.\n",
    "10. `.agg(avg(\"salary\").alias(\"avg_salary\"), ...)`: Performs aggregations on the grouped data. `avg(\"salary\")` calculates the average salary within each group. `.alias()` gives the resulting column a meaningful name. `count(\"*\")` counts the rows in each group.\n",
    "11. `.dropna(subset=[\"age\"])`: Transformation to remove rows where the value in the \"age\" column is null.\n",
    "12. `.fillna({\"age\": 0, \"name\": \"Unknown\"})`: Transformation to replace null values in the \"age\" column with 0 and null values in the \"name\" column with \"Unknown\".\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Spark SQL\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Spark SQL allows you to run standard SQL queries directly on DataFrames or against data sources configured within Spark. It integrates seamlessly with the DataFrame API.\n",
    "\n",
    "You can register a DataFrame as a temporary view (scoped to the current `SparkSession`) or a global temporary view (shared across `SparkSession`s on the same Spark application) and then query it using SQL syntax via `spark.sql()`.\n",
    "\n",
    "This is extremely useful for:\n",
    "\n",
    "*   Leveraging existing SQL skills.\n",
    "*   Performing complex queries that might be more concise in SQL than DataFrame API code.\n",
    "*   Interoperability with tools that connect via JDBC/ODBC.\n",
    "\n",
    "**Code Example (Spark SQL):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Assume csv_df from the previous example is available or recreate it\n",
    "data = [(\"Alice\", 34, 55000.0, \"HR\"), (\"Bob\", 45, 72000.0, \"Engineering\"),\n",
    "        (\"Charlie\", 29, 48000.0, \"Sales\"), (\"David\", 34, 65000.0, \"Engineering\"),\n",
    "        (\"Eve\", 29, 52000.0, \"HR\")]\n",
    "schema = [\"name\", \"age\", \"salary\", \"department\"]\n",
    "csv_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Theory: Register the DataFrame as a temporary view.\n",
    "# This view exists only within this SparkSession.\n",
    "csv_df.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "# Theory: Execute SQL queries directly using spark.sql().\n",
    "# The query runs against the registered temporary view 'employees_view'.\n",
    "# The result is another DataFrame.\n",
    "high_earners_df = spark.sql(\"\"\"\n",
    "    SELECT name, salary\n",
    "    FROM employees_view\n",
    "    WHERE salary > 60000\n",
    "\"\"\")\n",
    "\n",
    "print(\"High Earners (via Spark SQL):\")\n",
    "high_earners_df.show()\n",
    "high_earners_df.printSchema() # Note the result is a DataFrame\n",
    "\n",
    "# Theory: Perform aggregations using SQL syntax.\n",
    "dept_avg_salary_sql_df = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) as avg_salary, COUNT(*) as num_employees\n",
    "    FROM employees_view\n",
    "    WHERE age IS NOT NULL\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Department Average Salary (via Spark SQL):\")\n",
    "dept_avg_salary_sql_df.show()\n",
    "\n",
    "# Practical Use Case: Allows analysts familiar with SQL to query data loaded\n",
    "# via Spark, or complex transformations can sometimes be expressed more easily in SQL.\n",
    "# Useful for mixing DataFrame API operations and SQL queries.\n",
    "\n",
    "# Check if the view exists\n",
    "print(f\"Does 'employees_view' exist? {'employees_view' in spark.catalog.listTables()}\")\n",
    "\n",
    "# Theory: Global temporary views are tied to the Spark application, not the session.\n",
    "# They are registered in a global temporary database `global_temp`.\n",
    "csv_df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "\n",
    "# Querying a global temporary view requires qualifying the name with `global_temp.`\n",
    "global_df = spark.sql(\"SELECT COUNT(*) FROM global_temp.global_employees\")\n",
    "print(\"Count from global temporary view:\")\n",
    "global_df.show()\n",
    "\n",
    "# A new SparkSession *within the same application* can access the global view\n",
    "new_spark = spark.newSession()\n",
    "global_df_new_session = new_spark.sql(\"SELECT name FROM global_temp.global_employees WHERE age < 30\")\n",
    "print(\"Accessed global view from new session:\")\n",
    "global_df_new_session.show()\n",
    "\n",
    "# Drop the views when done (optional, they disappear when SparkSession/Application stops)\n",
    "spark.catalog.dropTempView(\"employees_view\")\n",
    "spark.catalog.dropGlobalTempView(\"global_employees\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `csv_df.createOrReplaceTempView(\"employees_view\")`: Makes the DataFrame `csv_df` queryable via SQL using the name `employees_view`. This view is temporary and session-scoped.\n",
    "2.  `spark.sql(\"\"\"...\"\"\")`: Executes the provided string as a SQL query against the registered views and tables known to the SparkSession.\n",
    "3.  `SELECT name, salary FROM employees_view WHERE salary > 60000`: A standard SQL query retrieving specific columns based on a filter condition from the temporary view. The result is returned as a *new DataFrame*.\n",
    "4.  `SELECT department, AVG(salary)... GROUP BY department...`: A more complex SQL query performing aggregation (`AVG`, `COUNT`), filtering (`WHERE`), grouping (`GROUP BY`), and ordering (`ORDER BY`). Again, the result is a DataFrame.\n",
    "5.  `spark.catalog.listTables()`: A utility to list registered temporary tables/views in the current session's catalog.\n",
    "6.  `csv_df.createOrReplaceGlobalTempView(\"global_employees\")`: Registers the DataFrame as a global temporary view, accessible across different SparkSessions within the *same Spark application*.\n",
    "7.  `spark.sql(\"SELECT ... FROM global_temp.global_employees\")`: Queries the global temporary view. Note the necessary prefix `global_temp.`.\n",
    "8.  `new_spark = spark.newSession()`: Creates a new SparkSession *within the same running Spark application* (same Driver JVM). This new session can access global temporary views created by other sessions in the application.\n",
    "9.  `spark.catalog.dropTempView(...)` / `spark.catalog.dropGlobalTempView(...)`: Explicitly removes the views from the catalog.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Persistence (Caching)\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Spark's lazy evaluation means computations are re-run every time an action is called on a DataFrame or RDD derived from the same lineage. If a specific DataFrame or RDD is used multiple times in an application (e.g., in iterative algorithms like machine learning or interactive analysis), recomputing it can be inefficient.\n",
    "\n",
    "**Persistence** (or Caching) allows you to store the intermediate results of a DataFrame or RDD in memory, on disk, or a combination of both. When an action is subsequently called on the persisted RDD/DataFrame, Spark will fetch the partitions from the cache rather than recomputing them.\n",
    "\n",
    "*   `cache()`: A shorthand for persisting with the default storage level, which is `StorageLevel.MEMORY_ONLY` for DataFrames/Datasets and RDDs in PySpark.\n",
    "*   `persist(StorageLevel)`: Allows specifying different storage levels:\n",
    "    *   `MEMORY_ONLY`: Store partitions as deserialized Java objects in JVM memory. If not enough memory, partitions won't be cached (or evicted if already cached). CPU-efficient for access.\n",
    "    *   `MEMORY_ONLY_SER`: Store partitions as *serialized* Java objects in JVM memory. More space-efficient than `MEMORY_ONLY`, but requires deserialization on access (more CPU).\n",
    "    *   `MEMORY_AND_DISK`: Store partitions in memory. If memory is full, spill excess partitions to disk. Slower access if read from disk.\n",
    "    *   `MEMORY_AND_DISK_SER`: Like `MEMORY_AND_DISK`, but store serialized objects in memory and on disk.\n",
    "    *   `DISK_ONLY`: Store partitions only on disk. CPU-intensive for reads/writes.\n",
    "    *   Off-heap options (`OFF_HEAP`) exist too, using memory managed outside the JVM heap.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "*   Caching is also lazy. The RDD/DataFrame is not actually materialized and stored until an action is performed on it.\n",
    "*   Use caching strategically. Caching everything can fill up memory and lead to performance degradation due to Garbage Collection pressure or data spills. Cache datasets that are accessed repeatedly.\n",
    "*   Remember to `unpersist()` DataFrames/RDDs when they are no longer needed to free up storage resources.\n",
    "\n",
    "**Code Example (Caching):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CachingExample\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame (e.g., reading from a file or complex transformation)\n",
    "# Let's simulate a costly operation\n",
    "data = range(1, 1000000) # One million numbers\n",
    "rdd = spark.sparkContext.parallelize(data, 100)\n",
    "# Simulate some transformations\n",
    "def complex_processing(x):\n",
    "    # Replace with actual complex logic\n",
    "    import time\n",
    "    # time.sleep(0.0001) # Uncomment to simulate work - makes caching effect obvious\n",
    "    return (x, x * x, str(x) * 2)\n",
    "\n",
    "processed_rdd = rdd.map(complex_processing)\n",
    "df = processed_rdd.toDF([\"id\", \"id_squared\", \"id_string\"])\n",
    "\n",
    "# ---- Without Caching ----\n",
    "print(\"--- Without Caching ---\")\n",
    "# Action 1: Count rows. Triggers computation.\n",
    "start_time = time.time()\n",
    "count1 = df.count()\n",
    "duration1 = time.time() - start_time\n",
    "print(f\"Action 1 (count) took: {duration1:.2f} seconds. Count: {count1}\")\n",
    "\n",
    "# Action 2: Perform aggregation. Re-computes the entire lineage.\n",
    "start_time = time.time()\n",
    "avg_sq = df.agg({\"id_squared\": \"avg\"}).first()[0]\n",
    "duration2 = time.time() - start_time\n",
    "print(f\"Action 2 (avg) took: {duration2:.2f} seconds. Avg Squared: {avg_sq}\")\n",
    "\n",
    "# ---- With Caching ----\n",
    "print(\"\\n--- With Caching ---\")\n",
    "# Theory: Persist the DataFrame in memory. cache() is MEMORY_ONLY by default for DataFrames.\n",
    "# You can use df.persist(StorageLevel.MEMORY_AND_DISK) for spilling to disk if memory is insufficient.\n",
    "df.cache()\n",
    "# df.persist(StorageLevel.MEMORY_AND_DISK) # Alternative example\n",
    "\n",
    "# Theory: The first action materializes the DataFrame and stores its partitions in cache.\n",
    "start_time = time.time()\n",
    "count_cached = df.count() # This triggers the computation AND caching\n",
    "duration_cached1 = time.time() - start_time\n",
    "print(f\"Action 1 (count + cache materialization) took: {duration_cached1:.2f} seconds. Count: {count_cached}\")\n",
    "\n",
    "# Theory: The second action now reads directly from the cache (if partitions fit in memory).\n",
    "# Should be much faster as the 'complex_processing' is not re-run.\n",
    "start_time = time.time()\n",
    "avg_sq_cached = df.agg({\"id_squared\": \"avg\"}).first()[0]\n",
    "duration_cached2 = time.time() - start_time\n",
    "print(f\"Action 2 (avg from cache) took: {duration_cached2:.2f} seconds. Avg Squared: {avg_sq_cached}\")\n",
    "\n",
    "# Practical Use Case: Cache intermediate DataFrames in iterative ML algorithms\n",
    "# or during interactive data exploration where the same base data is queried multiple times.\n",
    "\n",
    "# Theory: Unpersist the DataFrame to free up memory/disk resources when done.\n",
    "df.unpersist()\n",
    "print(\"\\nDataFrame unpersisted.\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `processed_rdd = rdd.map(complex_processing)`: Simulates a potentially expensive transformation step.\n",
    "2.  `df = processed_rdd.toDF(...)`: Converts the RDD to a DataFrame.\n",
    "3.  `df.count()` / `df.agg(...)`: Actions performed *without* caching. Each action triggers the full computation lineage (`parallelize` -> `map` -> `toDF` -> `count`/`agg`).\n",
    "4.  `df.cache()`: Marks the DataFrame `df` for caching using the default storage level (`MEMORY_ONLY`). This is still lazy.\n",
    "5.  `count_cached = df.count()`: The *first* action after `cache()`. This triggers the computation of `df` and stores the resulting partitions in the executor's memory (if they fit).\n",
    "6.  `avg_sq_cached = df.agg(...)`: The *second* action on the *same cached* DataFrame. Spark recognizes `df` is cached and attempts to read its partitions directly from memory/disk instead of recomputing the `map` operation. This should be significantly faster if the initial computation was costly.\n",
    "7.  `df.unpersist()`: Explicitly removes the cached partitions of `df` from executor storage, freeing up resources. Crucial in long-running applications or when memory is constrained.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Partitioning\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Partitioning is fundamental to Spark's parallelism. A DataFrame or RDD is split into smaller chunks called **partitions**, and each partition is processed as a single task on an executor. Understanding and controlling partitioning is key to performance tuning.\n",
    "\n",
    "*   **How Partitions are Determined:**\n",
    "    *   **Source:** When reading data (e.g., from HDFS, S3), the number of partitions often depends on the underlying file system's blocks or file splits. For Kafka, it often aligns with Kafka partitions.\n",
    "    *   **Transformations:** Some transformations maintain the parent RDD's partitioning (e.g., `map`, `filter` - *narrow transformations*). Others require data shuffling across the network, which can change the number of partitions and is expensive (e.g., `groupByKey`, `reduceByKey`, `join` - *wide transformations*). The number of partitions after a shuffle is often determined by the `spark.sql.shuffle.partitions` configuration (default is 200).\n",
    "*   **Why Partitioning Matters:**\n",
    "    *   **Parallelism:** The number of partitions dictates the maximum level of parallelism for tasks processing that data. Too few partitions, and you might not utilize all available cores.\n",
    "    *   **Shuffles:** Wide transformations require shuffling data between executors based on a key (e.g., the group-by key, the join key). The amount of data shuffled heavily impacts performance. Proper partitioning can sometimes minimize shuffle data (e.g., if data is already partitioned by the join key).\n",
    "    *   **Data Skew:** If data is not evenly distributed across partitions (some partitions are much larger than others), tasks processing the large partitions become bottlenecks.\n",
    "    *   **Task Size:** Too many partitions can lead to scheduling overhead and tasks that are too small (processing tiny amounts of data).\n",
    "*   **Controlling Partitioning:**\n",
    "    *   `repartition(numPartitions, [colName(s)])`: Redistributes data across the specified `numPartitions`. This *always* incurs a full shuffle. Can increase or decrease the number of partitions. Can optionally partition by specific columns (hash partitioning), which can optimize subsequent joins or group-bys on the same columns.\n",
    "    *   `coalesce(numPartitions)`: Reduces the number of partitions to `numPartitions`. This is optimized to avoid a full shuffle by combining existing partitions on the *same executor*. It's more efficient than `repartition` for *decreasing* partition count but cannot increase it.\n",
    "\n",
    "**Code Example (Partitioning):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PartitioningExample\").master(\"local[4]\").getOrCreate() # Use 4 cores locally\n",
    "\n",
    "# Create a DataFrame\n",
    "data = range(1, 10001) # 10,000 numbers\n",
    "rdd = spark.sparkContext.parallelize(data, 8) # Start with 8 partitions\n",
    "df = rdd.toDF(\"id\")\n",
    "\n",
    "# Theory: Check the current number of partitions.\n",
    "# For DataFrames, access the underlying RDD's getNumPartitions.\n",
    "initial_partitions = df.rdd.getNumPartitions()\n",
    "print(f\"Initial number of partitions: {initial_partitions}\")\n",
    "\n",
    "# Theory: Show partition distribution using spark_partition_id()\n",
    "# This function returns the ID of the partition containing each row.\n",
    "# groupBy().count() shows how many rows are in each partition.\n",
    "print(\"Initial Partition Distribution:\")\n",
    "df.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "  .groupBy(\"partition_id\").count().orderBy(\"partition_id\").show()\n",
    "\n",
    "# --- Repartition ---\n",
    "# Theory: Repartition the DataFrame into fewer partitions (e.g., 4).\n",
    "# This involves a full shuffle of the data across the network (even locally).\n",
    "repartitioned_df = df.repartition(4)\n",
    "\n",
    "# Theory: Check the new number of partitions.\n",
    "print(f\"\\nNumber of partitions after repartition(4): {repartitioned_df.rdd.getNumPartitions()}\")\n",
    "print(\"Partition Distribution after repartition(4):\")\n",
    "repartitioned_df.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "                .groupBy(\"partition_id\").count().orderBy(\"partition_id\").show()\n",
    "\n",
    "# Theory: Repartition based on a column's hash value. Rows with the same hash value\n",
    "# (often, rows with the same value in the partitioning column) end up in the same partition.\n",
    "# Useful for optimizing joins or group-bys on 'id % 4'.\n",
    "hashed_repartitioned_df = df.repartition(4, \"id\") # Hash partition by 'id' into 4 partitions\n",
    "print(f\"\\nNumber of partitions after repartition(4, 'id'): {hashed_repartitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "\n",
    "# --- Coalesce ---\n",
    "# Theory: Coalesce the DataFrame into fewer partitions (e.g., 2).\n",
    "# This avoids a full shuffle, preferred for reducing partitions.\n",
    "coalesced_df = repartitioned_df.coalesce(2)\n",
    "\n",
    "# Theory: Check the new number of partitions.\n",
    "print(f\"\\nNumber of partitions after coalesce(2): {coalesced_df.rdd.getNumPartitions()}\")\n",
    "print(\"Partition Distribution after coalesce(2):\")\n",
    "coalesced_df.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "             .groupBy(\"partition_id\").count().orderBy(\"partition_id\").show()\n",
    "\n",
    "\n",
    "# Practical Use Case:\n",
    "# - Use coalesce() before writing to a file system if you want fewer output files\n",
    "#   (e.g., df.coalesce(1).write.csv(...)). Be careful, coalescing to 1 bottlenecks writes.\n",
    "# - Use repartition(N) when you need to increase parallelism or control data distribution\n",
    "#   before a shuffle operation (like a join or groupBy).\n",
    "# - Use repartition(N, col) to optimize joins/aggregations by pre-shuffling data\n",
    "#   based on the join/grouping key.\n",
    "\n",
    "# Configuration: Set default shuffle partitions\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\") # Default is 200\n",
    "# print(f\"Default shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `spark.sparkContext.parallelize(data, 8)`: Creates an RDD with the data explicitly distributed into 8 partitions.\n",
    "2.  `df.rdd.getNumPartitions()`: Retrieves the number of partitions for the DataFrame (by accessing its underlying RDD).\n",
    "3.  `df.withColumn(\"partition_id\", spark_partition_id())...`: Adds a column showing the partition ID for each row, then groups by it and counts to show row distribution across partitions.\n",
    "4.  `repartitioned_df = df.repartition(4)`: Creates a *new* DataFrame with the data shuffled into exactly 4 partitions. This involves network transfer as data is redistributed based on hash partitioning (by default, round-robin if no columns specified).\n",
    "5.  `hashed_repartitioned_df = df.repartition(4, \"id\")`: Creates a *new* DataFrame with 4 partitions, where rows are assigned to partitions based on the hash of the `id` column. Rows with the same `id` hash end up in the same partition.\n",
    "6.  `coalesced_df = repartitioned_df.coalesce(2)`: Creates a *new* DataFrame with only 2 partitions. This operation tries to minimize data movement by merging existing partitions located on the same executor. It's generally faster than `repartition` for *reducing* partition count.\n",
    "7.  `spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")`: Example of setting a Spark configuration property. This controls the default number of partitions created after shuffle operations (like `groupBy`, `join`).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Performance Tuning and Optimization\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Optimizing PySpark jobs involves understanding bottlenecks and applying appropriate techniques. Key areas include:\n",
    "\n",
    "*   **Shuffles:** Minimize shuffles as they involve expensive disk I/O and network data transfer.\n",
    "    *   Avoid `groupByKey` when possible; prefer `reduceByKey`, `aggregateByKey`, or DataFrame `groupBy().agg()` which perform partial aggregation on the map side.\n",
    "    *   Use `repartition(col)` before joins or group-bys if the same data is used multiple times with the same key.\n",
    "    *   Tune `spark.sql.shuffle.partitions`. Too high -> small tasks, scheduling overhead. Too low -> insufficient parallelism, potential OOM in tasks. Adjust based on data size and cluster resources.\n",
    "*   **Data Skew:** Occurs when data is unevenly distributed across partitions, causing some tasks to take much longer.\n",
    "    *   **Salting:** Add a random key to skewed keys before grouping/joining, then aggregate/join, and finally remove the salt key. This distributes the skewed key across multiple partitions.\n",
    "    *   Analyze data distribution (`groupBy(key).count()`) to identify skewed keys.\n",
    "*   **Serialization:** Data needs to be serialized/deserialized when sent over the network (shuffles) or stored/cached using `_SER` levels, or when passing data between JVM and Python processes (especially with UDFs).\n",
    "    *   Use efficient serialization formats (Kyro serializer is often faster than default Java serializer). Configure via `spark.serializer`.\n",
    "    *   DataFrames using Tungsten operate on binary data, minimizing serialization overhead within JVM operations. Python UDFs still incur JVM <-> Python serialization cost.\n",
    "*   **User-Defined Functions (UDFs):**\n",
    "    *   Standard Python UDFs are black boxes to the Catalyst optimizer and incur serialization/deserialization overhead between JVM and Python processes.\n",
    "    *   **Prefer built-in Spark SQL functions** whenever possible. They operate directly on Tungsten's binary format within the JVM and are optimized by Catalyst.\n",
    "    *   If UDFs are necessary:\n",
    "        *   Consider **Pandas UDFs (Vectorized UDFs)**: Use Apache Arrow to transfer data efficiently between JVM and Python. They operate on Pandas Series/DataFrames batches, significantly reducing overhead for vectorized operations.\n",
    "*   **File Formats and Predicate Pushdown:**\n",
    "    *   Use efficient, splittable, columnar file formats like **Parquet** or **ORC**.\n",
    "        *   **Columnar:** Read only the required columns (column pruning).\n",
    "        *   **Predicate Pushdown:** Filters in `WHERE` clauses can be pushed down to the file reading layer, skipping entire chunks/row-groups of data if metadata (min/max stats stored in Parquet footers) indicates they don't match the filter. Partitioning source data (e.g., by date) further enhances this.\n",
    "    *   Avoid text formats like CSV or JSON for large datasets if performance is critical.\n",
    "*   **Caching Strategy:** Cache smartly (see Persistence section). Don't cache everything. Monitor cache usage in Spark UI.\n",
    "*   **Broadcast Joins:** When joining a large DataFrame with a small DataFrame, Spark can automatically **broadcast** the smaller DataFrame to all executors. This avoids shuffling the large DataFrame.\n",
    "    *   Controlled by `spark.sql.autoBroadcastJoinThreshold` (default 10MB). Increase if small tables are slightly larger but fit comfortably in executor memory.\n",
    "    *   Can explicitly hint using `broadcast()` function: `large_df.join(broadcast(small_df), \"join_key\")`.\n",
    "*   **Spark UI:** The most crucial tool for debugging and optimization. Monitor job/stage/task progress, execution times, shuffle read/write amounts, storage usage, GC time, and check the DAG visualization and SQL query plans. Identify long-running tasks, stages with large shuffles, or data skew.\n",
    "\n",
    "**Code Example (Illustrative Optimization Hints):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col, count, year\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OptimizationHints\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Assume large_sales_df and small_stores_df exist\n",
    "\n",
    "# Example: large_sales_df (~billions of records)\n",
    "# Columns: transaction_id, product_id, store_id, amount, timestamp\n",
    "# Example: small_stores_df (~hundreds of records)\n",
    "# Columns: store_id, store_name, city, state\n",
    "\n",
    "# Create dummy DataFrames for demonstration\n",
    "sales_data = [(i, 100 + i % 10, 1 + i % 5, float(10 + i % 50), \"2023-01-01\") for i in range(1000)] # Small example data\n",
    "sales_schema = [\"transaction_id\", \"product_id\", \"store_id\", \"amount\", \"timestamp\"]\n",
    "large_sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "stores_data = [(i, f\"Store_{i}\", f\"City_{i%2}\", f\"State_{i%3}\") for i in range(1, 6)]\n",
    "stores_schema = [\"store_id\", \"store_name\", \"city\", \"state\"]\n",
    "small_stores_df = spark.createDataFrame(stores_data, stores_schema)\n",
    "\n",
    "# ---- Optimization Example: Broadcast Join ----\n",
    "# Theory: Explicitly hint Spark to broadcast the small_stores_df.\n",
    "# This avoids shuffling the potentially much larger large_sales_df.\n",
    "# Spark might do this automatically if small_stores_df is below the threshold,\n",
    "# but hinting ensures it and improves plan readability.\n",
    "joined_df = large_sales_df.join(broadcast(small_stores_df), \"store_id\", \"inner\")\n",
    "\n",
    "print(\"Broadcast Join Result (showing a few rows):\")\n",
    "joined_df.show(5)\n",
    "# Check the Physical Plan in Spark UI (or via explain()) to confirm BroadcastHashJoin\n",
    "joined_df.explain()\n",
    "\n",
    "\n",
    "# ---- Optimization Example: Predicate Pushdown (Conceptual) ----\n",
    "# Assume sales data is stored as Parquet, partitioned by year and month\n",
    "# large_sales_df.write.partitionBy(\"year\", \"month\").parquet(\"sales_data.parquet\")\n",
    "\n",
    "# Theory: When reading partitioned Parquet data with filters on partition columns,\n",
    "# Spark reads only the necessary partitions (partition pruning).\n",
    "# Filters on non-partition columns can also be pushed down to Parquet reader\n",
    "# to skip row groups based on metadata (min/max stats).\n",
    "# sales_path = \"sales_data.parquet\"\n",
    "# filtered_sales = spark.read.parquet(sales_path) \\\n",
    "#                       .filter( (col(\"year\") == 2023) & (col(\"month\") == 12) & (col(\"amount\") > 100) )\n",
    "# filtered_sales.show()\n",
    "# In Spark UI, check the scan operation details for pushed filters.\n",
    "\n",
    "\n",
    "# ---- Optimization Example: Prefer Built-in Functions ----\n",
    "# Instead of a Python UDF to extract the year:\n",
    "# def get_year_udf(ts): return ts.split(\"-\")[0] # Simplified example\n",
    "# registered_udf = spark.udf.register(\"get_year_udf\", get_year_udf, StringType())\n",
    "# sales_with_year_udf = large_sales_df.withColumn(\"year\", registered_udf(col(\"timestamp\"))) # Less optimal\n",
    "\n",
    "# Theory: Use the built-in 'year' function for better performance.\n",
    "# It integrates with Catalyst and Tungsten.\n",
    "sales_with_year_builtin = large_sales_df.withColumn(\"year\", year(col(\"timestamp\")))\n",
    "print(\"\\nUsing built-in 'year' function:\")\n",
    "sales_with_year_builtin.show(5)\n",
    "sales_with_year_builtin.explain() # Plan will show use of optimized function\n",
    "\n",
    "# Practical Use Case: Always profile your jobs using Spark UI. Identify bottlenecks\n",
    "# (long stages, high shuffle, GC time, skew) and apply these techniques iteratively.\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `joined_df = large_sales_df.join(broadcast(small_stores_df), \"store_id\", \"inner\")`: Performs an inner join. The `broadcast()` hint explicitly tells Spark to send the entire `small_stores_df` to every executor that has partitions of `large_sales_df`. This avoids shuffling `large_sales_df` based on `store_id`.\n",
    "2.  `joined_df.explain()`: Prints the logical and physical execution plans for the DataFrame. Look for `BroadcastHashJoin` in the physical plan to confirm the broadcast hint worked.\n",
    "3.  `# large_sales_df.write.partitionBy...`: Commented-out example showing how data might be written partitioned by year/month in Parquet format.\n",
    "4.  `# filtered_sales = spark.read.parquet... .filter(...)`: Conceptual example showing how reading partitioned Parquet data with filters on partition columns (`year`, `month`) and other columns (`amount`) enables partition pruning and predicate pushdown, significantly reducing the amount of data read.\n",
    "5.  `sales_with_year_builtin = large_sales_df.withColumn(\"year\", year(col(\"timestamp\")))`: Uses the efficient, built-in Spark SQL function `year()` to extract the year from the timestamp column, which is preferable to using a Python UDF for the same task.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Shared Variables: Broadcast Variables and Accumulators\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Normally, when a function used in transformations (like `map` or `filter`) refers to an external variable, Spark ships a *copy* of that variable with each task. This can be inefficient if the variable is large. Spark provides two types of shared variables for specific use cases:\n",
    "\n",
    "*   **Broadcast Variables:** Used to efficiently distribute a large, read-only value (e.g., a lookup table, a machine learning model) to all worker nodes once, rather than sending it with every task. Tasks on each executor can then access the value from the local broadcast copy.\n",
    "*   **Accumulators:** Variables that can only be \"added\" to through associative and commutative operations (like counters or sums). They are used to implement distributed counters or sums reliably and efficiently. Only the Driver program can read the accumulator's final value. Tasks update the accumulator, but cannot read its value during execution. Useful for debugging or simple metrics.\n",
    "\n",
    "**Code Example (Shared Variables):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SharedVariables\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext # Need SparkContext for these\n",
    "\n",
    "# --- Broadcast Variable Example ---\n",
    "# Theory: A relatively large lookup dictionary needed by tasks.\n",
    "# Sending this with each task would be inefficient.\n",
    "states_lookup = {\n",
    "    \"CA\": \"California\", \"NY\": \"New York\", \"TX\": \"Texas\", \"FL\": \"Florida\",\n",
    "    # ... potentially thousands more entries\n",
    "}\n",
    "\n",
    "# Theory: Create a broadcast variable from the lookup table.\n",
    "# The driver serializes the data and sends it to each executor only once.\n",
    "broadcast_states = sc.broadcast(states_lookup)\n",
    "\n",
    "# Example RDD using the broadcast variable\n",
    "data = [(\"Alice\", \"CA\"), (\"Bob\", \"NY\"), (\"Charlie\", \"TX\"), (\"David\", \"CA\")]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Theory: Inside the transformation, access the broadcast variable's value using .value\n",
    "def map_state_name(record):\n",
    "    name, state_code = record\n",
    "    # Access the dictionary efficiently from the broadcast copy on the executor\n",
    "    full_state_name = broadcast_states.value.get(state_code, \"Unknown\")\n",
    "    return (name, full_state_name)\n",
    "\n",
    "mapped_rdd = rdd.map(map_state_name)\n",
    "print(\"Broadcast Variable Results:\")\n",
    "print(mapped_rdd.collect())\n",
    "\n",
    "# Practical Use Case: Distributing machine learning models, large static lookup tables,\n",
    "# configuration data needed by all tasks.\n",
    "\n",
    "# --- Accumulator Example ---\n",
    "# Theory: Create an accumulator, initialized to 0. Used for counting events across tasks.\n",
    "# Only the driver can read the final value. Tasks use += or .add()\n",
    "malformed_records_counter = sc.accumulator(0)\n",
    "\n",
    "data_with_errors = [\"1\", \"2\", \"three\", \"4\", \"five\", \"6\"]\n",
    "rdd_errors = sc.parallelize(data_with_errors)\n",
    "\n",
    "def process_and_count_errors(record):\n",
    "    global malformed_records_counter # Make accumulator accessible\n",
    "    try:\n",
    "        return int(record) * 2\n",
    "    except ValueError:\n",
    "        malformed_records_counter += 1 # Increment accumulator if conversion fails\n",
    "        return None # Or some other indicator\n",
    "\n",
    "processed_rdd_errors = rdd_errors.map(process_and_count_errors).filter(lambda x: x is not None)\n",
    "\n",
    "# Action needed to trigger processing and accumulator updates\n",
    "print(\"\\nProcessed numbers (errors excluded):\")\n",
    "print(processed_rdd_errors.collect())\n",
    "\n",
    "# Theory: Read the final value of the accumulator *on the driver* after actions complete.\n",
    "print(f\"Number of malformed records encountered (Accumulator): {malformed_records_counter.value}\")\n",
    "\n",
    "# Practical Use Case: Counting errors, number of records processed, simple metrics during a job.\n",
    "# Useful for debugging distributed code.\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `states_lookup = {...}`: Defines a Python dictionary (potentially large).\n",
    "2.  `broadcast_states = sc.broadcast(states_lookup)`: Creates a broadcast variable. Spark will handle efficient distribution to executors when it's first used by a task.\n",
    "3.  `rdd = sc.parallelize(data)`: Creates an RDD to process.\n",
    "4.  `broadcast_states.value.get(...)`: Inside the `map` function running on an executor, `.value` accesses the local copy of the broadcasted dictionary.\n",
    "5.  `mapped_rdd = rdd.map(map_state_name)`: Applies the function using the broadcast variable.\n",
    "6.  `malformed_records_counter = sc.accumulator(0)`: Initializes an accumulator with a starting value of 0.\n",
    "7.  `global malformed_records_counter`: Necessary within the function if modifying the accumulator created in the outer scope (standard Python scoping rule).\n",
    "8.  `malformed_records_counter += 1`: Tasks on executors increment the accumulator when an error occurs. This update is sent back to the driver efficiently. Tasks cannot read the current global value.\n",
    "9.  `processed_rdd_errors.collect()`: An action is required to execute the `map` transformation and trigger the accumulator updates.\n",
    "10. `malformed_records_counter.value`: After the action completes, the driver accesses the final aggregated value of the accumulator.\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson 22 Addendum: Interview Prep, Debugging, Resume\n",
    "\n",
    "This section complements the technical notes with practical advice for interviews, debugging, and resume building related to PySpark.\n",
    "\n",
    "### 100+ PySpark Coding Questions (Conceptual Categories)\n",
    "\n",
    "Instead of listing 100+ specific questions (which can become outdated), here are categories and types of questions commonly asked, covering the core concepts:\n",
    "\n",
    "**I. Core Spark/PySpark Concepts (15+ questions):**\n",
    "\n",
    "*   Explain Spark Architecture (Driver, Executor, Cluster Manager).\n",
    "*   What is RDD? Properties? When to use it over DataFrame?\n",
    "*   What is a DataFrame? Advantages over RDD?\n",
    "*   Explain Lazy Evaluation and its benefits.\n",
    "*   What is the difference between Transformation and Action? Give examples.\n",
    "*   What is DAG? How does Spark use it?\n",
    "*   What is SparkSession? What did it replace?\n",
    "*   Explain Persistence/Caching. Why use it? Storage levels?\n",
    "*   Difference between `cache()` and `persist()`?\n",
    "*   What are Broadcast Variables? Use case?\n",
    "*   What are Accumulators? Use case? Can tasks read them?\n",
    "*   Explain Partitioning. Why is it important?\n",
    "*   Difference between `repartition()` and `coalesce()`? When to use which?\n",
    "*   What is a Shuffle? Why is it expensive? What triggers it?\n",
    "*   What is `spark.sql.shuffle.partitions`? How to tune it?\n",
    "*   Explain Spark SQL. How do you use it with DataFrames?\n",
    "*   Difference between Temporary View and Global Temporary View?\n",
    "\n",
    "**II. DataFrame API Operations (25+ questions):**\n",
    "\n",
    "*   How to create a DataFrame (from RDD, list, file - CSV/JSON/Parquet)?\n",
    "*   How to define/specify a schema? Why is explicit schema better? (`StructType`, `StructField`)\n",
    "*   How to select columns? (`select`, `col`)\n",
    "*   How to filter rows? (`filter`, `where`)\n",
    "*   How to add or modify columns? (`withColumn`, `expr`)\n",
    "*   How to rename columns? (`withColumnRenamed`)\n",
    "*   How to drop columns? (`drop`)\n",
    "*   How to handle null values? (`dropna`, `fillna`)\n",
    "*   Explain `groupBy()` and `agg()`. Common aggregation functions? (`count`, `sum`, `avg`, `min`, `max`, `collect_list`)\n",
    "*   How to perform joins? Types of joins? (`join` method - `inner`, `left`, `right`, `full_outer`)\n",
    "*   How to handle duplicate rows? (`distinct`, `dropDuplicates`)\n",
    "*   How to sort data? (`orderBy`, `sort`)\n",
    "*   How to add a unique ID column? (`monotonically_increasing_id`)\n",
    "*   Explain window functions. Use cases? (`Window` spec, `rank`, `dense_rank`, `lag`, `lead`, `row_number`)\n",
    "*   How to union two DataFrames? Difference between `union()` and `unionByName()`?\n",
    "*   How to read/write Parquet files? Why preferred?\n",
    "*   How to read/write JSON/CSV files? Common options (`header`, `inferSchema`, `sep`, `multiLine`)?\n",
    "*   How to convert DataFrame to Pandas DataFrame? Risks? (`toPandas()`)\n",
    "*   How to create DataFrame from Pandas DataFrame? (`spark.createDataFrame(pandas_df)`)\n",
    "\n",
    "**III. Performance Tuning & Optimization (20+ questions):**\n",
    "\n",
    "*   What is the Catalyst Optimizer? What does it do?\n",
    "*   What is Tungsten? How does it improve performance?\n",
    "*   How do UDFs impact performance? What are the alternatives?\n",
    "*   Explain Pandas UDFs (Vectorized UDFs). Why are they faster? (Arrow)\n",
    "*   What is Data Skew? How to identify and handle it? (Salting)\n",
    "*   Explain Predicate Pushdown. How does it work with Parquet/ORC?\n",
    "*   Explain Partition Pruning. How does `partitionBy` help?\n",
    "*   How does `broadcast` join work? When is it useful? How to hint? (`spark.sql.autoBroadcastJoinThreshold`)\n",
    "*   How to use the Spark UI for debugging performance issues? What key metrics to look for? (Stage duration, Shuffle read/write, GC time, Task skew)\n",
    "*   Explain Kyro serialization. Why use it?\n",
    "*   Impact of file formats (Parquet vs CSV)?\n",
    "*   Impact of compression codecs (Snappy, Gzip)?\n",
    "*   When would you increase/decrease `spark.executor.memory`?\n",
    "*   When would you adjust `spark.executor.cores`?\n",
    "*   How can caching sometimes hurt performance? (GC pressure, spill)\n",
    "\n",
    "**IV. Coding Exercises (Scenario-Based):**\n",
    "\n",
    "*   Given a DataFrame of customer orders, find the top N customers by total spending.\n",
    "*   Given user activity logs, calculate session duration for each user.\n",
    "*   Clean a dataset: handle nulls, correct data types, filter outliers.\n",
    "*   Join sales data with store information, handling potential nulls in keys.\n",
    "*   Read multiple CSV files, infer schema (or use provided), union them, and write as partitioned Parquet.\n",
    "*   Implement logic using window functions (e.g., find salary rank within each department).\n",
    "*   Write a function using `groupBy` and `agg` to calculate multiple statistics per group.\n",
    "*   Convert specific business logic (e.g., complex conditional calculation) into PySpark DataFrame operations (`withColumn`, `when`, `expr`).\n",
    "*   Optimize a given PySpark code snippet (e.g., replace UDF, add broadcast hint, use `coalesce`).\n",
    "\n",
    "**V. Ecosystem & Advanced (10+ questions):**\n",
    "\n",
    "*   Briefly explain Spark Streaming (DStream or Structured Streaming). Key concepts?\n",
    "*   Briefly explain MLlib. What can it do?\n",
    "*   How does Spark integrate with Hadoop (HDFS, YARN)?\n",
    "*   How can Spark run on Kubernetes?\n",
    "*   Difference between `yarn-client` and `yarn-cluster` mode?\n",
    "*   What are common challenges when running PySpark in production? (Monitoring, dependency management, cost optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### MCQs and Short Answers (Examples)\n",
    "\n",
    "1.  **Which of the following is an Action in PySpark?**\n",
    "    (a) `filter()`\n",
    "    (b) `select()`\n",
    "    (c) `count()` (Correct)\n",
    "    (d) `withColumn()`\n",
    "2.  **Lazy Evaluation means:**\n",
    "    (a) Spark is slow to start.\n",
    "    (b) Transformations are executed only when an Action is called. (Correct)\n",
    "    (c) Spark avoids using memory.\n",
    "    (d) Code is evaluated line by line immediately.\n",
    "3.  **Which operation is most likely to cause a Shuffle?**\n",
    "    (a) `map()`\n",
    "    (b) `filter()`\n",
    "    (c) `groupByKey()` (Correct)\n",
    "    (d) `select()`\n",
    "4.  **To combine partitions *without* a full shuffle, you should use:**\n",
    "    (a) `repartition()`\n",
    "    (b) `coalesce()` (Correct)\n",
    "    (c) `union()`\n",
    "    (d) `cache()`\n",
    "5.  **What is the primary benefit of using explicit schemas for DataFrames?**\n",
    "    *   *Answer:* Performance (avoids schema inference pass) and data safety (prevents incorrect type assumptions).\n",
    "6.  **Why are built-in Spark SQL functions generally preferred over Python UDFs?**\n",
    "    *   *Answer:* Performance. Built-in functions integrate with Catalyst/Tungsten optimization and avoid JVM<->Python serialization overhead.\n",
    "7.  **What does `df.cache()` do?**\n",
    "    *   *Answer:* Marks the DataFrame for persistence using the default storage level (`MEMORY_ONLY`), materializing it in memory upon the next action.\n",
    "8.  **How can you optimize a join between a very large DataFrame and a small DataFrame?**\n",
    "    *   *Answer:* Use a broadcast join (either automatically via threshold or explicitly with `broadcast()` hint).\n",
    "\n",
    "---\n",
    "\n",
    "### Debugging Scenarios\n",
    "\n",
    "Debugging PySpark jobs often involves checking logs, the Spark UI, and understanding common failure patterns.\n",
    "\n",
    "1.  **Scenario: `OutOfMemoryError: Java heap space` (on Driver or Executors)**\n",
    "    *   **Cause (Driver):** `collect()`-ing too much data, broadcasting a huge variable, large number of partitions causing metadata overhead.\n",
    "    *   **Cause (Executor):** Processing large partitions, data skew, insufficient executor memory, inefficient UDFs holding large objects, memory leaks.\n",
    "    *   **Debugging Steps:**\n",
    "        *   Check Spark UI -> Executors tab for GC time and memory usage.\n",
    "        *   Avoid `collect()` on large DataFrames; use `take()`, `show()`, or write to disk.\n",
    "        *   Increase driver memory (`spark.driver.memory`) or executor memory (`spark.executor.memory`).\n",
    "        *   Check for data skew; repartition skewed data.\n",
    "        *   Increase shuffle partitions if tasks are processing too much data (`spark.sql.shuffle.partitions`).\n",
    "        *   Use more efficient data structures or algorithms.\n",
    "        *   Use `MEMORY_AND_DISK` persistence instead of `MEMORY_ONLY` if caching large data.\n",
    "2.  **Scenario: Job is very slow, especially during specific Stages (Shuffle Stages)**\n",
    "    *   **Cause:** Expensive shuffle operations, data skew, insufficient parallelism, network bottlenecks, disk I/O bottlenecks (spilling).\n",
    "    *   **Debugging Steps:**\n",
    "        *   Identify slow stages in Spark UI -> Stages tab. Look for high Shuffle Read/Write.\n",
    "        *   Analyze the DAG: what operations are causing the shuffle (`groupBy`, `join` etc.)?\n",
    "        *   Check task duration distribution within the stage. Are some tasks much slower (skew)?\n",
    "        *   If skewed, investigate keys and consider salting or other skew handling.\n",
    "        *   Tune `spark.sql.shuffle.partitions`. Maybe increase it for more parallelism or decrease if tasks are too small/fast.\n",
    "        *   Ensure data is partitioned appropriately *before* the shuffle if possible (`repartition(col)`).\n",
    "        *   Check if a broadcast join is applicable and used.\n",
    "        *   Ensure efficient file formats (Parquet) are used if I/O is involved.\n",
    "3.  **Scenario: `Py4JJavaError: ... NullPointerException`**\n",
    "    *   **Cause:** Often due to null values in DataFrame columns used in operations that don't expect them (e.g., UDFs not handling nulls, certain built-in functions under specific conditions). Can also indicate bugs in Spark code or connectors.\n",
    "    *   **Debugging Steps:**\n",
    "        *   Examine the full stack trace to pinpoint the operation causing the error.\n",
    "        *   Check data for unexpected nulls (`df.filter(col(\"problem_column\").isNull()).show()`).\n",
    "        *   Add null checks (`isNull()`, `isNotNull()`) in your DataFrame operations or UDFs.\n",
    "        *   Use `fillna()` or `dropna()` appropriately before the failing operation.\n",
    "4.  **Scenario: Serialization Errors (`NotSerializableException`, Kyro errors)**\n",
    "    *   **Cause:** Trying to use non-serializable objects within Spark transformations/actions (e.g., complex objects, lambda functions referencing non-serializable attributes of a class). Kyro might need classes to be registered.\n",
    "    *   **Debugging Steps:**\n",
    "        *   Ensure all objects/functions passed into RDD/DataFrame operations are serializable.\n",
    "        *   Avoid referencing entire objects inside lambdas if only attributes are needed; pass attributes instead.\n",
    "        *   If using Kyro, register custom classes (`spark.kryo.registrator` conf).\n",
    "        *   Simplify the code to isolate the non-serializable object.\n",
    "5.  **Scenario: Tasks Failing with `ExecutorLostFailure`**\n",
    "    *   **Cause:** Executors crashing due to OOM, node failures, loss of connectivity to driver, long GC pauses, issues within native code (e.g., Python UDFs crashing the Python process).\n",
    "    *   **Debugging Steps:**\n",
    "        *   Check executor logs on the worker nodes (via YARN UI or Kubernetes logs). Look for OOM errors, Python exceptions, or other fatal errors.\n",
    "        *   Monitor executor GC time and memory usage in Spark UI.\n",
    "        *   If OOM, increase executor memory or reduce memory footprint per task (e.g., process less data per task, optimize UDFs).\n",
    "        *   If related to specific code (e.g., Python UDFs), test the UDF logic locally with edge cases. Check library compatibility on executors.\n",
    "        *   Check cluster health and network stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Resume Tips for Data Engineers with PySpark Experience\n",
    "\n",
    "Highlighting PySpark effectively requires showing not just *what* you used, but *how* and *why*, focusing on impact and scale.\n",
    "\n",
    "1.  **Keywords are Crucial:** Recruiters and ATS (Applicant Tracking Systems) scan for keywords. Include:\n",
    "    *   `Apache Spark`, `PySpark`, `Spark SQL`\n",
    "    *   `DataFrames`, `RDDs`\n",
    "    *   `Spark Performance Tuning`, `Optimization`, `Partitioning`, `Caching`\n",
    "    *   `Distributed Computing`, `Big Data Processing`\n",
    "    *   Specific Spark libraries used: `Spark Streaming`, `Structured Streaming`, `MLlib` (if applicable)\n",
    "    *   Cluster managers: `YARN`, `Kubernetes`, `Mesos`\n",
    "    *   Related tech: `Hadoop`, `HDFS`, `Hive`, `Parquet`, `Delta Lake`, `Airflow`, `Kafka`, `Python`, `SQL`, `Scala` (if applicable), Cloud platforms (`AWS EMR`, `Azure Databricks`, `GCP DataProc`).\n",
    "\n",
    "2.  **Structure Your Skills Section:** Group related technologies.\n",
    "    *   **Big Data Technologies:** Apache Spark (PySpark, Spark SQL), Hadoop (HDFS, YARN), Kafka, Flink (if applicable)...\n",
    "    *   **Data Processing & ETL:** Spark Performance Tuning, Data Modeling, Data Warehousing (e.g., Redshift, BigQuery, Snowflake), ETL Tools (Airflow, Nifi)...\n",
    "    *   **Programming Languages:** Python (Pandas, NumPy), SQL, Scala (Basic/Intermediate)...\n",
    "    *   **Databases:** SQL (PostgreSQL, MySQL), NoSQL (Cassandra, MongoDB)...\n",
    "    *   **Cloud Platforms:** AWS (EMR, S3, Glue, Lambda), Azure (Databricks, ADLS, Data Factory), GCP (DataProc, GCS, BigQuery)...\n",
    "    *   **Concepts:** Distributed Systems, Data Structures, Algorithms, Performance Optimization, Data Partitioning...\n",
    "\n",
    "3.  **Quantify Achievements in Project/Experience Sections:** Don't just list responsibilities; show impact. Use the STAR method (Situation, Task, Action, Result).\n",
    "\n",
    "    *   **Weak:** \"Used PySpark for ETL jobs.\"\n",
    "    *   **Strong:** \"Developed and optimized PySpark ETL pipelines processing 1TB+ daily data from Kafka into a Parquet-based data lake on S3, reducing processing time by 40% through partition tuning and broadcast joins.\"\n",
    "    *   **Weak:** \"Worked on Spark performance tuning.\"\n",
    "    *   **Strong:** \"Improved performance of critical Spark batch jobs by 3x (from 2 hours to 40 minutes) by identifying and resolving data skew using salting techniques and optimizing shuffle partitions via Spark UI analysis.\"\n",
    "    *   **Weak:** \"Built data processing applications using PySpark.\"\n",
    "    *   **Strong:** \"Engineered a scalable PySpark application on AWS EMR to aggregate user behaviour data (500M+ events/day), enabling near real-time dashboard reporting for product teams; implemented robust error handling and monitoring.\"\n",
    "    *   **Weak:** \"Used Spark SQL.\"\n",
    "    *   **Strong:** \"Leveraged Spark SQL and DataFrame API to build complex data transformation logic, joining data from multiple sources (Hive, RDBMS) for downstream analytics; validated data quality using PySpark assertions.\"\n",
    "\n",
    "4.  **Highlight Specific Optimization Techniques:** If you have experience, mention it:\n",
    "    *   \"Optimized PySpark jobs using techniques like caching, broadcast joins, predicate pushdown, and UDF optimization (replacing Python UDFs with built-in functions/Pandas UDFs).\"\n",
    "    *   \"Managed Spark partitioning strategies (`repartition`, `coalesce`, `partitionBy` writes) to improve job parallelism and reduce shuffle overhead.\"\n",
    "    *   \"Utilized Spark UI extensively to diagnose performance bottlenecks (GC pressure, task skew, shuffle spill).\"\n",
    "\n",
    "5.  **Tailor to the Job Description:** Emphasize the PySpark skills and experiences most relevant to the specific role you're applying for. If the job mentions streaming, highlight your Spark Streaming/Structured Streaming experience. If it focuses on ETL/Data Warehousing, focus on those pipeline examples.\n",
    "\n",
    "6.  **Show, Don't Just Tell:** Link to a GitHub profile with personal PySpark projects (even small, well-documented ones) or relevant blog posts if applicable.\n",
    "\n",
    "By combining detailed technical knowledge with practical examples of application and optimization, you can create a compelling resume that showcases your PySpark expertise effectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
