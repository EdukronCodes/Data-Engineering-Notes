{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92753c77",
   "metadata": {},
   "source": [
    "# Lesson 17 - Structured Streaming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55272e30",
   "metadata": {},
   "source": [
    "Okay, here are the detailed technical notes on PySpark Structured Streaming, covering the specified concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Structured Streaming: Core Concepts and Implementation\n",
    "\n",
    "### Introduction to Structured Streaming\n",
    "\n",
    "Structured Streaming is Apache Spark's scalable and fault-tolerant stream processing engine, built upon the Spark SQL engine. It allows you to express streaming computations the same way you would express batch computations on static data. The core idea is to treat a live data stream as a table that is continuously being appended. This \"unbounded table\" model allows you to apply standard Spark SQL operations (like selection, projection, aggregation, windowing, joins) to streaming data.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1.  **Unbounded Table:** A conceptual table representing the data stream. New records arriving in the stream are like new rows being appended to this table.\n",
    "2.  **Event Time:** The time at which an event actually occurred in the source system. This is often embedded within the data record itself. Processing based on event time allows handling out-of-order or late-arriving data correctly.\n",
    "3.  **Processing Time:** The time at which the data is processed by the Spark engine.\n",
    "4.  **Triggers:** Mechanisms that control when Spark processes the next batch of streaming data.\n",
    "5.  **Watermarking:** A technique to handle late-arriving data in event-time processing, primarily used to bound the state required for aggregations and joins.\n",
    "6.  **Output Modes:** Define what is written to the output sink when the unbounded table is updated (e.g., `Append`, `Complete`, `Update`).\n",
    "7.  **Checkpointing:** Saving the stream's progress (offsets processed, running aggregates) to reliable storage (e.g., HDFS, S3, Azure Blob Storage). This is crucial for fault tolerance and recovery.\n",
    "\n",
    "---\n",
    "\n",
    "### Processing Modes: Micro-batch vs. Continuous\n",
    "\n",
    "Structured Streaming offers two primary modes for processing data, each with different latency characteristics and operational constraints.\n",
    "\n",
    "#### 1. Micro-batch Processing (Default Mode)\n",
    "\n",
    "**Theory:**\n",
    "Micro-batch processing executes the streaming query as a series of small, discrete batch jobs. At configured intervals (determined by a trigger), the engine checks for new data arrived since the last batch ended. If new data is available, it forms a \"micro-batch,\" and the Spark engine runs a batch job to process it and update the results. This leverages the robustness and exactly-once fault tolerance guarantees (where supported by sources/sinks) of the underlying Spark batch engine.\n",
    "\n",
    "**How it Works:**\n",
    "- The engine periodically queries the source for new data.\n",
    "- New data forms a small batch (DataFrame).\n",
    "- Standard Spark SQL optimizations and execution are applied to this batch.\n",
    "- Results are committed transactionally to the sink.\n",
    "- State (like running counts) is maintained across batches via checkpointing.\n",
    "\n",
    "**Pros:**\n",
    "- Achieves end-to-end exactly-once fault tolerance guarantees (with replayable sources and idempotent sinks).\n",
    "- High throughput.\n",
    "- Supports the full range of Spark SQL operations (aggregations, joins, arbitrary stateful operations using `mapGroupsWithState` or `flatMapGroupsWithState`).\n",
    "- Easier to reason about and debug due to its batch-like nature.\n",
    "\n",
    "**Cons:**\n",
    "- Latency is inherently tied to the micro-batch interval (typically ranging from ~100ms to several seconds). It cannot achieve sub-second latency reliably.\n",
    "\n",
    "**Use Cases:**\n",
    "- Near real-time ETL (Extract, Transform, Load).\n",
    "- Data warehousing updates.\n",
    "- Reporting dashboards where latency of a few seconds is acceptable.\n",
    "- Complex analytics involving aggregations and joins on streams.\n",
    "\n",
    "#### 2. Continuous Processing (Experimental in some versions, maturing)\n",
    "\n",
    "**Theory:**\n",
    "Continuous processing is a lower-latency mode designed to achieve end-to-end latencies as low as ~1 millisecond. Instead of discrete batches, Spark launches long-running tasks that continuously read, process, and write data. Records are processed as soon as they arrive.\n",
    "\n",
    "**How it Works:**\n",
    "- Dedicated long-running tasks are launched on Spark executors.\n",
    "- These tasks continuously ingest data from sources.\n",
    "- Records flow through the operators in the logical plan.\n",
    "- Processed records are continuously written to sinks.\n",
    "- Checkpointing occurs asynchronously without stopping processing, providing at-least-once guarantees. Achieving exactly-once is more complex and source/sink dependent.\n",
    "\n",
    "**Pros:**\n",
    "- Very low end-to-end latency (~1ms).\n",
    "\n",
    "**Cons:**\n",
    "- **Guarantees:** Primarily offers at-least-once guarantees. Achieving exactly-once requires specific source/sink combinations and careful configuration.\n",
    "- **Supported Operations:** Supports a subset of Spark SQL operations compared to micro-batching. Typically map-like operations (select, where, map, flatMap) are well-supported. Certain aggregations might be supported, but complex operations like stream-stream joins or arbitrary stateful operations often are not.\n",
    "- **Source/Sink Support:** Limited to specific sources (e.g., Kafka) and sinks (e.g., Kafka, console) that support continuous processing.\n",
    "- **Resource Intensive:** Can require dedicated cluster resources due to the long-running tasks.\n",
    "- **Experimental Nature:** While maturing, it might have more limitations or edge cases than the well-established micro-batch mode.\n",
    "\n",
    "**Use Cases:**\n",
    "- Ultra-low latency alerting systems (e.g., real-time fraud detection).\n",
    "- Real-time monitoring systems.\n",
    "- Scenarios where processing each record immediately is critical and at-least-once semantics are sufficient.\n",
    "\n",
    "**Choosing Between Modes:**\n",
    "- Start with the **default micro-batch mode** unless sub-second latency is an absolute requirement.\n",
    "- If ultra-low latency is needed, evaluate if your query structure, sources, and sinks are compatible with **continuous processing** and if at-least-once guarantees are acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Sources in Structured Streaming\n",
    "\n",
    "Structured Streaming can ingest data from various sources. The core API remains consistent: `spark.readStream`.\n",
    "\n",
    "#### 1. Kafka\n",
    "\n",
    "Kafka is a distributed streaming platform widely used for building real-time data pipelines. Structured Streaming provides a robust Kafka connector.\n",
    "\n",
    "**Theory:**\n",
    "Spark reads data from specified Kafka topics. It tracks offsets (positions within topic partitions) to ensure fault tolerance and process data exactly once (when used with checkpointing and idempotent sinks).\n",
    "\n",
    "**PySpark Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamReader\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) # Example configuration\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the Kafka message value (assuming JSON)\n",
    "# Adjust schema based on your actual data format\n",
    "json_schema = StructType() \\\n",
    "    .add(\"event_id\", StringType()) \\\n",
    "    .add(\"value\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) # Event time often included here\n",
    "\n",
    "# Read from Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka_host1:9092,kafka_host2:9092\") \\\n",
    "    .option(\"subscribe\", \"topic1,topic2\") # Subscribe to one or more topics\n",
    "    # .option(\"subscribePattern\", \"topic.*\") # Alternatively, subscribe using a pattern\n",
    "    .option(\"startingOffsets\", \"latest\") # Options: \"latest\", \"earliest\", or specific JSON string offsets\n",
    "    # .option(\"kafka.group.id\", \"my-streaming-group\") # Optional consumer group id\n",
    "    # .option(\"failOnDataLoss\", \"true\") # Abort query if offsets are lost (e.g., topic deleted)\n",
    "    .load()\n",
    "\n",
    "# Kafka messages have key, value, topic, partition, offset, timestamp, timestampType\n",
    "# Value is often binary, needs casting and parsing (e.g., JSON, Avro)\n",
    "parsed_df = kafka_df \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING) as json_value\", \"timestamp as kafka_timestamp\") \\\n",
    "    .select(\n",
    "        col(\"key\"),\n",
    "        # Assuming value is JSON, parse it using from_json\n",
    "        expr(\"from_json(json_value, '{}')\".format(json_schema.json())).alias(\"data\"),\n",
    "        col(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\"key\", \"data.*\", \"kafka_timestamp\") # Flatten the parsed structure\n",
    "\n",
    "# Example: Simple processing - just print to console\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `SparkSession.builder...getOrCreate()`: Standard way to get a SparkSession. `spark.sql.shuffle.partitions` is often tuned for performance.\n",
    "2.  `json_schema`: Define the expected structure of the JSON payload within the Kafka message `value`. This is crucial for parsing.\n",
    "3.  `spark.readStream`: Entry point for creating a streaming DataFrame.\n",
    "4.  `.format(\"kafka\")`: Specifies the Kafka source connector.\n",
    "5.  `.option(\"kafka.bootstrap.servers\", ...)`: **Required.** List of Kafka broker addresses.\n",
    "6.  `.option(\"subscribe\", ...)` or `.option(\"subscribePattern\", ...)`: **Required.** Specifies the topic(s) to read from.\n",
    "7.  `.option(\"startingOffsets\", ...)`: Determines where to start reading when the query starts for the first time (default is `latest`). `earliest` reads all existing data. Specific offsets can be provided via JSON for precise control or resuming from a known point without checkpoints.\n",
    "8.  `.option(\"failOnDataLoss\", ...)`: Controls behavior if Spark detects data loss in Kafka (e.g., offsets missing). Default is `true` in newer Spark versions.\n",
    "9.  `.load()`: Creates the initial streaming DataFrame from Kafka. It contains columns like `key`, `value` (binary), `topic`, `partition`, `offset`, `timestamp`, `timestampType`.\n",
    "10. `selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING) as json_value\", ...)`: Casts the binary key and value to strings. The `value` often needs further parsing.\n",
    "11. `expr(\"from_json(json_value, '{}')\".format(json_schema.json()))`: Parses the JSON string `json_value` according to the predefined `json_schema`.\n",
    "12. `.select(\"key\", \"data.*\", ...)`: Selects the key, flattens the parsed data structure, and includes the Kafka timestamp.\n",
    "13. `parsed_df.writeStream...`: Defines the output sink (here, `console`).\n",
    "14. `.outputMode(\"append\")`: Specifies that only new rows should be written to the sink.\n",
    "15. `.format(\"console\")`: Specifies the output sink type.\n",
    "16. `.start()`: Starts the streaming query execution asynchronously.\n",
    "17. `query.awaitTermination()`: Blocks the current thread until the query terminates (either manually or due to an error).\n",
    "\n",
    "**Use Cases:** Processing event streams, feeding real-time dashboards, triggering actions based on Kafka events.\n",
    "\n",
    "#### 2. Azure Event Hubs\n",
    "\n",
    "Azure Event Hubs is a fully managed, real-time data ingestion service on Azure, often used for telemetry and event streaming. It provides a Kafka-compatible endpoint, allowing the use of Spark's Kafka connector.\n",
    "\n",
    "**Theory:**\n",
    "By enabling the Kafka endpoint on Event Hubs, you can treat it like a Kafka cluster. Authentication typically uses SASL (Simple Authentication and Security Layer) with connection strings.\n",
    "\n",
    "**PySpark Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EventHubsStreamReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---- Event Hubs Configuration ----\n",
    "# Replace with your Event Hubs namespace connection string\n",
    "EH_NAMESPACE_CONNECTION_STRING = \"Endpoint=sb://<your-namespace>.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=<your-key>\"\n",
    "# Replace with your Event Hub name\n",
    "EH_NAME = \"<your-event-hub-name>\"\n",
    "\n",
    "# Construct the Kafka options for Event Hubs\n",
    "eh_kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"<your-namespace>.servicebus.windows.net:9093\", # Use port 9093 for Kafka endpoint\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{EH_NAMESPACE_CONNECTION_STRING}\";',\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"false\" # Recommended setting for Event Hubs Kafka protocol\n",
    "}\n",
    "\n",
    "# Define schema for the event data (assuming JSON in the Event Hub message body)\n",
    "event_schema = StructType() \\\n",
    "    .add(\"deviceId\", StringType()) \\\n",
    "    .add(\"measurement\", DoubleType()) \\\n",
    "    .add(\"eventTimestamp\", TimestampType())\n",
    "\n",
    "# Read from Event Hubs using the Kafka connector\n",
    "eventhubs_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**eh_kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the Event Hub message body (value column)\n",
    "parsed_df = eventhubs_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_body\", \"enqueuedTime as eventhub_timestamp\") \\\n",
    "    .select(\n",
    "        # Parse the JSON body\n",
    "        expr(\"from_json(json_body, '{}')\".format(event_schema.json())).alias(\"data\"),\n",
    "        col(\"eventhub_timestamp\") # Event Hubs provides enqueuedTime\n",
    "    ) \\\n",
    "    .select(\"data.*\", \"eventhub_timestamp\")\n",
    "\n",
    "# Example: Write to console\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') # Example trigger\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `EH_NAMESPACE_CONNECTION_STRING`, `EH_NAME`: Store your Event Hubs credentials and name. **Never hardcode credentials in production code; use secure methods like environment variables or secret management systems.**\n",
    "2.  `eh_kafka_options`: A dictionary holding the Kafka connection options tailored for Event Hubs.\n",
    "    *   `kafka.bootstrap.servers`: Uses the Event Hubs namespace name and port `9093`.\n",
    "    *   `subscribe`: The name of the specific Event Hub.\n",
    "    *   `kafka.sasl.mechanism`, `kafka.security.protocol`: Set to `PLAIN` and `SASL_SSL` respectively for Event Hubs authentication.\n",
    "    *   `kafka.sasl.jaas.config`: **Crucial for authentication.** Uses the special username `$ConnectionString` and the full namespace connection string as the password.\n",
    "    *   `failOnDataLoss`: Often set to `false` as Event Hubs Kafka protocol might handle retention differently than native Kafka.\n",
    "3.  `event_schema`: Schema definition for the expected JSON data within the Event Hub message body.\n",
    "4.  `spark.readStream.format(\"kafka\").options(**eh_kafka_options).load()`: Reads from Event Hubs using the configured Kafka options.\n",
    "5.  `selectExpr(\"CAST(value AS STRING)...\")`: Extracts the message body (value) as a string. Note Event Hubs provides `enqueuedTime` which can be useful.\n",
    "6.  `expr(\"from_json(...)\")...`: Parses the JSON body using the defined schema.\n",
    "7.  `.trigger(processingTime='10 seconds')`: An example of setting a trigger (covered next).\n",
    "8.  The rest of the code (writing to console, starting, awaiting termination) is similar to the Kafka example.\n",
    "\n",
    "**Use Cases:** Ingesting telemetry from IoT devices, application logs, clickstreams hosted on Azure.\n",
    "\n",
    "#### 3. Sockets (TCP Sockets)\n",
    "\n",
    "The socket source reads UTF-8 text data from a TCP socket connection. It's primarily intended for **testing and development purposes**, not production deployments, due to its lack of fault tolerance and recovery mechanisms.\n",
    "\n",
    "**Theory:**\n",
    "Spark connects to a specified host and port and listens for newline-delimited text data. If the connection drops, data might be lost. There's no offset tracking like in Kafka.\n",
    "\n",
    "**PySpark Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SocketStreamWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "# Run netcat (nc) on your terminal: nc -lk 9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Ensure the input lines are strings\n",
    "lines = lines.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Split the lines into words\n",
    "# split() splits the string column by space, returns an array\n",
    "# explode() transforms the array into multiple rows\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") # Output the entire updated counts table each time\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# To run this:\n",
    "# 1. Start this Spark application.\n",
    "# 2. Open a terminal and run: nc -lk 9999\n",
    "# 3. Type words/sentences into the netcat terminal and press Enter.\n",
    "# 4. Observe the word counts updating in the Spark application output.\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `spark.readStream.format(\"socket\")`: Specifies the socket source.\n",
    "2.  `.option(\"host\", \"localhost\")`: The hostname to connect to.\n",
    "3.  `.option(\"port\", 9999)`: The port number to connect to.\n",
    "4.  `.load()`: Creates the streaming DataFrame. It has a single column named `value` of type String (after casting).\n",
    "5.  `lines.select(explode(split(lines.value, \" \")).alias(\"word\"))`: A common pattern for word count: split lines into words and explode the resulting array into individual rows.\n",
    "6.  `words.groupBy(\"word\").count()`: Performs a streaming aggregation (word count).\n",
    "7.  `.outputMode(\"complete\")`: Since it's an aggregation without watermarking, `complete` mode is required to output the full set of counts every time the stream updates.\n",
    "8.  The rest starts the query and awaits termination. A netcat (`nc`) listener is needed to send data to the socket.\n",
    "\n",
    "**Use Cases:** Simple local testing, debugging streaming logic, educational examples. **Not suitable for production.**\n",
    "\n",
    "---\n",
    "\n",
    "### Triggers\n",
    "\n",
    "Triggers define the timing of streaming data processing in the micro-batch execution engine. They are specified using the `.trigger(...)` method on the `DataStreamWriter` (`df.writeStream.trigger(...)`).\n",
    "\n",
    "**Types of Triggers:**\n",
    "\n",
    "1.  **Default (No trigger specified):**\n",
    "    *   Processes data as soon as the previous micro-batch finishes processing.\n",
    "    *   If new data arrives while the previous batch is running, it will be processed immediately in the next batch upon completion of the current one.\n",
    "    *   Aims for lowest latency possible within the micro-batch paradigm, balanced with processing overhead.\n",
    "\n",
    "2.  **ProcessingTime (`trigger(processingTime='interval')`):**\n",
    "    *   The micro-batch will run at fixed intervals specified by the user (e.g., `'10 seconds'`, `'1 minute'`).\n",
    "    *   If a previous batch takes longer than the interval to complete, the next batch will start immediately after the previous one finishes (it won't wait for the *next* interval boundary). If a batch finishes quickly, the engine waits until the next interval boundary before starting a new batch.\n",
    "    *   **Use Case:** When you need predictable, regular batch updates, perhaps aligning with downstream reporting needs or controlling resource consumption.\n",
    "\n",
    "    ```python\n",
    "    query = wordCounts \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime='30 seconds') # Trigger every 30 seconds\n",
    "        .start()\n",
    "    ```\n",
    "\n",
    "3.  **Once (`trigger(once=True)`):**\n",
    "    *   The query will execute only *one* micro-batch. It processes all the available data since the last trigger (or from the start, if it's the first time) and then stops itself.\n",
    "    *   **Use Case:** Useful for scenarios where you want to periodically \"catch up\" with a stream in a batch-like manner, perhaps triggered by an external scheduler. Effectively turns a streaming source into a batch source for a single execution.\n",
    "\n",
    "    ```python\n",
    "    query = parsed_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", \"/path/to/output/data\") \\\n",
    "        .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "        .trigger(once=True) # Process available data once and stop\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination() # Wait for the single batch to complete\n",
    "    ```\n",
    "\n",
    "4.  **AvailableNow (`trigger(availableNow=True)`) (Spark 3.3+):**\n",
    "    *   Similar to `once=True`, but processes *all* available data in multiple micro-batches if needed, then stops. `once=True` typically processes in a single micro-batch.\n",
    "    *   This can lead to better resource utilization and faster catch-up compared to `once=True` if a large amount of data has accumulated.\n",
    "    *   **Use Case:** More efficient batch processing of streaming sources compared to `once`, especially when dealing with large backlogs of data. Ideal for replacing time-based batch jobs that process data accumulated over intervals like every hour or day.\n",
    "\n",
    "    ```python\n",
    "    query = parsed_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"delta\") # Example with Delta Lake sink\n",
    "        .option(\"path\", \"/path/to/delta/table\") \\\n",
    "        .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "        .trigger(availableNow=True) # Process all available data in potentially multiple batches, then stop\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination() # Wait for all batches to complete\n",
    "    ```\n",
    "\n",
    "5.  **Continuous (`trigger(continuous='interval')`):**\n",
    "    *   Switches the query to the **Continuous Processing** mode (discussed earlier).\n",
    "    *   The interval (e.g., `'1 second'`) specifies the desired checkpointing interval, not the processing latency itself. Processing happens near-continuously between checkpoints.\n",
    "    *   Subject to the limitations of continuous processing (supported operations, sources, sinks, at-least-once guarantee).\n",
    "    *   **Use Case:** When millisecond-level latency is critical and the query/source/sink constraints are met.\n",
    "\n",
    "    ```python\n",
    "    # Note: Continuous processing has stricter requirements on sources, sinks, and operations.\n",
    "    # This is a conceptual example; check compatibility for your specific query.\n",
    "    query = kafka_df \\ # Assuming kafka_df is a compatible source\n",
    "        .selectExpr(\"CAST(value AS STRING)\") \\ # Example simple transformation\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"kafka\") # Assuming kafka is a compatible sink\n",
    "        .option(\"kafka.bootstrap.servers\", \"...\") \\\n",
    "        .option(\"topic\", \"output_topic\") \\\n",
    "        .option(\"checkpointLocation\", \"/path/to/checkpoint/dir/continuous\") \\\n",
    "        .trigger(continuous='1 second') # Enable continuous mode, checkpoint every second\n",
    "        .start()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Watermarking\n",
    "\n",
    "Watermarking is a crucial mechanism in Structured Streaming for dealing with **late-arriving data** when performing **event-time based processing**, especially aggregations and joins.\n",
    "\n",
    "**Theory:**\n",
    "In real-world streams, data might arrive out of order or delayed due to network issues, clock skew, etc. When grouping data by event time (e.g., calculating counts per minute based on event timestamps), the engine needs to know when it's safe to finalize the aggregate for a given time window and emit the result. Otherwise, it would have to keep the state for that window indefinitely, waiting for potentially very late data, leading to unbounded memory usage.\n",
    "\n",
    "A watermark tells the Spark engine the point up to which it expects data to have arrived based on event time. It's defined as a moving threshold based on the maximum event time observed so far in the stream.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "1.  **Define Watermark:** You specify a watermark on an event-time column using `withWatermark(\"eventTimeColumn\", \"delayThreshold\")`.\n",
    "    *   `eventTimeColumn`: Must be of TimestampType.\n",
    "    *   `delayThreshold`: A duration string (e.g., `\"10 minutes\"`, `\"1 hour\"`). This specifies how late data is allowed to be relative to the maximum event time seen so far.\n",
    "2.  **Track Max Event Time:** Spark continuously tracks the maximum event time (`maxEventTime`) seen across all processed data.\n",
    "3.  **Calculate Watermark:** The watermark `W` for the next trigger is calculated as `W = maxEventTime - delayThreshold`.\n",
    "4.  **State Management & Output:**\n",
    "    *   For windowed aggregations, Spark maintains the state (e.g., counts) for each window.\n",
    "    *   Once the watermark `W` passes the end time of a window (`window.end`), Spark considers that window closed for late arrivals. Any data arriving with an event time `t` such that `t < W` (and specifically `t < window.end` which is already implied by `W > window.end`) is considered \"too late\" and is dropped (ignored for aggregation).\n",
    "    *   The engine can then safely finalize the aggregate for windows older than the watermark and emit results (in `Append` or `Update` mode) and clean up the state associated with those windows.\n",
    "5.  **Joins:** Watermarks are also essential for stream-stream joins to bound the state required to match records between the two streams based on event time constraints.\n",
    "\n",
    "**PySpark Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WatermarkingExample\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assume Kafka source with JSON data including event time\n",
    "# { \"device\": \"sensorA\", \"value\": 10, \"timestamp\": \"2023-10-27T10:00:15.000Z\" }\n",
    "json_schema = StructType() \\\n",
    "    .add(\"device\", StringType()) \\\n",
    "    .add(\"value\", IntegerType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) # Crucial: Event time column\n",
    "\n",
    "# Read from Kafka (similar setup as before)\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"device-events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and select relevant columns\n",
    "events_df = kafka_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") # Select device, value, timestamp\n",
    "\n",
    "# --- Define Watermark ---\n",
    "# Allow data to be up to 10 minutes late based on the 'timestamp' column\n",
    "events_with_watermark = events_df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "\n",
    "# --- Perform Windowed Aggregation ---\n",
    "# Count events per device in 5-minute tumbling windows\n",
    "windowedCounts = events_with_watermark \\\n",
    "    .groupBy(\n",
    "        col(\"device\"),\n",
    "        window(col(\"timestamp\"), \"5 minutes\") # Group by device and 5-min event time window\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\") # Optional: Order by window for clearer console output\n",
    "\n",
    "# Write the results to the console\n",
    "# Use 'update' mode: Only rows whose aggregates were updated are written\n",
    "query = windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") # 'update' mode is suitable for watermarked aggregations\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark/checkpoint_watermark\") # Checkpointing is MANDATORY for watermarking/aggregation\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `json_schema`: Includes a `timestamp` field of `TimestampType` which holds the event time.\n",
    "2.  `events_df`: The DataFrame containing the parsed events, including the `timestamp` column.\n",
    "3.  `.withWatermark(\"timestamp\", \"10 minutes\")`: This is the core watermarking step. It tells Spark to track the max `timestamp` and allow data to be considered if it's within 10 minutes of that max time. Data older than `maxEventTime - 10 minutes` might be dropped for stateful operations like the following aggregation.\n",
    "4.  `.groupBy(col(\"device\"), window(col(\"timestamp\"), \"5 minutes\"))`: Groups data by `device` and a 5-minute tumbling window based on the `timestamp` (event time) column. Other window types like sliding windows (`window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\")`) are also possible.\n",
    "5.  `.count()`: Performs the aggregation within each group and window.\n",
    "6.  `.writeStream`: Sets up the output sink.\n",
    "7.  `.outputMode(\"update\")`: In `Update` mode, only the rows in the result table that were updated since the last trigger will be written to the sink. This is typically used with watermarking, as windows aggregates get updated and then finalized once the watermark passes them. `Append` mode can also be used if you only want results *after* a window is finalized (i.e., after the watermark passes its end time). `Complete` mode is generally not allowed with watermarking as the full table of aggregates (including potentially infinite past windows) cannot be maintained.\n",
    "8.  `.option(\"checkpointLocation\", ...)`: **Checkpointing is mandatory** for any query using watermarking or aggregation to store intermediate state and watermark progress reliably.\n",
    "9.  The rest starts the query and awaits termination.\n",
    "\n",
    "**Use Cases:** Any stateful stream processing based on event time, such as time-windowed aggregations (e.g., calculating metrics per minute/hour), stream-stream joins based on event time ranges. It's essential for managing state size and ensuring results are emitted in a timely manner despite late data.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Considerations\n",
    "\n",
    "*   **Performance Tuning:**\n",
    "    *   **Shuffle Partitions:** Tune `spark.sql.shuffle.partitions` appropriately based on cluster size and data volume to avoid bottlenecks during stateful operations or joins.\n",
    "    *   **State Store:** For large state (e.g., many active groups in aggregation), consider using the RocksDB-based state store (`spark.sql.streaming.stateStore.providerClass=org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider`) for potentially better performance and memory management off-heap (requires adding the RocksDB dependency). Configure `spark.sql.streaming.stateStore.rocksdb.compactOnCommit` and other RocksDB options.\n",
    "    *   **Source Parallelism:** Ensure sufficient partitions in your source (e.g., Kafka topic partitions) to allow Spark to read in parallel. Match the number of Kafka partitions roughly with the number of cores available for reading.\n",
    "    *   **Sink Performance:** Ensure your sink can handle the write throughput. Use `foreachBatch` for complex sink logic or writing to systems without built-in Spark support, allowing batch-level optimizations within the streaming query.\n",
    "\n",
    "*   **Optimization:** Structured Streaming benefits from the Catalyst optimizer just like batch Spark SQL. Predicate pushdown (filtering data at the source) happens automatically where supported (e.g., Kafka with certain filter types).\n",
    "\n",
    "*   **Fault Tolerance:** Relies heavily on **checkpointing**. Ensure the checkpoint location is on reliable distributed storage (HDFS, S3, ADLS, GCS). Checkpointing stores processed offsets and intermediate state, allowing the query to restart from where it left off with exactly-once guarantees (for most sources/sinks).\n",
    "\n",
    "---\n",
    "\n",
    "These notes provide a comprehensive overview of Structured Streaming basics, processing modes, common sources, triggers, and watermarking in PySpark, suitable for professional learners building real-time data processing applications. Remember to consult the official Spark documentation for the most up-to-date details and specific configuration options."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
