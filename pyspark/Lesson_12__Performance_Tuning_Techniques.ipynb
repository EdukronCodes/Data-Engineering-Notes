{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e748748",
   "metadata": {},
   "source": [
    "# Lesson 12 - Performance Tuning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c9755",
   "metadata": {},
   "source": [
    "Okay, let's construct the detailed technical notes for Lesson 12 on PySpark Performance Tuning Techniques.\n",
    "\n",
    "---\n",
    "\n",
    "**Technical Notes: PySpark Performance Tuning Fundamentals**\n",
    "\n",
    "**Objective:** These notes provide practical techniques and conceptual understanding for optimizing the performance of PySpark applications. Effective tuning is crucial for processing large datasets efficiently, reducing job completion times, and minimizing resource consumption in distributed environments. We will focus on optimizing data reads, mitigating shuffle costs, and utilizing diagnostic tools like `explain()` alongside caching strategies.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Optimizing Data Input: Reading Data Efficiently**\n",
    "\n",
    "*   **Theory:**\n",
    "    The initial phase of reading data is often a significant bottleneck in Spark jobs. Optimizing this step involves choosing appropriate file formats, defining schemas correctly, and leveraging data layout strategies like partitioning and predicate pushdown.\n",
    "\n",
    "    *   **File Formats:**\n",
    "        *   **Text-Based (CSV, JSON):** Convenient but generally slower. Spark needs to parse entire rows even if only a few columns are needed. Schema inference can be slow and unreliable. Limited predicate pushdown capabilities.\n",
    "        *   **Columnar (Parquet, ORC):** **Highly recommended** for analytics workloads. Store data column-wise, enabling:\n",
    "            *   **Column Pruning:** Spark reads only the data for columns referenced in the query, significantly reducing I/O.\n",
    "            *   **Efficient Compression:** Often achieve better compression ratios, further reducing I/O.\n",
    "            *   **Predicate Pushdown:** Filters (predicates) in the query can often be pushed down to the file reading layer, skipping entire chunks (row groups in Parquet) of data that don't match the filter, drastically reducing data scanned.\n",
    "            *   **Schema Evolution:** Store schema metadata within the files.\n",
    "    *   **Schema Definition:**\n",
    "        *   **`inferSchema=True`:** Convenient for exploration, but requires an extra pass over (part of) the data, can be slow, and may infer incorrect types, leading to runtime errors or suboptimal plans. **Avoid in production.**\n",
    "        *   **Manual Schema:** Providing an explicit schema (`StructType`) is faster (no inference pass) and safer (ensures data type consistency). It allows Spark to optimize read operations more effectively.\n",
    "    *   **Storage Partitioning:**\n",
    "        *   Organizing data in storage (e.g., HDFS, S3) into directories based on the values of one or more columns (e.g., `/data/sales/year=2023/month=10/`).\n",
    "        *   When queries filter on partition columns (e.g., `WHERE year = 2023 AND month = 10`), Spark can prune entire directories, reading only the relevant data partitions. This drastically reduces the amount of data listed and scanned. Choose partition columns with low-to-moderate cardinality that are frequently used in filters.\n",
    "    *   **Predicate Pushdown:**\n",
    "        *   The ability of Spark (and the underlying data source/format) to apply filter conditions directly at the data reading stage *before* data is loaded into Spark executors' memory.\n",
    "        *   Columnar formats like Parquet are particularly effective at this, using internal metadata (min/max statistics per column chunk) to skip reading irrelevant data blocks.\n",
    "\n",
    "*   **Code Example: Demonstrating Partitioning and Schema**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"EfficientReading\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # --- Setup: Create Sample Data and Write Partitioned Parquet ---\n",
    "    data = [\n",
    "        (\"P101\", \"Widget\", 100, \"2023-10-26\"),\n",
    "        (\"P102\", \"Gadget\", 50, \"2023-10-26\"),\n",
    "        (\"P201\", \"Doohickey\", 200, \"2023-10-27\"),\n",
    "        (\"P103\", \"Widget Pro\", 75, \"2023-10-27\")\n",
    "    ]\n",
    "    # Define Schema Manually - crucial for performance and reliability\n",
    "    schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True) # Read as string initially for partitioning\n",
    "    ])\n",
    "\n",
    "    initial_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Define the output path and partition column\n",
    "    output_path = \"partitioned_sales.parquet\"\n",
    "    partition_col = \"sale_date\"\n",
    "\n",
    "    print(f\"--- Writing Partitioned Parquet Data to: {output_path} by '{partition_col}' ---\")\n",
    "    initial_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(partition_col) \\\n",
    "        .parquet(output_path)\n",
    "\n",
    "    print(\"Listing created partition directories:\")\n",
    "    # Verify directory structure (e.g., sale_date=2023-10-26/, sale_date=2023-10-27/)\n",
    "    if os.path.exists(output_path):\n",
    "        for item in os.listdir(output_path):\n",
    "            if os.path.isdir(os.path.join(output_path, item)) and \"=\" in item:\n",
    "                print(f\"- {item}\")\n",
    "\n",
    "    # --- Reading Partitioned Data Efficiently ---\n",
    "    # Define schema for reading (correct type for date now)\n",
    "    read_schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"sale_date\", DateType(), True) # Read partition col with correct type\n",
    "    ])\n",
    "\n",
    "    print(f\"\\n--- Reading all data from: {output_path} ---\")\n",
    "    all_sales_df = spark.read.schema(read_schema).parquet(output_path)\n",
    "    all_sales_df.show()\n",
    "    print(\"Physical Plan (All Data):\")\n",
    "    all_sales_df.explain() # Note the PartitionFilters: [], PushedFilters: [] initially\n",
    "\n",
    "    print(f\"\\n--- Reading data with filter on partition column ('sale_date' = '2023-10-27') ---\")\n",
    "    filtered_sales_df = spark.read.schema(read_schema).parquet(output_path) \\\n",
    "        .filter(\"sale_date = '2023-10-27'\") # Filter applied AFTER read in this syntax for demo\n",
    "        # Alternatively and often better: apply filter on read path if possible\n",
    "        # filtered_sales_df = spark.read.schema(read_schema).parquet(f\"{output_path}/sale_date=2023-10-27\") # Path filtering\n",
    "\n",
    "    filtered_sales_df.show()\n",
    "    print(\"Physical Plan (Filtered Data):\")\n",
    "    # CRITICAL: Observe the Physical Plan output for 'PartitionFilters'\n",
    "    filtered_sales_df.explain()\n",
    "    # Expect PartitionFilters: [isnotnull(sale_date#...), (sale_date#... = 19657)]\n",
    "    # This shows Spark will only scan the relevant partition directory.\n",
    "\n",
    "    # Clean up\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path)\n",
    "\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "*   **Explanation:**\n",
    "    *   We first create sample data and write it using `.partitionBy(\"sale_date\")`. This creates subdirectories like `sale_date=2023-10-26/` and `sale_date=2023-10-27/` within `partitioned_sales.parquet/`.\n",
    "    *   A `read_schema` is defined manually, specifying the correct `DateType` for `sale_date`.\n",
    "    *   When reading *all* data, `explain()` shows no partition filters applied initially.\n",
    "    *   When reading and then applying `.filter(\"sale_date = '2023-10-27'\")`, the `explain()` output is key. Look for the `PartitionFilters` section in the `FileScan parquet` part of the plan. It will indicate that Spark has pushed the `sale_date` filter down and will *only* scan the `sale_date=2023-10-27` directory, skipping the other partition entirely. This is a massive I/O saving on large datasets.\n",
    "    *   Using columnar formats like Parquet also enables predicate pushdown on non-partition columns (seen as `PushedFilters` in `explain()`), further reducing data read *within* the selected files/partitions.\n",
    "\n",
    "*   **Use Case:** Essential for Data Lake architectures, large-scale ETL pipelines, and any scenario involving querying large, structured datasets where filtering is common.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Understanding and Minimizing Shuffles**\n",
    "\n",
    "*   **Theory:**\n",
    "    A **shuffle** is the process of redistributing data across partitions, often moving it between executors over the network. This occurs during operations that need to group or relate data with the same key from different partitions.\n",
    "\n",
    "    **Why Shuffles are Expensive:**\n",
    "    *   **Network I/O:** Serializing and sending data over the network is slow.\n",
    "    *   **Disk I/O:** Intermediate shuffle data is often written to disk on executors.\n",
    "    *   **Serialization/Deserialization:** CPU overhead for converting objects to/from byte streams.\n",
    "    *   **Garbage Collection:** Increased memory pressure on executors can lead to more frequent GC pauses.\n",
    "\n",
    "    **Common Shuffle-Inducing Operations:**\n",
    "    *   `groupByKey()`, `reduceByKey()`, `aggregateByKey()`\n",
    "    *   `join()` (except for Broadcast Hash Joins)\n",
    "    *   `distinct()`\n",
    "    *   `repartition()`\n",
    "    *   Window functions without appropriate `PARTITION BY` clauses.\n",
    "    *   `sort()`, `orderBy()` (unless data is already partitioned and sorted correctly).\n",
    "\n",
    "    **Minimization Strategies:**\n",
    "    1.  **Avoid `groupByKey()`:** When possible, use `reduceByKey()` or `aggregateByKey()`. These perform partial aggregation locally on each partition *before* shuffling, reducing the amount of data transferred. `groupByKey()` shuffles *all* values for a given key to a single executor, potentially causing OOM errors for keys with many values.\n",
    "    2.  **Use Broadcast Joins:** As covered previously, broadcast small DataFrames in joins to avoid shuffling the large DataFrame. Check `explain()` for `BroadcastHashJoin`.\n",
    "    3.  **Filter Early:** Reduce the amount of data entering shuffle operations by applying filters (`where`, `filter`) *before* joins or aggregations.\n",
    "    4.  **Optimize Window Functions:** Use `PARTITION BY` clauses within window functions (`Window.partitionBy(...)`) to limit the data processed per partition/task. Ensure the partitioning aligns with how data is distributed.\n",
    "    5.  **Use `coalesce()` instead of `repartition()` for *reducing* partitions:** `coalesce(n)` merges existing partitions to reduce parallelism, attempting to minimize data movement (often avoiding a full shuffle). `repartition(n)` always incurs a full shuffle but can be used to *increase* parallelism or combat data skew by redistributing data more evenly (often using hash partitioning). Use `repartition` intentionally when needed, not just arbitrarily.\n",
    "\n",
    "*   **Code Example: `reduceByKey` vs. `groupByKey` (RDD level illustration)**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"ShuffleAvoidance\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext # Need SparkContext for RDD example\n",
    "\n",
    "    data = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\n",
    "    rdd = sc.parallelize(data, 4) # Create an RDD with 4 partitions\n",
    "\n",
    "    # Map to key-value pairs\n",
    "    pairs_rdd = rdd.map(lambda x: (x, 1))\n",
    "    print(f\"Number of partitions: {pairs_rdd.getNumPartitions()}\")\n",
    "\n",
    "    # --- Using groupByKey (Causes more shuffling) ---\n",
    "    print(\"\\n--- groupByKey() - Conceptually Shuffles All Values ---\")\n",
    "    # Internally: All 'apple' values shuffled to one task, all 'banana' to another...\n",
    "    grouped_rdd = pairs_rdd.groupByKey()\n",
    "    # Now apply mapValues to sum - aggregation happens AFTER shuffle\n",
    "    counts_grouped = grouped_rdd.mapValues(sum)\n",
    "    print(f\"Counts via groupByKey: {counts_grouped.collectAsMap()}\")\n",
    "    # Note: check Spark UI Stage details to see shuffle read/write for groupByKey\n",
    "\n",
    "    # --- Using reduceByKey (More efficient - map-side combine) ---\n",
    "    print(\"\\n--- reduceByKey() - Combines Locally Before Shuffle ---\")\n",
    "    # Internally: Sums occur within each partition first, then results are shuffled & summed\n",
    "    counts_reduced = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    print(f\"Counts via reduceByKey: {counts_reduced.collectAsMap()}\")\n",
    "    # Note: check Spark UI Stage details - shuffle write may be smaller than groupByKey\n",
    "\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "*   **Explanation:**\n",
    "    *   While both methods yield the same final count, their execution differs significantly.\n",
    "    *   `groupByKey()` collects *all* values associated with each key (`apple -> [1, 1, 1]`, `banana -> [1, 1]`) onto a single executor task *after* a shuffle. The summation (`mapValues(sum)`) happens afterward. This can be very memory-intensive if keys have many values.\n",
    "    *   `reduceByKey(lambda a, b: a + b)` performs the summation *locally within each original partition first*. For example, if one partition has `[(\"apple\", 1), (\"apple\", 1)]`, it calculates `(\"apple\", 2)` locally. Only these partially aggregated results (`(\"apple\", 2)`) are shuffled, significantly reducing the data volume transferred and memory pressure on the receiving tasks.\n",
    "    *   **DataFrame Equivalent:** While DataFrames abstract this, using aggregate functions like `count()`, `sum()`, `avg()` within a `groupBy().agg(...)` operation leverages similar optimized aggregation strategies (like partial aggregation) under the hood, generally avoiding the pitfalls of RDD `groupByKey`.\n",
    "\n",
    "*   **Use Case:** Optimizing aggregations, avoiding OOM errors during large-scale grouping operations, improving join performance via broadcasting.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Leveraging Caching and Persistence (`cache()`, `persist()`)**\n",
    "\n",
    "*   **Theory:**\n",
    "    Spark DataFrames (and RDDs) are computed lazily and are typically recomputed each time an action is called on them or their derivatives. If a specific DataFrame is used multiple times in a workflow (e.g., in iterative algorithms, interactive analysis, or branching logic), recomputing it can be wasteful.\n",
    "\n",
    "    *   **`cache()`:** A shorthand for `persist(StorageLevel.MEMORY_ONLY)`. It requests Spark to store the computed partitions of the DataFrame in memory on the executors.\n",
    "    *   **`persist(storageLevel)`:** Provides more control over *how* the DataFrame is stored. Common `StorageLevel`s (from `pyspark.StorageLevel`):\n",
    "        *   `MEMORY_ONLY`: Fast access, but partitions might be evicted if memory is insufficient (requiring recomputation).\n",
    "        *   `MEMORY_ONLY_SER`: Stores *serialized* objects in memory. More space-efficient than `MEMORY_ONLY` but requires CPU time for deserialization on access.\n",
    "        *   `MEMORY_AND_DISK`: Stores partitions in memory; spills excess partitions to disk. Slower access for disk-spilled partitions but more robust to memory pressure.\n",
    "        *   `MEMORY_AND_DISK_SER`: Serialized version of `MEMORY_AND_DISK`.\n",
    "        *   `DISK_ONLY`: Stores partitions only on disk. Slowest access, but reliable for very large DataFrames that don't fit in memory and whose recomputation is extremely costly.\n",
    "        *   Replicated versions (`_2` suffix): Store partitions on two nodes for better fault tolerance (can survive one node failure without recomputation).\n",
    "    *   **Lazy Caching:** Calling `cache()` or `persist()` marks the DataFrame for caching, but it's only actually populated when an action is executed on it for the first time.\n",
    "    *   **`unpersist()`:** **Crucial** to manually release the cached data from memory/disk when it's no longer needed, freeing up resources for subsequent stages.\n",
    "\n",
    "*   **When to Cache:**\n",
    "    *   DataFrames used repeatedly in iterative algorithms (e.g., machine learning training loops).\n",
    "    *   Intermediate DataFrames accessed multiple times in a complex workflow or interactive session.\n",
    "    *   When a computationally expensive DataFrame needs to be queried multiple times.\n",
    "\n",
    "*   **Code Example: Caching an Intermediate DataFrame**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, upper\n",
    "    from pyspark.storagelevel import StorageLevel\n",
    "    import time\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CachingExample\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Simulate reading a somewhat large DataFrame\n",
    "    # In reality, this would be spark.read.parquet(...) or similar\n",
    "    data_range = spark.range(0, 10 * 1000 * 1000).withColumnRenamed(\"id\", \"value\") # 10M rows\n",
    "\n",
    "    # Define a moderately expensive transformation\n",
    "    def complex_transform(df):\n",
    "        print(\"--> Performing complex_transform...\")\n",
    "        # Simulate some work - e.g., adding columns, UDFs etc.\n",
    "        time.sleep(1) # Simulate computation time\n",
    "        return df.withColumn(\"value_str\", col(\"value\").cast(\"string\")) \\\n",
    "                 .withColumn(\"value_upper\", upper(col(\"value_str\")))\n",
    "\n",
    "    # Apply the transformation\n",
    "    transformed_df = complex_transform(data_range)\n",
    "\n",
    "    # --- Scenario 1: Without Caching ---\n",
    "    print(\"\\n--- Running actions without caching ---\")\n",
    "    start = time.time()\n",
    "    count1 = transformed_df.filter(col(\"value\") < 1000).count() # Action 1\n",
    "    duration1 = time.time() - start\n",
    "    print(f\"Action 1 Count: {count1}, Duration: {duration1:.2f}s\") # complex_transform runs\n",
    "\n",
    "    start = time.time()\n",
    "    count2 = transformed_df.filter(col(\"value\") > 9999000).count() # Action 2\n",
    "    duration2 = time.time() - start\n",
    "    print(f\"Action 2 Count: {count2}, Duration: {duration2:.2f}s\") # complex_transform runs AGAIN\n",
    "\n",
    "    # --- Scenario 2: With Caching ---\n",
    "    print(\"\\n--- Caching the transformed DataFrame ---\")\n",
    "    # Cache using MEMORY_AND_DISK for robustness\n",
    "    transformed_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    # transformed_df.cache() # Shorthand for MEMORY_ONLY\n",
    "\n",
    "    print(\"\\n--- Running actions with caching ---\")\n",
    "    start = time.time()\n",
    "    # First action triggers computation AND caching\n",
    "    count3 = transformed_df.filter(col(\"value\") < 1000).count() # Action 3\n",
    "    duration3 = time.time() - start\n",
    "    print(f\"Action 3 Count (computes & caches): {count3}, Duration: {duration3:.2f}s\") # complex_transform runs\n",
    "\n",
    "    start = time.time()\n",
    "    # Second action should read from cache (memory or disk)\n",
    "    count4 = transformed_df.filter(col(\"value\") > 9999000).count() # Action 4\n",
    "    duration4 = time.time() - start\n",
    "    print(f\"Action 4 Count (reads from cache): {count4}, Duration: {duration4:.2f}s\") # Should be faster\n",
    "\n",
    "    # IMPORTANT: Unpersist when done\n",
    "    transformed_df.unpersist()\n",
    "    print(\"\\n--- DataFrame unpersisted ---\")\n",
    "\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "*   **Explanation:**\n",
    "    *   We simulate an expensive transformation (`complex_transform`).\n",
    "    *   Without caching, each action (`count()`) triggers the re-execution of the `range()` and `complex_transform` lineage. Durations for Action 1 and Action 2 should be similar (and include the `time.sleep`).\n",
    "    *   With `persist(StorageLevel.MEMORY_AND_DISK)`, the first action (Action 3) still runs the transformation but also stores the resulting partitions in memory or on disk. Its duration includes computation + caching overhead.\n",
    "    *   The second action (Action 4) finds the persisted data and reads directly from the cache, skipping the `complex_transform` step. Its duration should be significantly shorter.\n",
    "    *   `unpersist()` is called to free up the storage resources used by the cache.\n",
    "\n",
    "*   **Use Case:** Iterative machine learning, interactive data analysis in notebooks, complex ETL pipelines where intermediate results are reused.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Using `explain()` for Diagnosis**\n",
    "\n",
    "*   **Theory:**\n",
    "    The `explain()` method is an indispensable tool for understanding how Spark translates your DataFrame operations into a physical execution plan that runs on the cluster. Analyzing this plan helps identify potential bottlenecks like unnecessary shuffles, missed broadcast joins, or inefficient data scans.\n",
    "\n",
    "    *   **Plan Stages:** `explain()` typically shows:\n",
    "        *   **Parsed Logical Plan:** Initial plan representing the user's code logic.\n",
    "        *   **Analyzed Logical Plan:** Schema information is resolved, attributes are bound.\n",
    "        *   **Optimized Logical Plan:** Catalyst optimizer applies rules (predicate pushdown, projection pruning, etc.) to refine the logical plan.\n",
    "        *   **Physical Plan:** The **most important** plan for performance tuning. It shows the *actual* RDD operations, join algorithms (e.g., `BroadcastHashJoin`, `SortMergeJoin`), data exchanges (shuffles), and scan methods Spark will use.\n",
    "    *   **What to Look For in the Physical Plan:**\n",
    "        *   **Scan Operations:** `FileScan parquet`, `Scan JDBC`, etc. Look for `PartitionFilters` (shows partition pruning) and `PushedFilters` (shows predicate pushdown).\n",
    "        *   **Join Operations:** `BroadcastHashJoin` (good if intended), `SortMergeJoin`, `ShuffledHashJoin` (indicate shuffling).\n",
    "        *   **Shuffle Indicators:** `Exchange hashpartitioning(...)`, `Exchange RoundRobinPartitioning(...)`, etc., clearly mark shuffle boundaries between stages.\n",
    "        *   **Aggregations:** `HashAggregate` (often indicates efficient map-side aggregation combined with final aggregation).\n",
    "        *   **Whole-Stage Code Generation:** Stages often start with `*(N)` (e.g., `*(1) Project`, `*(2) Filter`). This indicates Tungsten's whole-stage code generation is active for that stage, which is generally good for CPU performance.\n",
    "\n",
    "*   **Code Example: Analyzing a Simple Query Plan**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, broadcast\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ExplainExample\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # Disable auto-broadcast for demo\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Sample DataFrames from join lesson\n",
    "    employees_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 101), (5,\"Eve\", 104)]\n",
    "    employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
    "    departments_data = [(101, \"Eng\"), (102, \"Sales\"), (103, \"HR\")]\n",
    "    departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "    # --- Scenario 1: Default Join (Likely Shuffle) ---\n",
    "    print(\"\\n--- Plan for Default Join (Shuffle Expected) ---\")\n",
    "    joined_df_shuffle = employees_df.join(departments_df, on=\"dept_id\", how=\"inner\") \\\n",
    "                                   .filter(col(\"emp_id\") > 1) \\\n",
    "                                   .select(\"name\", \"dept_name\")\n",
    "    joined_df_shuffle.explain()\n",
    "    # Look for: SortMergeJoin or ShuffledHashJoin, Exchange operators\n",
    "\n",
    "    # --- Scenario 2: Broadcast Join Hint ---\n",
    "    print(\"\\n--- Plan for Broadcast Join ---\")\n",
    "    joined_df_broadcast = employees_df.join(broadcast(departments_df), on=\"dept_id\", how=\"inner\") \\\n",
    "                                     .filter(col(\"emp_id\") > 1) \\\n",
    "                                     .select(\"name\", \"dept_name\")\n",
    "    joined_df_broadcast.explain(extended=True) # Use extended=True for more details\n",
    "    # Look for: BroadcastHashJoin, BroadcastExchange (for the small table)\n",
    "    # Notice NO Exchange on the larger (employee) side for the join itself.\n",
    "\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "*   **Explanation:**\n",
    "    *   We disable automatic broadcasting (`autoBroadcastJoinThreshold=-1`) to force a shuffle join initially.\n",
    "    *   The `explain()` output for `joined_df_shuffle` will likely show `SortMergeJoin` (or `ShuffledHashJoin`) and associated `Exchange` operations, indicating data is being shuffled based on `dept_id`.\n",
    "    *   In the second scenario, we explicitly add `broadcast(departments_df)`. The `explain()` output for `joined_df_broadcast` should now clearly show `BroadcastHashJoin`. You'll see a `BroadcastExchange` operation responsible for distributing the small `departments_df`, but the potentially large `employees_df` avoids a shuffle for the join itself.\n",
    "    *   `explain(extended=True)` provides all plan levels (Parsed, Analyzed, Optimized, Physical).\n",
    "\n",
    "*   **Use Case:** Essential for *diagnosing* performance issues, *validating* optimizations (did my broadcast hint work? did filters get pushed down?), and understanding *how* Spark is executing a query. It's the primary tool for understanding the impact of code changes on execution strategy.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary & Further Steps:**\n",
    "\n",
    "Performance tuning in PySpark is an iterative process involving understanding the execution model, identifying bottlenecks, and applying appropriate optimization techniques. Key strategies include:\n",
    "\n",
    "1.  **Optimize I/O:** Use efficient columnar formats (Parquet), define schemas manually, and leverage storage partitioning with predicate pushdown.\n",
    "2.  **Minimize Shuffles:** Prefer shuffle-optimized operations (`reduceByKey`), use broadcast joins for small tables, filter data early, and use partitioning effectively in window functions.\n",
    "3.  **Use Caching Wisely:** Persist intermediate DataFrames that are reused, choosing appropriate storage levels, and remember to `unpersist`.\n",
    "4.  **Diagnose with `explain()`:** Regularly inspect physical execution plans to understand join strategies, shuffles, and filter application.\n",
    "\n",
    "Beyond these, monitoring the **Spark UI** is crucial for observing stage/task durations, shuffle read/write sizes, data skew, GC time, and executor resource usage, providing further insights for targeted tuning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
