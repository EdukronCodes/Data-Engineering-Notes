{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7562236c",
   "metadata": {},
   "source": [
    "# Lesson 10 - Working with Complex and Nested Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984080fb",
   "metadata": {},
   "source": [
    "Okay, here are detailed technical notes on working with Complex and Nested Data in PySpark, suitable for professional learners and training material.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Working with Complex and Nested Data\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Modern data sources, particularly semi-structured formats like JSON and Avro, often contain complex data types. These include nested structures (structs), lists (arrays), and key-value pairs (maps). PySpark provides powerful capabilities to natively handle these complex types within its DataFrame API, allowing for sophisticated querying and manipulation without needing to manually parse text or rely heavily on User-Defined Functions (UDFs).\n",
    "\n",
    "Understanding how to effectively read, query, transform, and restructure these nested formats is crucial for working with real-world data originating from web APIs, NoSQL databases, event streams, and configuration files. These notes explore PySpark's features for managing `StructType`, `ArrayType`, and `MapType` columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Working with JSON and StructType\n",
    "\n",
    "**Theory**\n",
    "\n",
    "JSON (JavaScript Object Notation) is a ubiquitous format for data interchange, often featuring nested objects and arrays. PySpark can directly ingest JSON data into DataFrames. When reading JSON, Spark can either:\n",
    "\n",
    "1.  **Infer the Schema:** Spark reads a sample of the JSON data (configurable) to automatically determine the structure, including nested objects (`StructType`), lists (`ArrayType`), and data types. While convenient for exploration, schema inference can be slow for large datasets, might miss fields present only later in the file, and can infer incorrect types (e.g., inferring `LongType` when `DoubleType` is needed later).\n",
    "2.  **Use an Explicit Schema:** Defining the schema explicitly using `StructType` and `StructField` provides robustness, performance benefits, and data integrity. Spark skips the inference step, validates data against the schema during read, and ensures correct data types. `StructType` represents a row or a nested object, containing a list of `StructField` objects. Each `StructField` defines the name, data type (`StringType`, `IntegerType`, `ArrayType(ElementType())`, `MapType(KeyType(), ValueType())`, `StructType(...)`, etc.), and nullability of a column or nested field.\n",
    "\n",
    "Accessing fields within a `StructType` column is done using dot (`.`) notation or the `getField()` method within expressions.\n",
    "\n",
    "**Code Examples**\n",
    "\n",
    "Let's assume we have a JSON file (`data.json`) or a list of JSON strings:\n",
    "\n",
    "```json\n",
    "// Sample JSON data (imagine this in a file or RDD)\n",
    "{\"id\": 1, \"name\": \"Alice\", \"address\": {\"street\": \"123 Main St\", \"city\": \"Anytown\"}, \"roles\": [\"Admin\", \"Editor\"], \"attributes\": {\"level\": 10, \"status\": \"Active\"}}\n",
    "{\"id\": 2, \"name\": \"Bob\", \"address\": {\"street\": \"456 Oak Ave\", \"city\": \"Otherville\"}, \"roles\": [\"Viewer\"], \"attributes\": {\"level\": 5, \"status\": \"Inactive\", \"validated\": false}}\n",
    "{\"id\": 3, \"name\": \"Charlie\", \"address\": null, \"roles\": [\"Editor\", \"Viewer\"], \"attributes\": null}\n",
    "```\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, BooleanType\n",
    "from pyspark.sql.functions import col, get_json_object, json_tuple\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ComplexJson\").getOrCreate()\n",
    "\n",
    "# Sample data as a list of strings (simulating reading lines from a file)\n",
    "json_strings = [\n",
    "    '{\"id\": 1, \"name\": \"Alice\", \"address\": {\"street\": \"123 Main St\", \"city\": \"Anytown\"}, \"roles\": [\"Admin\", \"Editor\"], \"attributes\": {\"level\": 10, \"status\": \"Active\"}}',\n",
    "    '{\"id\": 2, \"name\": \"Bob\", \"address\": {\"street\": \"456 Oak Ave\", \"city\": \"Otherville\"}, \"roles\": [\"Viewer\"], \"attributes\": {\"level\": 5, \"status\": \"Inactive\", \"validated\": false}}',\n",
    "    '{\"id\": 3, \"name\": \"Charlie\", \"address\": null, \"roles\": [\"Editor\", \"Viewer\"], \"attributes\": null}'\n",
    "]\n",
    "json_rdd = spark.sparkContext.parallelize(json_strings)\n",
    "\n",
    "# ----- Method 1: Reading JSON with Schema Inference -----\n",
    "print(\"Reading with Schema Inference:\")\n",
    "df_inferred = spark.read.json(json_rdd)\n",
    "df_inferred.printSchema()\n",
    "df_inferred.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `spark.sparkContext.parallelize(json_strings)`: Creates an RDD from the list of JSON strings.\n",
    "# 2. `spark.read.json(json_rdd)`: Reads the JSON data. Since no schema is provided, Spark infers it by sampling the data. It correctly identifies nested structures ('address', 'attributes') and arrays ('roles').\n",
    "# 3. `df_inferred.printSchema()`: Displays the inferred schema structure. Notice `address` is `StructType`, `attributes` is `StructType`, and `roles` is `ArrayType`.\n",
    "\n",
    "# ----- Method 2: Defining an Explicit Schema -----\n",
    "print(\"\\nDefining an Explicit Schema:\")\n",
    "explicit_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"roles\", ArrayType(StringType()), True),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True) # Infer schema guessed struct, let's define as map\n",
    "    # Note: Original data had mixed types in attributes (int, string, bool). MapType requires consistent value types.\n",
    "    # If types truly vary, reading as Map<String, String> and casting later, or keeping as Struct is better.\n",
    "    # For this example, let's *assume* we want them treated as strings in the map for simplicity here,\n",
    "    # or better yet, adjust the schema if it should be a Struct.\n",
    "])\n",
    "\n",
    "# Let's redefine schema for attributes based on data:\n",
    "explicit_schema_corrected = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"roles\", ArrayType(StringType()), True),\n",
    "    StructField(\"attributes\", StructType([ # More accurate than MapType for this specific data\n",
    "         StructField(\"level\", IntegerType(), True),\n",
    "         StructField(\"status\", StringType(), True),\n",
    "         StructField(\"validated\", BooleanType(), True) # Field only present in some records\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "\n",
    "print(\"\\nReading with Explicit Schema:\")\n",
    "df_explicit = spark.read.schema(explicit_schema_corrected).json(json_rdd)\n",
    "df_explicit.printSchema()\n",
    "df_explicit.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `StructType([...])`: Defines the overall structure of the DataFrame.\n",
    "# 2. `StructField(\"name\", DataType(), nullable)`: Defines each field within a StructType.\n",
    "# 3. `StructType([...])`: Used nestedly for the 'address' and 'attributes' fields.\n",
    "# 4. `ArrayType(StringType())`: Defines the 'roles' field as an array of strings.\n",
    "# 5. `MapType(StringType(), StringType())`: Defines 'attributes' as a map with string keys and string values (original attempt). Corrected to StructType based on data.\n",
    "# 6. `spark.read.schema(explicit_schema_corrected).json(json_rdd)`: Reads the JSON using the *provided* schema. This is generally faster and safer. It enforces the structure; non-conforming data might result in nulls or errors depending on options.\n",
    "\n",
    "# ----- Accessing Nested Fields -----\n",
    "print(\"\\nAccessing Nested Fields:\")\n",
    "df_selected = df_explicit.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"address.street\").alias(\"street_address\"), # Dot notation\n",
    "    col(\"address\").getField(\"city\").alias(\"city_name\"), # getField() method\n",
    "    col(\"roles\")[0].alias(\"first_role\"), # Access array element by index\n",
    "    col(\"attributes.level\").alias(\"attribute_level\"), # Dot notation for nested struct\n",
    "    col(\"attributes\")[\"status\"].alias(\"attribute_status\") # Map-like access for Struct field\n",
    ")\n",
    "df_selected.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `col(\"address.street\")`: Accesses the 'street' field within the 'address' struct using dot notation.\n",
    "# 2. `col(\"address\").getField(\"city\")`: Achieves the same using the `getField()` method, which can be useful if field names contain dots or special characters.\n",
    "# 3. `.alias(...)`: Renames the selected field for clarity in the resulting DataFrame.\n",
    "# 4. `col(\"roles\")[0]`: Accesses the first element (index 0) of the 'roles' array.\n",
    "# 5. `col(\"attributes.level\")`: Accesses the 'level' field within the nested 'attributes' struct.\n",
    "# 6. `col(\"attributes\")[\"status\"]`: Uses map-like key access notation to retrieve the 'status' field within the 'attributes' struct. Both dot notation and key access work for StructFields. For MapType, only key access works.\n",
    "```\n",
    "\n",
    "**Practical Use Cases & Performance:**\n",
    "\n",
    "*   **Explicit Schemas:** Strongly recommended for production pipelines. They prevent errors from schema drift, improve read performance (no inference step), and ensure data type consistency. Use schema inference primarily for initial exploration or when the schema is highly variable and cannot be predefined (though even then, reading as raw text and parsing might be safer).\n",
    "*   **Schema Evolution:** For evolving JSON schemas, Spark's schema merging (`spark.read.option(\"mergeSchema\", \"true\").json(...)`) can be useful, but it adds overhead. Defining a superset schema or handling schema differences explicitly is often more robust. The `schema_of_json` function can help extract a schema from JSON strings already loaded in a DataFrame column.\n",
    "*   **Complex Queries:** Dot notation and `getField` allow complex filtering and transformations directly on nested fields (e.g., `df.filter(col(\"address.city\") == \"Anytown\")`).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Flattening Nested Columns\n",
    "\n",
    "**Theory**\n",
    "\n",
    "While Spark handles nested structures well, sometimes it's necessary to \"flatten\" them. Flattening transforms nested fields into top-level columns. This might be required for:\n",
    "\n",
    "*   Compatibility with systems or tools that don't support nested types (e.g., traditional relational databases, some BI tools).\n",
    "*   Simplifying analysis or feature engineering for certain machine learning models.\n",
    "*   Improving human readability of tabular data.\n",
    "\n",
    "The most common way to flatten a `StructType` column is to select each nested field individually and assign it a unique top-level alias.\n",
    "\n",
    "**Code Examples**\n",
    "\n",
    "Using `df_explicit` from the previous section:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Original Nested DataFrame:\")\n",
    "df_explicit.show(truncate=False)\n",
    "\n",
    "# ----- Flattening the 'address' struct -----\n",
    "print(\"\\nFlattening 'address' struct:\")\n",
    "df_flat_address = df_explicit.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"address.street\").alias(\"address_street\"), # Select nested field and alias\n",
    "    col(\"address.city\").alias(\"address_city\"),   # Select nested field and alias\n",
    "    col(\"roles\"),\n",
    "    col(\"attributes\")\n",
    ")\n",
    "df_flat_address.printSchema()\n",
    "df_flat_address.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `df_explicit.select(...)`: Selects specific columns to create the new DataFrame.\n",
    "# 2. `col(\"address.street\").alias(\"address_street\")`: Selects the 'street' field from within 'address' and gives it the new top-level name 'address_street'.\n",
    "# 3. `col(\"address.city\").alias(\"address_city\")`: Does the same for the 'city' field.\n",
    "# 4. Other columns ('id', 'name', 'roles', 'attributes') are selected directly.\n",
    "\n",
    "# ----- Flattening all nested structures ('address' and 'attributes') -----\n",
    "print(\"\\nFlattening all nested structures ('address', 'attributes'):\")\n",
    "# Be careful with potential name collisions if nested fields have same names.\n",
    "df_fully_flattened = df_explicit.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"address.street\").alias(\"address_street\"),\n",
    "    col(\"address.city\").alias(\"address_city\"),\n",
    "    col(\"roles\"),\n",
    "    col(\"attributes.level\").alias(\"attr_level\"), # Alias to avoid potential future collisions\n",
    "    col(\"attributes.status\").alias(\"attr_status\"),\n",
    "    col(\"attributes.validated\").alias(\"attr_validated\")\n",
    "    # Note: We deliberately skip the original 'address' and 'attributes' columns.\n",
    ")\n",
    "df_fully_flattened.printSchema()\n",
    "df_fully_flattened.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. We now select fields from both 'address' and 'attributes' structs.\n",
    "# 2. Aliases like 'attr_level' are used. Good practice to ensure unique and descriptive top-level names, especially if nested structures might share field names (e.g., 'id' inside 'address' and 'id' at top level).\n",
    "# 3. The original struct columns ('address', 'attributes') are *not* included in the select list, effectively removing them and replacing them with their flattened contents.\n",
    "\n",
    "# ----- Dynamic Flattening (Conceptual - Requires helper function usually) -----\n",
    "# For schemas with many nested fields, manual selection is tedious.\n",
    "# You might write a helper function to generate the select expressions dynamically.\n",
    "def flatten_struct_cols(df, struct_col_name):\n",
    "    \"\"\" Helper to generate select expressions for flattening one struct column \"\"\"\n",
    "    field_names = [f.name for f in df.schema[struct_col_name].dataType.fields]\n",
    "    select_exprs = [f\"{struct_col_name}.{name} as {struct_col_name}_{name}\" for name in field_names]\n",
    "    # Include other columns\n",
    "    other_cols = [c for c in df.columns if c != struct_col_name]\n",
    "    return df.selectExpr(other_cols + select_exprs)\n",
    "\n",
    "print(\"\\nFlattening 'address' dynamically (using a helper concept):\")\n",
    "df_flat_dynamic = flatten_struct_cols(df_explicit, \"address\")\n",
    "df_flat_dynamic.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. This section demonstrates the *idea* of dynamic flattening. A real implementation would need more robust recursion for multi-level nesting and better name collision handling.\n",
    "# 2. `df.schema[struct_col_name].dataType.fields`: Accesses the schema definition of the struct column to get its field names.\n",
    "# 3. `selectExpr(...)`: Uses SQL-like expressions generated dynamically to perform the flattening.\n",
    "```\n",
    "\n",
    "**Practical Use Cases & Performance:**\n",
    "\n",
    "*   **Data Warehousing:** Flattening is common when loading data into traditional DWHs with flat table structures.\n",
    "*   **Feature Engineering:** Creating individual features from nested attributes for ML models.\n",
    "*   **Performance:** Flattening itself is primarily a projection operation (`select`), which is generally efficient in Spark. However, it increases the number of columns, which might slightly impact the performance of subsequent operations and increase metadata size. Keeping data nested can sometimes be more efficient for queries that only need a subset of nested fields, especially with columnar storage formats like Parquet where entire structs can be skipped (column pruning). Choose based on downstream requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Exploding Arrays and Maps\n",
    "\n",
    "**Theory**\n",
    "\n",
    "`ArrayType` and `MapType` columns store multiple values within a single row field. \"Exploding\" these columns transforms the DataFrame so that each element in the array or map gets its own row, duplicating the other column values for each new row.\n",
    "\n",
    "*   `explode(col)`: For arrays, creates a new row for each element in the array, placing the element in a new column named `col` by default. For maps, creates a new row for each key-value pair, creating two new columns named `key` and `value` by default. If the array/map is null or empty, the original row disappears from the result (like an inner join).\n",
    "*   `explode_outer(col)`: Similar to `explode`, but if the array/map is null or empty, it keeps the original row and produces `null` in the new column(s). (Like a left outer join).\n",
    "*   `posexplode(col)`: For arrays only. Similar to `explode`, but adds an additional column (default name `pos`) indicating the position (index) of the element in the original array.\n",
    "*   `posexplode_outer(col)`: Outer version of `posexplode`.\n",
    "\n",
    "Exploding is essential when you need to work with individual array elements or map entries as separate records, for example, to join them with other data, perform aggregations per element, or filter based on individual element values.\n",
    "\n",
    "**Code Examples**\n",
    "\n",
    "Using `df_explicit` which contains an array ('roles') and a struct that we read as a map in the first attempt ('attributes'). Let's use `df_explicit` for exploding the array and create a separate example for maps.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode, explode_outer, posexplode, map_keys, map_values\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df_explicit.select(\"id\", \"name\", \"roles\", \"attributes\").show(truncate=False)\n",
    "\n",
    "# ----- Exploding the 'roles' array -----\n",
    "print(\"\\nExploding 'roles' array (explode):\")\n",
    "df_exploded_roles = df_explicit.withColumn(\"role\", explode(col(\"roles\")))\n",
    "# or: df_exploded_roles = df_explicit.select(\"id\", \"name\", \"attributes\", explode(col(\"roles\")).alias(\"role\"))\n",
    "df_exploded_roles.select(\"id\", \"name\", \"role\", \"attributes\").show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `explode(col(\"roles\"))`: Takes the 'roles' array column as input.\n",
    "# 2. `withColumn(\"role\", ...)`: Adds a new column named 'role' containing the exploded element.\n",
    "# 3. Notice how rows with id 1 and 3 are duplicated, one for each role in their original 'roles' array. The original 'roles' column is typically dropped or ignored afterwards.\n",
    "\n",
    "print(\"\\nExploding 'roles' array (explode_outer):\")\n",
    "# Let's add a row with null/empty roles to see the difference\n",
    "data_with_empty_roles = json_strings + ['{\"id\": 4, \"name\": \"Dave\", \"address\": null, \"roles\": [], \"attributes\": null}']\n",
    "rdd_with_empty = spark.sparkContext.parallelize(data_with_empty_roles)\n",
    "df_with_empty = spark.read.schema(explicit_schema_corrected).json(rdd_with_empty)\n",
    "\n",
    "df_exploded_outer_roles = df_with_empty.withColumn(\"role\", explode_outer(col(\"roles\")))\n",
    "df_exploded_outer_roles.select(\"id\", \"name\", \"role\", \"attributes\").show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `explode_outer(col(\"roles\"))`: Works like explode, but for row id 4 where 'roles' is empty, the row is kept, and the new 'role' column has a null value. If 'roles' itself was null, the result would be the same.\n",
    "\n",
    "print(\"\\nExploding 'roles' array with position (posexplode):\")\n",
    "df_posexploded_roles = df_explicit.select(\n",
    "    col(\"id\"), col(\"name\"), posexplode(col(\"roles\")).alias(\"pos\", \"role\") # Alias multiple output columns\n",
    ")\n",
    "df_posexploded_roles.show(truncate=False)\n",
    "# Explanation:\n",
    "# 1. `posexplode(col(\"roles\"))`: Explodes the array and adds a position column.\n",
    "# 2. `.alias(\"pos\", \"role\")`: Renames the default output columns (`pos`, `col`) to `pos` and `role`.\n",
    "\n",
    "# ----- Example with Maps -----\n",
    "map_data = [(1, {\"a\": \"apple\", \"b\": \"banana\"}), (2, {\"c\": \"carrot\"}), (3, {})]\n",
    "map_df = spark.createDataFrame(map_data, [\"id\", \"data_map\"])\n",
    "map_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nExploding a Map column:\")\n",
    "df_exploded_map = map_df.select(col(\"id\"), explode(col(\"data_map\"))) # Default columns: key, value\n",
    "df_exploded_map.show()\n",
    "# Explanation:\n",
    "# 1. `explode(col(\"data_map\"))`: Explodes the map column.\n",
    "# 2. It creates two new columns, 'key' and 'value', by default. Row 1 is duplicated for keys 'a' and 'b'. Row 3 (empty map) disappears.\n",
    "\n",
    "print(\"\\nExploding a Map column (explode_outer):\")\n",
    "df_exploded_outer_map = map_df.select(col(\"id\"), explode_outer(col(\"data_map\")))\n",
    "df_exploded_outer_map.show()\n",
    "# Explanation:\n",
    "# 1. `explode_outer(col(\"data_map\"))`: Explodes the map, but keeps row 3, placing nulls in the 'key' and 'value' columns because the original map was empty.\n",
    "```\n",
    "\n",
    "**Practical Use Cases & Performance:**\n",
    "\n",
    "*   **Unnesting Data:** Converting lists of items (e.g., products in an order, tags on an article) into individual rows for analysis or joining.\n",
    "*   **Processing Key-Value Pairs:** Handling flexible attribute maps where keys represent attribute names and values represent their values.\n",
    "*   **Performance:** Exploding significantly increases the number of rows in the DataFrame. This can dramatically increase the amount of data shuffled in subsequent operations like joins or aggregations. If the arrays/maps have highly variable sizes, exploding can lead to data skew (some tasks processing vastly more rows than others). Use `explode_outer` if you need to preserve rows with empty/null complex types. Consider whether the explosion is necessary or if functions operating directly on arrays/maps (`array_contains`, `transform`, `map_filter`, etc.) can achieve the goal more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "PySpark's built-in support for complex data types (`StructType`, `ArrayType`, `MapType`) is a cornerstone of its ability to handle diverse, real-world data. Understanding how to define schemas for, query, flatten, and explode these types using functions like `col`, `getField`, `explode`, and `posexplode` is essential for data engineers and analysts. While these operations provide immense flexibility, always consider the performance implications, especially regarding schema inference versus explicit schemas and the potential data volume increase caused by flattening or exploding nested structures. Choosing the right approach depends on the specific data characteristics and the requirements of the downstream processing steps.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
