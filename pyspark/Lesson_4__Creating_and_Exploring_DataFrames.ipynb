{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29d897b",
   "metadata": {},
   "source": [
    "# Lesson 4 - Creating and Exploring DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fb1e1",
   "metadata": {},
   "source": [
    "Okay, let's craft the detailed technical notes for Lesson 4, focusing on PySpark DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "**Technical Notes: PySpark DataFrame Fundamentals**\n",
    "\n",
    "**Objective:** These notes delve into the PySpark DataFrame API, the primary interface for working with structured and semi-structured data in modern Spark applications. We will cover DataFrame creation from various sources, understanding and managing schemas, and performing fundamental data manipulation operations.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Introduction to DataFrames**\n",
    "\n",
    "*   **Theory:**\n",
    "    A PySpark DataFrame is a **distributed, immutable collection of data organized into named columns**, conceptually equivalent to a table in a relational database or a data frame in R/pandas, but with powerful optimizations for distributed processing. It's built on top of RDDs but provides a higher-level abstraction with richer semantics.\n",
    "\n",
    "    **Key Advantages over RDDs for Structured Data:**\n",
    "    1.  **Schema:** DataFrames enforce a schema, a defined structure specifying column names and data types. This allows Spark to understand the data layout.\n",
    "    2.  **Optimization (Catalyst Optimizer):** Spark leverages the schema and high-level operations (like `select`, `filter`, `groupBy`) to perform sophisticated query optimization through its Catalyst optimizer. Catalyst creates logical and physical execution plans, applying rules like predicate pushdown, column pruning, and join reordering to significantly improve performance.\n",
    "    3.  **Tungsten Execution Engine:** Spark executes DataFrame operations using the Tungsten engine, which optimizes memory usage (off-heap storage) and CPU efficiency (whole-stage code generation).\n",
    "    4.  **Unified API:** Provides seamless integration between Python, Scala, Java, R, and SQL for data manipulation.\n",
    "\n",
    "    The primary entry point for DataFrame operations is the `SparkSession` object, typically named `spark`.\n",
    "\n",
    "*   **Architecture Context:**\n",
    "    While RDDs form the base, DataFrame operations are translated by Catalyst into optimized RDD operations. This abstraction layer allows developers to focus on *what* they want to compute, letting Spark figure out the most efficient *how*.\n",
    "\n",
    "    ```\n",
    "      +-----------------------+\n",
    "      | DataFrame API (Python)| ----> User Code\n",
    "      +-----------------------+\n",
    "                | (Logical Plan)\n",
    "      +-----------------------+\n",
    "      |   Catalyst Optimizer  | ----> Optimization Rules, Cost Models\n",
    "      +-----------------------+\n",
    "                | (Optimized Logical & Physical Plans)\n",
    "      +-----------------------+\n",
    "      | Tungsten Execution    | ----> Code Generation, Off-Heap Memory\n",
    "      +-----------------------+\n",
    "                | (RDD Operations)\n",
    "      +-----------------------+\n",
    "      |      Spark Core (RDDs)| ----> Cluster Execution\n",
    "      +-----------------------+\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Creating DataFrames from External Sources**\n",
    "\n",
    "*   **Theory:**\n",
    "    The most common way to create DataFrames is by reading data from external storage systems. PySpark's `DataFrameReader` interface, accessed via `spark.read`, provides methods to load data from various formats. The reader can often infer the schema, but providing an explicit schema is recommended for production robustness and performance.\n",
    "\n",
    "*   **a) Reading from CSV Files**\n",
    "    *   **Context:** Comma-Separated Values (CSV) files are ubiquitous for storing tabular data in plain text. However, they lack explicit type information and can have variations in delimiters, quoting, and header presence.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        import os\n",
    "\n",
    "        # Initialize SparkSession\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"DataFrameFromCSV\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        # Create a dummy CSV file for demonstration\n",
    "        csv_file_path = \"users.csv\"\n",
    "        with open(csv_file_path, \"w\") as f:\n",
    "            f.write(\"id,name,age,country\\n\")\n",
    "            f.write(\"1,Alice,30,USA\\n\")\n",
    "            f.write(\"2,Bob,25,Canada\\n\")\n",
    "            f.write(\"3,Charlie,35,USA\\n\")\n",
    "            f.write(\"4,David,,UK\\n\") # Missing age\n",
    "            f.write('5,\"Eve, Jr.\",40,Canada\\n') # Name with comma\n",
    "\n",
    "        # --- Reading CSV ---\n",
    "\n",
    "        # Option 1: Basic read with header and schema inference\n",
    "        print(\"--- Reading CSV with Header & Inference ---\")\n",
    "        df_infer = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "        df_infer.show()\n",
    "        df_infer.printSchema()\n",
    "\n",
    "        # Option 2: More robust read with specific options\n",
    "        print(\"\\n--- Reading CSV with Specific Options ---\")\n",
    "        df_options = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .option(\"sep\", \",\") \\\n",
    "            .option(\"nullValue\", \"\") \\\n",
    "            .option(\"quote\", \"\\\"\") \\\n",
    "            .option(\"escape\", \"\\\"\") \\\n",
    "            .load(csv_file_path) # Use .load() when using .format() or multiple options\n",
    "\n",
    "        # Note: Without inferSchema=True, all columns will be string type initially\n",
    "        df_options.show()\n",
    "        df_options.printSchema()\n",
    "\n",
    "        # Clean up the dummy file\n",
    "        os.remove(csv_file_path)\n",
    "\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `spark.read.csv(path, ...)`: The primary method for reading CSV. Takes the path and various options as keyword arguments.\n",
    "        *   `header=True`: Instructs Spark to use the first line as column names. Default is `False`.\n",
    "        *   `inferSchema=True`: Tells Spark to sample the data and guess the data type for each column. Can be slow and potentially inaccurate, especially with nulls or inconsistent data. Default is `False`.\n",
    "        *   `spark.read.format(\"csv\")`: An alternative way to specify the format, useful when chaining multiple `.option()` calls.\n",
    "        *   `.option(\"sep\", \",\")`: Specifies the delimiter character (default is comma).\n",
    "        *   `.option(\"nullValue\", \"\")`: Defines the string representation of null values in the CSV (here, empty strings).\n",
    "        *   `.option(\"quote\", \"\\\"\")`: Specifies the character used for quoting fields that may contain the delimiter (default is `\"`).\n",
    "        *   `.option(\"escape\", \"\\\"\")`: Specifies the character used to escape quotes within a quoted field (often the quote character itself).\n",
    "        *   `.load(path)`: Loads the data from the specified path after configuring options.\n",
    "        *   `df.show()`: An action that displays the first 20 rows of the DataFrame in a tabular format.\n",
    "        *   `df.printSchema()`: An action that prints the DataFrame's schema (column names and inferred/defined types). Notice how `inferSchema=False` results in all `string` types.\n",
    "    *   **Use Case:** Ingesting data exports from spreadsheets, databases, or simple log files where data is tabular and text-based. Be mindful of the parsing options required for reliable loading.\n",
    "\n",
    "*   **b) Reading from JSON Files**\n",
    "    *   **Context:** JavaScript Object Notation (JSON) is a common format for semi-structured data, often used in web APIs and configuration files. Spark typically expects **line-delimited JSON** (one valid JSON object per line) for efficient parallel reading. It can handle nested structures and arrays.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        import os\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"DataFrameFromJSON\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        # Create a dummy line-delimited JSON file\n",
    "        json_file_path = \"events.json\"\n",
    "        with open(json_file_path, \"w\") as f:\n",
    "            f.write('{\"timestamp\": \"2023-10-27T10:00:00Z\", \"user_id\": 1, \"event\": \"login\", \"details\": {\"ip\": \"192.168.1.1\"}}\\n')\n",
    "            f.write('{\"timestamp\": \"2023-10-27T10:05:00Z\", \"user_id\": 2, \"event\": \"view_page\", \"details\": {\"page\": \"/home\"}}\\n')\n",
    "            f.write('{\"timestamp\": \"2023-10-27T10:10:00Z\", \"user_id\": 1, \"event\": \"logout\"}\\n') # Missing details field\n",
    "\n",
    "        # --- Reading JSON ---\n",
    "        print(\"--- Reading Line-Delimited JSON ---\")\n",
    "        df_json = spark.read.json(json_file_path)\n",
    "        # Schema is typically inferred for JSON\n",
    "        df_json.show(truncate=False)\n",
    "        df_json.printSchema()\n",
    "\n",
    "        # Example for multi-line JSON (less common for large datasets)\n",
    "        # multi_line_json_path = \"single_object.json\"\n",
    "        # with open(multi_line_json_path, \"w\") as f:\n",
    "        #     f.write('[{\"id\": 1}, {\"id\": 2}]') # A single JSON array across multiple lines\n",
    "        # df_multi = spark.read.option(\"multiLine\", True).json(multi_line_json_path)\n",
    "        # os.remove(multi_line_json_path)\n",
    "\n",
    "        os.remove(json_file_path)\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `spark.read.json(path)`: Reads line-delimited JSON files. Schema inference is the default and generally works well due to JSON's self-describing nature.\n",
    "        *   `truncate=False` (in `show()`): Prevents truncating long column values in the output display. Useful for inspecting nested structures.\n",
    "        *   `printSchema()` output: Notice how Spark correctly infers nested structures (`details` as a `struct`) and handles missing fields (nullable).\n",
    "        *   `option(\"multiLine\", True)`: Required if the entire file represents a single JSON object or array spanning multiple lines. This is less scalable as the entire file might need to be read by a single node.\n",
    "    *   **Use Case:** Processing data from web APIs, application logs formatted in JSON, data exports from NoSQL databases like MongoDB.\n",
    "\n",
    "*   **c) Reading from Parquet Files**\n",
    "    *   **Context:** Apache Parquet is a **columnar storage format** optimized for analytics workloads in the Hadoop ecosystem. It's highly efficient for Spark.\n",
    "        *   **Columnar Storage:** Data for each column is stored contiguously, leading to better compression ratios and allowing queries to read only the required columns (column pruning).\n",
    "        *   **Schema Evolution:** Stores the schema within the data files, making it self-describing.\n",
    "        *   **Predicate Pushdown:** Allows Spark to filter rows directly at the storage level based on query predicates, minimizing data read from disk/network.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        import os\n",
    "        import shutil # To remove directory\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"DataFrameFromParquet\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        # --- First, create a Parquet file using a DataFrame ---\n",
    "        data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
    "        columns = [\"name\", \"age\"]\n",
    "        df_source = spark.createDataFrame(data, columns)\n",
    "\n",
    "        parquet_dir_path = \"people.parquet\"\n",
    "        print(f\"--- Writing DataFrame to Parquet: {parquet_dir_path} ---\")\n",
    "        # Parquet is typically written to a directory, not a single file\n",
    "        df_source.write.mode(\"overwrite\").parquet(parquet_dir_path)\n",
    "        print(\"Write complete.\")\n",
    "\n",
    "        # --- Reading Parquet ---\n",
    "        print(f\"\\n--- Reading Parquet from: {parquet_dir_path} ---\")\n",
    "        df_parquet = spark.read.parquet(parquet_dir_path)\n",
    "\n",
    "        df_parquet.show()\n",
    "        df_parquet.printSchema() # Schema is read directly from Parquet metadata\n",
    "\n",
    "        # Clean up the dummy directory\n",
    "        shutil.rmtree(parquet_dir_path)\n",
    "\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `df_source = spark.createDataFrame(...)`: Creates a sample DataFrame to write out.\n",
    "        *   `df_source.write.mode(\"overwrite\").parquet(path)`: Writes the DataFrame to Parquet format.\n",
    "            *   `write`: Accesses the `DataFrameWriter` interface.\n",
    "            *   `mode(\"overwrite\")`: Specifies the behavior if the path already exists. Other modes include `append`, `ignore`, `errorifexists` (default).\n",
    "            *   `parquet(path)`: Specifies the format and output path (usually a directory).\n",
    "        *   `spark.read.parquet(path)`: Reads data from a Parquet file or directory. Schema inference (`inferSchema`) is **not** needed as the schema is stored within the Parquet files' metadata. This makes reading faster and more reliable.\n",
    "    *   **Use Case:** **Highly recommended format** for storing intermediate data in Spark pipelines, building data lakes, and achieving optimal performance for analytical queries. It's the de facto standard in many big data environments.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Schema: Inference vs. Manual Definition**\n",
    "\n",
    "*   **Theory:**\n",
    "    The schema defines the structure of your DataFrame. Getting it right is crucial for data integrity and performance.\n",
    "    *   **Schema Inference:** Convenient for initial exploration (`inferSchema=True`). Spark reads a portion of the data to guess column types.\n",
    "        *   **Pros:** Easy, less code initially.\n",
    "        *   **Cons:**\n",
    "            *   Can be slow (requires an extra pass over data).\n",
    "            *   May infer incorrect types (e.g., inferring `int` when `long` is needed, misinterpreting dates, treating everything as `string` if ambiguity or nulls dominate the sample).\n",
    "            *   Potential for runtime errors if data outside the sample doesn't match the inferred schema.\n",
    "            *   **Generally not recommended for production ETL pipelines.**\n",
    "    *   **Manual Schema Definition:** Explicitly defining the schema using PySpark's data types.\n",
    "        *   **Pros:**\n",
    "            *   **Reliability:** Ensures data conforms to expected types. Errors occur predictably if data mismatches.\n",
    "            *   **Performance:** Avoids the schema inference pass, speeding up reads.\n",
    "            *   **Clarity:** Documents the expected data structure.\n",
    "        *   **Cons:** Requires more upfront code.\n",
    "\n",
    "*   **Code Example: Defining and Applying a Manual Schema**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "    import os\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"ManualSchema\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "    # Create a dummy CSV file (potentially with ambiguous data)\n",
    "    csv_file_path = \"products.csv\"\n",
    "    with open(csv_file_path, \"w\") as f:\n",
    "        f.write(\"product_id,name,price,stock_count,last_updated\\n\")\n",
    "        f.write(\"P100,Widget A,99.99,50,2023-10-27 10:00:00\\n\")\n",
    "        f.write(\"P200,Gadget B,145.00,,2023-10-26 15:30:00\\n\") # Empty stock_count\n",
    "        f.write(\"P300,Thingamajig,,10,2023-10-27 11:00:00\\n\") # Empty price\n",
    "\n",
    "    # Define the desired schema explicitly\n",
    "    # StructType: Represents the overall structure (list of fields)\n",
    "    # StructField: Defines a single column (name, dataType, nullable)\n",
    "    product_schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), True),  # Column name, data type, nullable flag\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),        # Use Double for currency\n",
    "        StructField(\"stock_count\", IntegerType(), True), # Use Integer for counts\n",
    "        StructField(\"last_updated\", TimestampType(), True) # Use Timestamp for date/time\n",
    "    ])\n",
    "\n",
    "    print(\"--- Reading CSV with Manual Schema ---\")\n",
    "    df_manual = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"nullValue\", \"\") \\\n",
    "        .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "        .schema(product_schema) \\\n",
    "        .load(csv_file_path)\n",
    "\n",
    "    df_manual.show()\n",
    "    df_manual.printSchema() # Schema matches our definition\n",
    "\n",
    "    os.remove(csv_file_path)\n",
    "    spark.stop()\n",
    "    ```\n",
    "*   **Explanation:**\n",
    "    *   `from pyspark.sql.types import ...`: Imports necessary classes for schema definition.\n",
    "    *   `StructType([...])`: Creates the schema object, taking a list of `StructField` objects.\n",
    "    *   `StructField(\"col_name\", DataType(), nullable)`: Defines each column:\n",
    "        *   `col_name`: The desired column name (string).\n",
    "        *   `DataType()`: The PySpark SQL data type (e.g., `StringType()`, `IntegerType()`, `DoubleType()`, `BooleanType()`, `DateType()`, `TimestampType()`, `ArrayType()`, `MapType()`, `StructType()` for nested structures).\n",
    "        *   `nullable`: A boolean indicating if the column can contain `null` values (usually `True` unless you have strong guarantees).\n",
    "    *   `.option(\"timestampFormat\", \"...\")`: Crucial when reading strings into `TimestampType` or `DateType` to specify the exact format for parsing. Use Java SimpleDateFormat patterns.\n",
    "    *   `.schema(product_schema)`: Applies the defined schema during the read operation. Spark will now enforce these types. If data cannot be parsed according to the type (e.g., \"abc\" into an `IntegerType`), it will typically result in `null` for that field (or potentially throw errors depending on parse modes).\n",
    "*   **Use Case:** Essential for production data pipelines, reading data with well-defined formats, ensuring type consistency, and improving read performance.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Basic DataFrame Operations**\n",
    "\n",
    "*   **Theory:**\n",
    "    Once a DataFrame is created, you can perform various operations to manipulate and analyze the data. These operations are generally **transformations**, meaning they are lazily evaluated and return *new* DataFrames, preserving immutability. Common functions are available directly as DataFrame methods or within `pyspark.sql.functions`.\n",
    "\n",
    "*   **a) Selecting Columns (`select`)**\n",
    "    *   **Context:** Used to choose a subset of columns, rename columns, or derive new columns based on existing ones. Analogous to the `SELECT` clause in SQL.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import col, lit, upper # Import column functions\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"SelectOperation\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        data = [(\"Alice\", 30, \"USA\"), (\"Bob\", 25, \"Canada\"), (\"Charlie\", 35, \"USA\")]\n",
    "        df = spark.createDataFrame(data, [\"name\", \"age\", \"country\"])\n",
    "        print(\"--- Original DataFrame ---\")\n",
    "        df.show()\n",
    "\n",
    "        # Select specific columns by name\n",
    "        print(\"\\n--- Selecting 'name' and 'country' ---\")\n",
    "        df.select(\"name\", \"country\").show()\n",
    "\n",
    "        # Select using col() function (recommended for clarity and complex expressions)\n",
    "        print(\"\\n--- Selecting using col() ---\")\n",
    "        df.select(col(\"name\"), col(\"age\")).show()\n",
    "\n",
    "        # Select with expressions and aliasing\n",
    "        print(\"\\n--- Selecting with expression and alias ---\")\n",
    "        df.select(\n",
    "            col(\"name\"),\n",
    "            (col(\"age\") + 5).alias(\"age_in_5_years\"), # Calculate and rename\n",
    "            upper(col(\"country\")).alias(\"country_upper\") # Apply function and rename\n",
    "        ).show()\n",
    "\n",
    "        # Select all columns (*)\n",
    "        # df.select(\"*\") # Less common in programmatic API but works\n",
    "\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `df.select(\"col1\", \"col2\", ...)`: Selects columns by passing their string names.\n",
    "        *   `from pyspark.sql.functions import col`: Imports the `col` function, which returns a `Column` object based on the name. Using `col()` is often clearer and required for applying methods or operators directly to columns.\n",
    "        *   `df.select(col(\"name\"), ...)`: Selects using `Column` objects.\n",
    "        *   `(col(\"age\") + 5)`: Creates a new `Column` expression by performing arithmetic on an existing column.\n",
    "        *   `.alias(\"new_name\")`: Renames the resulting column expression. Essential for derived columns.\n",
    "        *   `upper(col(\"country\"))`: Applies a built-in Spark SQL function (`upper` from `pyspark.sql.functions`) to a column. Many functions are available (string manipulation, date/time functions, math functions, etc.).\n",
    "    *   **Use Case:** Reducing data width for downstream processing, renaming columns for clarity, projecting data for reports, performing simple transformations within the selection.\n",
    "\n",
    "*   **b) Filtering Rows (`filter` / `where`)**\n",
    "    *   **Context:** Used to select a subset of rows based on specified conditions. Analogous to the `WHERE` clause in SQL. `filter()` and `where()` are aliases for the same operation.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"FilterOperation\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        data = [(\"Alice\", 30, \"USA\"), (\"Bob\", 25, \"Canada\"), (\"Charlie\", 35, \"USA\"), (\"David\", 20, \"UK\")]\n",
    "        df = spark.createDataFrame(data, [\"name\", \"age\", \"country\"])\n",
    "        print(\"--- Original DataFrame ---\")\n",
    "        df.show()\n",
    "\n",
    "        # Filter using a SQL-like string expression\n",
    "        print(\"\\n--- Filtering age > 28 (string expression) ---\")\n",
    "        df.filter(\"age > 28\").show()\n",
    "\n",
    "        # Filter using column expressions (recommended for programmatic conditions)\n",
    "        print(\"\\n--- Filtering country == 'USA' (column expression) ---\")\n",
    "        df.filter(col(\"country\") == \"USA\").show()\n",
    "\n",
    "        # Combine multiple conditions (AND)\n",
    "        print(\"\\n--- Filtering age > 25 AND country == 'USA' ---\")\n",
    "        df.filter((col(\"age\") > 25) & (col(\"country\") == \"USA\")).show()\n",
    "        # Note: Parentheses are important due to operator precedence\n",
    "\n",
    "        # Combine multiple conditions (OR)\n",
    "        print(\"\\n--- Filtering country == 'Canada' OR country == 'UK' ---\")\n",
    "        df.filter((col(\"country\") == \"Canada\") | (col(\"country\") == \"UK\")).show()\n",
    "\n",
    "        # Using 'where' (identical to filter)\n",
    "        print(\"\\n--- Filtering using 'where' (age < 30) ---\")\n",
    "        df.where(col(\"age\") < 30).show()\n",
    "\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `df.filter(\"sql_condition_string\")`: Filters rows based on a SQL WHERE clause string. Simple for basic conditions but less flexible programmatically.\n",
    "        *   `df.filter(Column_Condition)`: Filters based on a boolean `Column` expression.\n",
    "        *   `col(\"country\") == \"USA\"`: Creates a boolean `Column` evaluating to true where the country is \"USA\". Standard comparison operators (`==`, `!=`, `>`, `<`, `>=`, `<=`) work on columns.\n",
    "        *   `&`: Logical AND operator for combining `Column` conditions.\n",
    "        *   `|`: Logical OR operator.\n",
    "        *   `~`: Logical NOT operator (e.g., `~ (col(\"country\") == \"USA\")`).\n",
    "        *   Parentheses `()`: Essential for grouping conditions correctly, especially when mixing `&` and `|`.\n",
    "        *   `df.where(...)`: An alias for `df.filter(...)`. Use whichever you prefer for readability.\n",
    "    *   **Use Case:** Selecting specific subsets of data based on criteria, data cleaning (removing invalid rows), isolating data for targeted analysis.\n",
    "\n",
    "*   **c) Adding or Replacing Columns (`withColumn`)**\n",
    "    *   **Context:** Used to add a new column to a DataFrame or replace an existing column with the same name. The new column's values are defined by a `Column` expression.\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import col, lit, concat_ws, lower, when\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"WithColumnOperation\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        data = [(\"Alice\", 30, \"USA\"), (\"Bob\", 25, \"Canada\"), (\"Charlie\", 35, \"USA\")]\n",
    "        df = spark.createDataFrame(data, [\"name\", \"age\", \"country\"])\n",
    "        print(\"--- Original DataFrame ---\")\n",
    "        df.show()\n",
    "\n",
    "        # Add a new column with a literal value\n",
    "        print(\"\\n--- Adding 'source' column with literal value ---\")\n",
    "        df_with_source = df.withColumn(\"source\", lit(\"internal_db\"))\n",
    "        # lit() creates a Column from a literal value\n",
    "        df_with_source.show()\n",
    "\n",
    "        # Add a new column derived from existing columns\n",
    "        print(\"\\n--- Adding 'description' column ---\")\n",
    "        df_with_desc = df_with_source.withColumn(\n",
    "            \"description\",\n",
    "            concat_ws(\" - \", col(\"name\"), col(\"age\").cast(\"string\"), col(\"country\"))\n",
    "            # concat_ws concatenates columns with a separator\n",
    "            # cast(\"string\") converts age to string for concatenation\n",
    "        )\n",
    "        df_with_desc.show(truncate=False)\n",
    "\n",
    "        # Replace an existing column (e.g., standardize country code)\n",
    "        print(\"\\n--- Replacing 'country' column with lowercase version ---\")\n",
    "        df_lower_country = df_with_desc.withColumn(\"country\", lower(col(\"country\")))\n",
    "        df_lower_country.show()\n",
    "\n",
    "        # Add a column based on conditional logic\n",
    "        print(\"\\n--- Adding 'age_group' column using 'when' ---\")\n",
    "        df_age_group = df_lower_country.withColumn(\"age_group\",\n",
    "             when(col(\"age\") < 30, \"Young\")\n",
    "            .when((col(\"age\") >= 30) & (col(\"age\") < 40), \"Middle-aged\")\n",
    "            .otherwise(\"Senior\") # Default case\n",
    "        )\n",
    "        df_age_group.show()\n",
    "\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `df.withColumn(\"new_col_name\", Column_Expression)`: Returns a *new* DataFrame with the added/replaced column.\n",
    "        *   `lit(value)`: Creates a `Column` object representing a literal (constant) value. Necessary when adding a constant value, as `withColumn` expects a `Column` object as the second argument.\n",
    "        *   `col(\"age\").cast(\"string\")`: Changes the data type of the `age` column to `string` for this operation. Casting is common when combining different types.\n",
    "        *   `concat_ws(separator, col1, col2, ...)`: A function from `pyspark.sql.functions` that concatenates multiple columns into a single string column using a specified separator.\n",
    "        *   `lower(col(\"country\"))`: Applies the `lower` function to the `country` column. Since the column name \"country\" already exists, this *replaces* the original column.\n",
    "        *   `when(condition1, value1).when(condition2, value2)...otherwise(default_value)`: Implements conditional logic (like SQL `CASE WHEN`). It evaluates conditions sequentially and returns the corresponding value for the first true condition, or the `otherwise` value if none match.\n",
    "    *   **Use Case:** Feature engineering (creating new predictors for machine learning), data enrichment (adding information from other sources - often via joins first), data standardization/cleaning (e.g., converting cases, formatting dates), deriving calculated fields.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "DataFrames are the cornerstone of modern PySpark development for structured data. We learned how to create them from common formats like CSV, JSON, and the highly recommended Parquet, emphasizing the importance of schema definition (manual over inference for production). We also explored fundamental transformations: `select` for column projection/manipulation, `filter`/`where` for row selection based on conditions, and `withColumn` for adding or modifying columns. These operations, combined with Spark's lazy evaluation and Catalyst optimizer, provide a powerful and efficient way to work with large datasets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
