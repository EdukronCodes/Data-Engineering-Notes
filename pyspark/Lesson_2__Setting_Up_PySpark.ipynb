{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ae7a12",
   "metadata": {},
   "source": [
    "# Lesson 2 - Setting Up PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ebcd3",
   "metadata": {},
   "source": [
    "Okay, let's move on to Lesson 2, focusing on getting PySpark up and running.\n",
    "\n",
    "---\n",
    "\n",
    "**Lesson 2: Setting Up PySpark**\n",
    "\n",
    "**Objective:** To learn how to install and configure PySpark in different environments, understand the primary entry points (`SparkSession`, `SparkContext`), write and execute a basic PySpark script, and get familiar with fundamental configuration settings.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Installing PySpark (Local, Cloud, Databricks)**\n",
    "\n",
    "Getting PySpark ready involves installing the necessary software and potentially configuring environment variables. The method varies depending on where you intend to run Spark.\n",
    "\n",
    "**a) Local Installation (on your personal machine)**\n",
    "\n",
    "This is ideal for learning, development, and testing on smaller datasets. Spark will run in \"local mode,\" using the cores on your single machine.\n",
    "\n",
    "*   **Prerequisites:**\n",
    "    *   **Python:** You need a compatible Python installation (Python 3.x is recommended). Check with `python --version`.\n",
    "    *   **Java Development Kit (JDK):** Spark runs on the Java Virtual Machine (JVM). You need a JDK installed (version 8 or 11 are commonly used and stable). You can download it from Oracle or use OpenJDK.\n",
    "        *   *Verification:* Check with `java -version`.\n",
    "        *   *Environment Variable:* You **must** set the `JAVA_HOME` environment variable to point to your JDK installation directory. The process differs for Windows, macOS, and Linux.\n",
    "            *   Example (Linux/macOS - add to `.bashrc` or `.zshrc`): `export JAVA_HOME=/path/to/your/jdk`\n",
    "            *   Example (Windows): Set via System Properties -> Environment Variables.\n",
    "*   **Installation using `pip`:** This is the simplest way to get PySpark. It bundles the necessary Spark components.\n",
    "    ```bash\n",
    "    pip install pyspark\n",
    "    ```\n",
    "    *   *Optional:* You might also want `findspark` if you have a separate Spark installation and want Python to find it easily, but `pip install pyspark` usually suffices for basic local use.\n",
    "*   **Verification:** Open your terminal or command prompt and run the PySpark interactive shell:\n",
    "    ```bash\n",
    "    pyspark\n",
    "    ```\n",
    "    If it launches successfully, you'll see the Spark logo and a `spark` variable available in the shell. Type `quit()` to exit.\n",
    "*   **Pros:** Easy setup, free, great for learning fundamentals.\n",
    "*   **Cons:** Limited by your machine's resources (CPU, RAM), not suitable for large-scale data processing.\n",
    "\n",
    "**b) Cloud Installation (Managed Services)**\n",
    "\n",
    "Cloud providers offer managed Spark services that handle cluster creation, management, and scaling.\n",
    "\n",
    "*   **Examples:**\n",
    "    *   **AWS:** Elastic MapReduce (EMR)\n",
    "    *   **Google Cloud:** Dataproc\n",
    "    *   **Microsoft Azure:** HDInsight, Azure Synapse Analytics, Azure Databricks\n",
    "*   **Process:**\n",
    "    1.  Log in to your cloud provider's console.\n",
    "    2.  Navigate to the relevant managed Spark service (e.g., EMR, Dataproc).\n",
    "    3.  Configure and launch a cluster: Choose instance types (master/worker nodes), number of nodes, Spark version, etc. PySpark is typically pre-installed.\n",
    "    4.  Connect to the cluster: This varies. Options often include:\n",
    "        *   SSH into the master node and use `pyspark` shell or `spark-submit`.\n",
    "        *   Use web-based notebook interfaces provided by the service (like JupyterHub, Zeppelin).\n",
    "        *   Submit jobs programmatically via APIs or SDKs.\n",
    "*   **Pros:** Scalable (pay for what you use), managed infrastructure (less Ops overhead), integrated with other cloud services.\n",
    "*   **Cons:** Cost involved, can have a steeper initial learning curve for cluster configuration, potential vendor lock-in.\n",
    "\n",
    "**c) Databricks**\n",
    "\n",
    "Databricks is a popular, commercial unified analytics platform built by the original creators of Apache Spark. It provides an optimized Spark environment with a collaborative notebook interface.\n",
    "\n",
    "*   **Setup:**\n",
    "    1.  Sign up for a Databricks account (offers community edition for free learning, or uses your cloud provider like AWS, Azure, GCP).\n",
    "    2.  Log in to your Databricks workspace.\n",
    "    3.  Create a Cluster: Use the UI to specify Spark version, node types, autoscaling options, etc. Start the cluster.\n",
    "    4.  Create a Notebook: Create a new notebook (Python is a common choice).\n",
    "    5.  Attach the notebook to your running cluster.\n",
    "*   **PySpark Access:** In a Databricks notebook attached to a cluster, PySpark (specifically the `SparkSession` object named `spark`) is **already initialized and available** for you to use directly. There's no separate installation needed within the notebook environment.\n",
    "*   **Pros:** Extremely easy setup, optimized Spark runtime, collaborative environment, integrated features (MLflow, Delta Lake).\n",
    "*   **Cons:** Commercial product (cost beyond free tier), platform-specific features might lead to lock-in.\n",
    "\n",
    "**Comparison Table:**\n",
    "\n",
    "| Feature             | Local Installation           | Cloud Managed Service (EMR, Dataproc) | Databricks                  |\n",
    "| :------------------ | :------------------------- | :------------------------------------ | :-------------------------- |\n",
    "| **Ease of Setup**   | Moderate (JDK, pip)        | Moderate to Complex (Cloud config)  | Easy (Web UI)               |\n",
    "| **Scalability**     | Low (Single Machine)       | High (Cluster)                        | High (Optimized Cluster)    |\n",
    "| **Cost**            | Free (Hardware cost only)  | Pay-as-you-go (Compute, Storage)    | Pay-as-you-go (DBUs, Cloud) |\n",
    "| **Management**      | Manual                     | Partially Managed                     | Fully Managed               |\n",
    "| **Primary Use Case**| Learning, Small Dev/Test   | Production Workloads, Large Data    | Dev, Production, Collaboration |\n",
    "| **PySpark Access**  | Install via `pip`, run shell/script | Pre-installed on cluster nodes      | Pre-configured in notebooks |\n",
    "\n",
    "---\n",
    "\n",
    "**2. SparkSession and SparkContext**\n",
    "\n",
    "These are the primary entry points for interacting with Spark.\n",
    "\n",
    "*   **SparkContext (`sc`)**\n",
    "    *   **Role:** The original (Spark 1.x) main entry point to Spark functionality. It represents the connection to a Spark cluster and is used to create RDDs, accumulators, and broadcast variables.\n",
    "    *   **Access:** In older Spark versions or when working directly with RDDs, you would create a `SparkContext`. In modern Spark, the `SparkSession` manages the `SparkContext`. You can access the underlying `SparkContext` from a `SparkSession` using `spark.sparkContext`.\n",
    "    *   **Key Functions:** Creating RDDs (`parallelize`, `textFile`), accessing cluster information.\n",
    "\n",
    "*   **SparkSession (`spark`)**\n",
    "    *   **Role:** Introduced in Spark 2.0 as a **unified entry point** for all Spark functionality. It subsumes `SQLContext`, `HiveContext`, and `StreamingContext` from earlier versions. It's the preferred way to interact with Spark now.\n",
    "    *   **Functionality:**\n",
    "        *   Creates DataFrames and Datasets.\n",
    "        *   Reads data from various sources (JSON, CSV, Parquet, JDBC, etc.).\n",
    "        *   Executes SQL queries (`spark.sql(...)`).\n",
    "        *   Provides access to configuration settings.\n",
    "        *   Manages the underlying `SparkContext`.\n",
    "    *   **Instantiation (in a standalone script):** You typically create a `SparkSession` using the builder pattern.\n",
    "        ```python\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        # Create a SparkSession\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"MyFirstApp\") \\ # Optional: Set application name\n",
    "            .master(\"local[*]\") \\ # Optional: Run locally using all available cores\n",
    "            .getOrCreate()\n",
    "\n",
    "        # Now you can use 'spark' to create DataFrames, etc.\n",
    "        print(f\"SparkSession available. Spark version: {spark.version}\")\n",
    "\n",
    "        # Access the underlying SparkContext if needed\n",
    "        sc = spark.sparkContext\n",
    "        print(f\"SparkContext available: {sc.appName}, Master: {sc.master}\")\n",
    "\n",
    "        # Don't forget to stop the session when done in a script\n",
    "        spark.stop()\n",
    "        ```\n",
    "    *   **`getOrCreate()`:** This method either gets an existing `SparkSession` or creates a new one if none exists. This prevents issues if multiple parts of your code try to create a session.\n",
    "    *   **In Interactive Shells/Databricks:** The `pyspark` shell and Databricks notebooks typically create a `SparkSession` instance named `spark` for you automatically.\n",
    "\n",
    "---\n",
    "\n",
    "**3. First PySpark Script: Hello World**\n",
    "\n",
    "Let's write a simple script that creates a DataFrame and prints its content. This verifies your setup and shows basic DataFrame manipulation.\n",
    "\n",
    "*   **Objective:** Create a DataFrame with names and ages, then display it.\n",
    "\n",
    "*   **Code (`hello_spark.py`):**\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import Row # Often used to create Rows for DataFrames\n",
    "\n",
    "    # 1. Create a SparkSession\n",
    "    # Use builder pattern; master(\"local[*]\") runs locally using all cores\n",
    "    # appName is a name for your application shown in the Spark UI\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HelloWorld\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(f\"SparkSession created. Spark version: {spark.version}\")\n",
    "\n",
    "    # 2. Create Sample Data\n",
    "    # Using a list of Row objects\n",
    "    data = [\n",
    "        Row(name=\"Alice\", age=30),\n",
    "        Row(name=\"Bob\", age=25),\n",
    "        Row(name=\"Charlie\", age=35)\n",
    "    ]\n",
    "\n",
    "    # 3. Create a DataFrame\n",
    "    # Spark can infer the schema from the Row objects\n",
    "    df = spark.createDataFrame(data)\n",
    "\n",
    "    # 4. Show DataFrame Content and Schema\n",
    "    print(\"DataFrame Content:\")\n",
    "    df.show()\n",
    "\n",
    "    print(\"DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # 5. Perform a simple operation (e.g., select a column)\n",
    "    print(\"Selecting only the 'name' column:\")\n",
    "    df.select(\"name\").show()\n",
    "\n",
    "    # 6. Stop the SparkSession (important in scripts)\n",
    "    spark.stop()\n",
    "    print(\"SparkSession stopped.\")\n",
    "    ```\n",
    "\n",
    "*   **Running the Script:**\n",
    "    *   Save the code above as `hello_spark.py`.\n",
    "    *   Open your terminal or command prompt.\n",
    "    *   Make sure your `JAVA_HOME` is set correctly.\n",
    "    *   Use `spark-submit` (a tool included with Spark/PySpark) to run the script:\n",
    "        ```bash\n",
    "        spark-submit hello_spark.py\n",
    "        ```\n",
    "        (If `spark-submit` isn't in your PATH, you might need to find its location within your Python environment's `site-packages/pyspark/bin` or your separate Spark installation's `bin` directory).\n",
    "\n",
    "*   **Expected Output:**\n",
    "    ```\n",
    "    SparkSession created. Spark version: <your_spark_version>\n",
    "    DataFrame Content:\n",
    "    +-------+---+\n",
    "    |   name|age|\n",
    "    +-------+---+\n",
    "    |  Alice| 30|\n",
    "    |    Bob| 25|\n",
    "    |Charlie| 35|\n",
    "    +-------+---+\n",
    "\n",
    "    DataFrame Schema:\n",
    "    root\n",
    "     |-- name: string (nullable = true)\n",
    "     |-- age: long (nullable = true)  # Note: Spark often infers integers as long\n",
    "\n",
    "    Selecting only the 'name' column:\n",
    "    +-------+\n",
    "    |   name|\n",
    "    +-------+\n",
    "    |  Alice|\n",
    "    |    Bob|\n",
    "    |Charlie|\n",
    "    +-------+\n",
    "\n",
    "    SparkSession stopped.\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Configuration Basics**\n",
    "\n",
    "You can control various Spark properties related to performance, resource allocation, and behavior.\n",
    "\n",
    "*   **Why Configure?**\n",
    "    *   Tune performance (e.g., memory allocation, parallelism).\n",
    "    *   Specify cluster manager details (if not running locally).\n",
    "    *   Set application-specific parameters.\n",
    "    *   Configure connectors to external systems (databases, message queues).\n",
    "\n",
    "*   **How to Set Configuration:**\n",
    "    1.  **Using `SparkSession.builder.config()`:** Set options when creating the session.\n",
    "        ```python\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ConfigExample\") \\\n",
    "            .master(\"local[2]\") \\ # Use 2 local cores\n",
    "            .config(\"spark.driver.memory\", \"1g\") \\ # Set driver memory to 1 GB\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"5\") \\ # Set default partitions for SQL shuffles\n",
    "            .getOrCreate()\n",
    "        ```\n",
    "    2.  **Using `spark-submit` command-line options:** Override or set configurations at runtime.\n",
    "        ```bash\n",
    "        spark-submit \\\n",
    "          --master local[4] \\\n",
    "          --driver-memory 2g \\\n",
    "          --conf spark.executor.memory=1g \\\n",
    "          --conf spark.sql.shuffle.partitions=10 \\\n",
    "          my_spark_app.py\n",
    "        ```\n",
    "    3.  **Using `spark-defaults.conf` file:** Place this file in Spark's `conf` directory for default settings across applications. Each line is a key-value pair (e.g., `spark.driver.memory 1g`).\n",
    "    4.  **Runtime Configuration:** You can also set *some* SQL-related configurations after the session is created:\n",
    "        ```python\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "        current_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "        print(f\"Current shuffle partitions: {current_partitions}\") # Output: 8\n",
    "        ```\n",
    "\n",
    "*   **Common Configurations (Examples):**\n",
    "    *   `spark.app.name`: Your application name.\n",
    "    *   `spark.master`: Cluster URL (e.g., `local[*]`, `yarn`, `spark://host:port`).\n",
    "    *   `spark.driver.memory`: Memory for the driver process (where your main script runs).\n",
    "    *   `spark.executor.memory`: Memory per executor process (worker processes).\n",
    "    *   `spark.executor.cores`: Number of CPU cores per executor.\n",
    "    *   `spark.sql.shuffle.partitions`: Default number of partitions to use when shuffling data for joins or aggregations. Tuning this is important for performance.\n",
    "\n",
    "*   **Viewing Configuration:**\n",
    "    ```python\n",
    "    # Get all configuration settings\n",
    "    all_conf = spark.sparkContext.getConf().getAll()\n",
    "    print(all_conf)\n",
    "\n",
    "    # Get a specific setting\n",
    "    driver_mem = spark.conf.get(\"spark.driver.memory\")\n",
    "    print(f\"Driver Memory: {driver_mem}\")\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Lesson 2:**\n",
    "\n",
    "*   PySpark can be installed **locally** (via `pip`, needs JDK), on **cloud platforms** (using managed services like EMR, Dataproc), or via **Databricks**.\n",
    "*   The **`SparkSession`** (`spark`) is the modern, unified entry point for PySpark applications, used for creating DataFrames, running SQL, and accessing configuration. It manages the underlying **`SparkContext`** (`sc`).\n",
    "*   We wrote and ran a basic **\"Hello World\"** script using `spark-submit`, demonstrating `SparkSession` creation, DataFrame creation (`spark.createDataFrame`), and basic actions (`show`, `printSchema`).\n",
    "*   Spark behavior can be tuned using **configuration properties**, set via the `SparkSession` builder, `spark-submit` options, or configuration files. Basic settings control application name, master URL, and memory allocation.\n",
    "\n",
    "This lesson equips you with the practical steps to start using PySpark in your chosen environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84901a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
