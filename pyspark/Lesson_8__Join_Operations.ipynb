{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ae2e08",
   "metadata": {},
   "source": [
    "# Lesson 8 - Join Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ceaba",
   "metadata": {},
   "source": [
    "Okay, let's dive into the crucial topic of Join Operations in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "**Technical Notes: Mastering Join Operations in PySpark**\n",
    "\n",
    "**Objective:** These notes provide a comprehensive guide to performing join operations on DataFrames in PySpark. Understanding joins is fundamental for combining data from different sources, enriching datasets, and performing complex analytical queries in distributed environments. We will cover various join types, strategies, and optimization techniques relevant for professional Spark developers.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Introduction to Joins in PySpark**\n",
    "\n",
    "*   **Theory:**\n",
    "    In distributed data processing, datasets are often normalized or originate from different sources. Join operations are used to combine rows from two DataFrames based on a related column or a set of columns (the join key/condition). PySpark's DataFrame API provides a versatile `join()` method that supports various standard SQL join types and includes optimizations suitable for large-scale data.\n",
    "\n",
    "    The core syntax involves calling the `join()` method on the \"left\" DataFrame and providing the \"right\" DataFrame, the join condition, and the join type.\n",
    "\n",
    "    ```python\n",
    "    # General Syntax\n",
    "    left_df.join(\n",
    "        right_df,\n",
    "        on=join_condition, # Condition for matching rows\n",
    "        how=join_type      # Type of join (e.g., 'inner', 'left_outer')\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    Choosing the correct join type and optimizing the join execution are critical for performance in Spark, as joins can involve significant data shuffling across the network (moving data between executors).\n",
    "\n",
    "*   **Setup: Sample DataFrames**\n",
    "    We'll use two simple DataFrames for consistent examples throughout this section: `employees_df` and `departments_df`.\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"JoinOperations\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") # Lower for local testing\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Employee data: Some employees have a valid dept_id, one has null, one has a non-existent dept_id\n",
    "    employees_data = [\n",
    "        (1, \"Alice\", 30, 101),\n",
    "        (2, \"Bob\", 25, 102),\n",
    "        (3, \"Charlie\", 35, 101),\n",
    "        (4, \"David\", 28, None), # No department assigned\n",
    "        (5, \"Eve\", 40, 104)     # Department 104 does not exist in departments table\n",
    "    ]\n",
    "    employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"name\", \"age\", \"dept_id\"])\n",
    "\n",
    "    # Department data: Includes departments referenced by employees, plus one extra\n",
    "    departments_data = [\n",
    "        (101, \"Engineering\", \"New York\"),\n",
    "        (102, \"Sales\", \"London\"),\n",
    "        (103, \"HR\", \"New York\")\n",
    "    ]\n",
    "    departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"])\n",
    "\n",
    "    print(\"--- Employees DataFrame ---\")\n",
    "    employees_df.show()\n",
    "    print(\"--- Departments DataFrame ---\")\n",
    "    departments_df.show()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Standard SQL Join Types**\n",
    "\n",
    "These are the most common join types, mirroring standard SQL behavior.\n",
    "\n",
    "*   **a) Inner Join**\n",
    "    *   **Theory:** Returns only the rows where the join key exists in **both** the left and right DataFrames. Rows with unmatched keys in either DataFrame are discarded. It's the default join type if `how` is not specified.\n",
    "    *   **Syntax:** `how=\"inner\"` (or omit `how`)\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Inner Join ---\")\n",
    "        # Joining employees with their departments based on 'dept_id'\n",
    "        # Note: When the join column name is identical ('dept_id'), Spark may duplicate it.\n",
    "        # It's often better to specify the condition explicitly or rename/drop columns after.\n",
    "\n",
    "        # Method 1: Using string column name (if identical)\n",
    "        inner_join_df_simple = employees_df.join(departments_df, on=\"dept_id\", how=\"inner\")\n",
    "        inner_join_df_simple.show()\n",
    "\n",
    "        # Method 2: Using explicit column condition (avoids ambiguity if names differ or for complex conditions)\n",
    "        inner_join_df_explicit = employees_df.join(\n",
    "            departments_df,\n",
    "            employees_df[\"dept_id\"] == departments_df[\"dept_id\"], # Join condition\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        print(\"\\n--- Inner Join (Explicit Condition - showing duplicate key column) ---\")\n",
    "        inner_join_df_explicit.show()\n",
    "\n",
    "        # Method 3: Explicit condition and dropping ambiguous column\n",
    "        inner_join_df_clean = employees_df.join(\n",
    "            departments_df,\n",
    "            employees_df[\"dept_id\"] == departments_df[\"dept_id\"],\n",
    "            how=\"inner\"\n",
    "        ).drop(departments_df[\"dept_id\"]) # Drop the duplicate key from the right DF\n",
    "        print(\"\\n--- Inner Join (Explicit Condition - Cleaned) ---\")\n",
    "        inner_join_df_clean.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   `employees_df.join(departments_df, on=\"dept_id\", how=\"inner\")`: Performs an inner join using the `dept_id` column present in both DataFrames.\n",
    "        *   Only employees Alice (101), Bob (102), and Charlie (101) have `dept_id` values present in the `departments_df`.\n",
    "        *   David (null `dept_id`) and Eve (non-matching `dept_id` 104) are excluded.\n",
    "        *   Department 103 (HR) is excluded because no employee belongs to it.\n",
    "        *   `employees_df[\"dept_id\"] == departments_df[\"dept_id\"]`: Explicitly defines the join condition using `Column` objects. This is necessary if join keys have different names or if the condition is more complex (e.g., involving inequalities). This syntax results in *both* `dept_id` columns appearing in the output.\n",
    "        *   `.drop(departments_df[\"dept_id\"])`: After the join with an explicit condition, we drop the redundant `dept_id` column originating from the `departments_df` for a cleaner result.\n",
    "    *   **Use Case:** Combining related datasets where you only care about records that have a corresponding match in the other dataset (e.g., finding customers and their orders, students and their enrolled courses).\n",
    "\n",
    "*   **b) Left Outer Join (or Left Join)**\n",
    "    *   **Theory:** Returns all rows from the **left** DataFrame and the matched rows from the right DataFrame. If there's no match for a row from the left DataFrame in the right DataFrame, the columns from the right DataFrame will have `null` values for that row.\n",
    "    *   **Syntax:** `how=\"left\"` or `how=\"left_outer\"`\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Left Outer Join ---\")\n",
    "        left_join_df = employees_df.join(\n",
    "            departments_df,\n",
    "            on=\"dept_id\", # Simple syntax works well here\n",
    "            how=\"left_outer\"\n",
    "        )\n",
    "        left_join_df.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   All employees (Alice, Bob, Charlie, David, Eve) are included in the result because `employees_df` is the left DataFrame.\n",
    "        *   Alice, Bob, and Charlie have matching `dept_id`s, so their corresponding `dept_name` and `location` are populated.\n",
    "        *   David has `null` for `dept_id`, so the join condition doesn't match any row in `departments_df`. His `dept_name` and `location` are `null`.\n",
    "        *   Eve has `dept_id` 104, which doesn't exist in `departments_df`. Her `dept_name` and `location` are also `null`.\n",
    "        *   Department 103 (HR) is still excluded because it doesn't match any `dept_id` from the left DataFrame *and* outer joins don't add unmatched rows from the *right* side in this case.\n",
    "    *   **Use Case:** Enriching a primary dataset (left) with optional information from another dataset (right). Useful when you want to keep all records from the primary table regardless of whether they have a match (e.g., listing all customers and their latest order details, if any).\n",
    "\n",
    "*   **c) Right Outer Join (or Right Join)**\n",
    "    *   **Theory:** Returns all rows from the **right** DataFrame and the matched rows from the left DataFrame. If there's no match for a row from the right DataFrame in the left DataFrame, the columns from the left DataFrame will have `null` values for that row. It's the mirror image of a left join.\n",
    "    *   **Syntax:** `how=\"right\"` or `how=\"right_outer\"`\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Right Outer Join ---\")\n",
    "        right_join_df = employees_df.join(\n",
    "            departments_df,\n",
    "            on=\"dept_id\",\n",
    "            how=\"right_outer\"\n",
    "        )\n",
    "        right_join_df.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   All departments (101 - Engineering, 102 - Sales, 103 - HR) are included because `departments_df` is the right DataFrame.\n",
    "        *   Departments 101 and 102 have matching employees (Alice, Charlie for 101; Bob for 102), so their `emp_id`, `name`, and `age` are populated. Notice department 101 appears twice due to multiple matching employees.\n",
    "        *   Department 103 (HR) has no matching employee in `employees_df`, so `emp_id`, `name`, and `age` are `null` for that row.\n",
    "        *   David (null `dept_id`) and Eve (`dept_id` 104) from the left DataFrame are excluded because their `dept_id`s don't match any in the right DataFrame *and* this is a right join.\n",
    "    *   **Use Case:** Less common than left joins, but useful when the focus is on ensuring all records from the secondary table (right) are present, supplemented with matching data from the primary table (left) (e.g., listing all products and any sales data associated with them).\n",
    "\n",
    "*   **d) Full Outer Join (or Full Join)**\n",
    "    *   **Theory:** Returns all rows when there is a match in either the left or the right DataFrame. It combines the results of both Left and Right Outer Joins. If a row from one DataFrame doesn't have a match in the other, the columns from the other DataFrame will be `null`.\n",
    "    *   **Syntax:** `how=\"full\"` or `how=\"outer\"` or `how=\"full_outer\"`\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Full Outer Join ---\")\n",
    "        full_join_df = employees_df.join(\n",
    "            departments_df,\n",
    "            on=\"dept_id\",\n",
    "            how=\"full_outer\"\n",
    "        )\n",
    "        full_join_df.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   Includes all employees (Alice, Bob, Charlie, David, Eve) from the left side.\n",
    "        *   Includes all departments (101, 102, 103) from the right side.\n",
    "        *   Matched rows (Alice/101, Bob/102, Charlie/101) have values from both sides.\n",
    "        *   Unmatched employees (David with null `dept_id`, Eve with `dept_id` 104) have `null`s for department columns.\n",
    "        *   Unmatched departments (103 - HR) have `null`s for employee columns.\n",
    "    *   **Use Case:** Combining two datasets where you need to retain all information from both, regardless of matches. Useful for comparing two datasets or creating a complete view when records might exist independently in either source.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Semi and Anti Joins**\n",
    "\n",
    "These are specialized join types primarily used for filtering based on the existence (or non-existence) of matches, rather than combining columns. They only return columns from the **left** DataFrame.\n",
    "\n",
    "*   **a) Left Semi Join**\n",
    "    *   **Theory:** Returns only the rows from the **left** DataFrame for which there is at least one matching row in the right DataFrame based on the join key. It acts like a filter on the left DataFrame. It does **not** include any columns from the right DataFrame.\n",
    "    *   **Syntax:** `how=\"left_semi\"`\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Left Semi Join (Employees with existing departments) ---\")\n",
    "        left_semi_join_df = employees_df.join(\n",
    "            departments_df,\n",
    "            on=\"dept_id\",\n",
    "            how=\"left_semi\"\n",
    "        )\n",
    "        left_semi_join_df.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   The join checks which `dept_id`s from `employees_df` exist in `departments_df`.\n",
    "        *   Alice (101), Bob (102), and Charlie (101) have matching `dept_id`s in `departments_df`.\n",
    "        *   The resulting DataFrame contains only these three employee rows, and importantly, *only the columns from `employees_df`*.\n",
    "        *   David (null) and Eve (104) are excluded as their `dept_id`s don't match.\n",
    "    *   **Use Case:** Efficiently filtering a large table based on the existence of keys in another table without needing the data from the second table (e.g., finding all customers who have placed at least one order, finding users who have logged activity recently). Often more efficient than an inner join followed by a `select` and `distinct` on the left table's columns.\n",
    "\n",
    "*   **b) Left Anti Join**\n",
    "    *   **Theory:** Returns only the rows from the **left** DataFrame for which there are **no** matching rows in the right DataFrame based on the join key. It's the opposite of a left semi join, useful for finding records that *don't* have a counterpart. It also only returns columns from the left DataFrame.\n",
    "    *   **Syntax:** `how=\"left_anti\"`\n",
    "    *   **Code Example:**\n",
    "\n",
    "        ```python\n",
    "        print(\"\\n--- Left Anti Join (Employees with non-existent or null departments) ---\")\n",
    "        left_anti_join_df = employees_df.join(\n",
    "            departments_df,\n",
    "            on=\"dept_id\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        left_anti_join_df.show()\n",
    "        ```\n",
    "    *   **Explanation:**\n",
    "        *   The join checks which `dept_id`s from `employees_df` *do not* exist in `departments_df`.\n",
    "        *   David has `null` for `dept_id`, which typically doesn't match anything (unless explicitly handling null keys, which is tricky).\n",
    "        *   Eve has `dept_id` 104, which does not exist in `departments_df`.\n",
    "        *   The resulting DataFrame contains only the rows for David and Eve, with only the columns from `employees_df`.\n",
    "        *   Alice, Bob, and Charlie are excluded because their `dept_id`s *do* have matches.\n",
    "    *   **Use Case:** Finding \"orphan\" records or records that fail a referential integrity check (e.g., finding customers who have *never* placed an order, identifying products listed in inventory but missing from the product catalog, finding users who registered but never logged in).\n",
    "\n",
    "---\n",
    "\n",
    "**4. Broadcast Joins (Broadcast Hash Join - BHJ)**\n",
    "\n",
    "*   **Theory:**\n",
    "    Joins in distributed systems often require a **shuffle**, where data matching the same key from different partitions across the cluster is physically moved to the same executor node for the join computation. Shuffles are expensive due to network I/O and serialization.\n",
    "\n",
    "    A **Broadcast Hash Join** is a significant optimization strategy used when one of the DataFrames being joined is **small enough to fit comfortably into the memory of each executor**. Instead of shuffling the large DataFrame, Spark **broadcasts** (sends a copy of) the *entire* small DataFrame to *every* executor node. Each executor then builds an in-memory hash table from the small DataFrame and performs the join locally by streaming through the partitions of the large DataFrame it holds, looking up matches in the hash table. This completely avoids the costly shuffle of the large DataFrame.\n",
    "\n",
    "    *   **Automatic Broadcasting:** Spark automatically attempts a broadcast join if the estimated physical size of one DataFrame is below the threshold set by the configuration `spark.sql.autoBroadcastJoinThreshold` (default is 10MB). The size estimation might not always be accurate.\n",
    "    *   **Manual Broadcasting:** You can explicitly hint to Spark to broadcast a specific DataFrame using the `broadcast()` function from `pyspark.sql.functions`. This is useful if Spark's estimation is wrong or if you know a DataFrame is small enough, even if slightly above the threshold.\n",
    "\n",
    "*   **Code Example:**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql.functions import broadcast\n",
    "\n",
    "    # Assume departments_df is known to be small\n",
    "    print(\"\\n--- Joining using Broadcast Hint ---\")\n",
    "\n",
    "    # Force broadcast of the smaller DataFrame (departments_df)\n",
    "    broadcast_join_df = employees_df.join(\n",
    "        broadcast(departments_df), # Apply broadcast hint to the right DataFrame\n",
    "        on=\"dept_id\",\n",
    "        how=\"inner\" # Broadcast can be used with most join types\n",
    "    )\n",
    "\n",
    "    # Check the physical execution plan\n",
    "    print(\"Physical Plan (should contain BroadcastHashJoin):\")\n",
    "    broadcast_join_df.explain()\n",
    "\n",
    "    # Show the result (same as inner join result)\n",
    "    broadcast_join_df.show()\n",
    "    ```\n",
    "*   **Explanation:**\n",
    "    *   `from pyspark.sql.functions import broadcast`: Imports the necessary function.\n",
    "    *   `broadcast(departments_df)`: Wraps the `departments_df` DataFrame with the `broadcast` hint. This tells Spark's Catalyst optimizer to strongly prefer a broadcast join strategy for this specific join operation, assuming `departments_df` is the smaller table being broadcast.\n",
    "    *   `broadcast_join_df.explain()`: This is a crucial method for performance tuning. It displays the physical execution plan Spark intends to use (or used). When a broadcast join is successfully planned, you will typically see `BroadcastHashJoin` listed in the plan output. If you see `SortMergeJoin` or `ShuffledHashJoin`, it indicates a shuffle is planned (which might be necessary if both DataFrames are large).\n",
    "    *   The output of `show()` will be identical to the standard inner join, but the execution mechanism is potentially much faster for large `employees_df`.\n",
    "*   **Use Case:** Extremely common and beneficial when joining a large fact table (e.g., billions of transaction records) with smaller dimension tables (e.g., thousands of store details, product categories, user profiles). **Always consider broadcast joins for small dimension tables.**\n",
    "\n",
    "---\n",
    "\n",
    "**5. Join Optimization Tips**\n",
    "\n",
    "Performing joins efficiently is critical for Spark job performance.\n",
    "\n",
    "1.  **Use Broadcast Joins Aggressively:** As discussed, if one DataFrame is small (typically < 100MB, but depends on executor memory), explicitly use `broadcast()`. Check the physical plan with `explain()` to confirm. Monitor the Spark UI to ensure the broadcast doesn't cause driver OOM errors if the table is larger than expected.\n",
    "2.  **Filter Early, Select Early (Predicate Pushdown & Column Pruning):** Apply `filter()` or `where()` clauses and `select()` needed columns *before* the `join()` call whenever possible. Spark's Catalyst optimizer often pushes these operations down automatically, but being explicit can sometimes help and makes the logic clearer. Reducing the size of DataFrames *before* they are shuffled or broadcast significantly reduces network traffic and computation.\n",
    "    ```python\n",
    "    # Example: Filter departments before joining\n",
    "    active_departments = departments_df.filter(col(\"location\") == \"New York\")\n",
    "    employees_in_ny = employees_df.join(\n",
    "        broadcast(active_departments), # Still broadcast the filtered (smaller) DF\n",
    "        on=\"dept_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    employees_in_ny.explain()\n",
    "    ```\n",
    "3.  **Choose the Right Join Type:** Use `left_semi` or `left_anti` joins when you only need to check for the existence/non-existence of matching keys and don't need columns from the right table. They are often more efficient than inner/left joins followed by `select`/`distinct`.\n",
    "4.  **Ensure Consistent Data Types for Join Keys:** Joining on columns with different data types (e.g., `string` and `int`) can prevent optimizations like Sort-Merge joins or require costly casting, potentially leading to poor performance or incorrect results. Ensure keys have matching types before the join.\n",
    "5.  **Handle Data Skew:** If the data in the join key columns is heavily skewed (i.e., a few key values represent a disproportionately large number of rows), the tasks responsible for those keys can become bottlenecks, slowing down the entire join. Techniques like \"salting\" (adding a random prefix to skewed keys to distribute them better) can help, but this is an advanced topic requiring careful implementation. Check the Spark UI for tasks taking much longer than others during join stages.\n",
    "6.  **Consider Bucketing (Advanced):** For very large tables that are joined frequently on the same keys, you can pre-process them by saving them as \"bucketed\" tables (using `bucketBy()` in `DataFrameWriter`). If two tables are bucketed identically on their join keys, Spark can perform the join without shuffling data between the corresponding buckets (using a technique similar to a Sort-Merge join but avoiding the sort phase if already sorted within buckets). This requires careful planning of the number of buckets and the bucketing keys.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "Joins are powerful tools for combining DataFrames in PySpark. Understanding the behavior of different join types (Inner, Left/Right/Full Outer, Left Semi/Anti) is essential for correct data manipulation. For performance, leveraging Broadcast Hash Joins for small tables is paramount. Additionally, applying filters and projections early, ensuring data type consistency, and being aware of potential data skew are key optimization strategies for efficient distributed joins. Always use `explain()` and the Spark UI to understand and verify the join strategy being executed.\n",
    "\n",
    "---\n",
    "\n",
    "*This concludes the technical notes on PySpark Join Operations.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
