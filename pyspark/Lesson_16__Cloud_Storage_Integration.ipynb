{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edb0a47",
   "metadata": {},
   "source": [
    "# Lesson 16 - Cloud Storage Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19dcea",
   "metadata": {},
   "source": [
    "Okay, let's structure the technical notes for Lesson 16: Cloud Storage Integration with PySpark in Azure Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Notes: PySpark Cloud Storage Integration (S3, ADLS Gen2, GCS)\n",
    "\n",
    "**Environment:** Azure Databricks\n",
    "\n",
    "**Objective:** Provide professional learners with a comprehensive understanding of how to read from and write to major cloud storage systems (AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage) using PySpark within the Azure Databricks environment. These notes cover authentication, data access patterns, optimization, and platform comparisons.\n",
    "\n",
    "### 1. Introduction: The Need for Cloud Storage in Big Data Workflows\n",
    "\n",
    "Modern data analytics platforms like Apache Spark thrive on scalable and elastic infrastructure. Cloud storage services provide the ideal backend for large-scale data processing due to their:\n",
    "\n",
    "*   **Scalability & Durability:** Offering virtually limitless storage capacity and high data durability guarantees.\n",
    "*   **Decoupling Compute and Storage:** Allowing compute clusters (like Databricks) to be scaled independently of storage, optimizing costs. Clusters can be spun up for processing and shut down afterwards, while data persists in the storage layer.\n",
    "*   **Cost-Effectiveness:** Pay-as-you-go pricing models and tiered storage options help manage costs effectively.\n",
    "*   **Accessibility:** Data stored in the cloud can be accessed by various services and tools, facilitating broader data integration and analytics ecosystems.\n",
    "\n",
    "PySpark, running on platforms like Azure Databricks, provides robust APIs to interact seamlessly with these storage systems. This lesson focuses on the practical aspects of connecting PySpark to AWS S3, Azure Data Lake Storage (ADLS) Gen2, and Google Cloud Storage (GCS).\n",
    "\n",
    "### 2. Core Concepts: Accessing Cloud Storage with PySpark\n",
    "\n",
    "#### 2.1 PySpark DataFrameReader and DataFrameWriter\n",
    "\n",
    "The primary interfaces for interacting with external data sources, including cloud storage, are the `DataFrameReader` (`spark.read`) and `DataFrameWriter` (`df.write`).\n",
    "\n",
    "*   **`spark.read`**: Used to load data from various sources into a DataFrame. Key methods include:\n",
    "    *   `format(source_format)`: Specifies the data format (e.g., \"parquet\", \"csv\", \"json\", \"delta\").\n",
    "    *   `option(key, value)` / `options(**options)`: Sets configuration options specific to the data source (e.g., path, schema inference, authentication details - though direct keys are discouraged).\n",
    "    *   `schema(schema)`: Specifies the schema explicitly.\n",
    "    *   `load(path)`: Specifies the path to the data in cloud storage.\n",
    "*   **`df.write`**: Used to save DataFrame contents to external sources. Key methods include:\n",
    "    *   `format(source_format)`: Specifies the output format.\n",
    "    *   `option(key, value)` / `options(**options)`: Sets configuration options (e.g., compression, path).\n",
    "    *   `mode(save_mode)`: Specifies behavior if data already exists (e.g., \"overwrite\", \"append\", \"ignore\", \"errorifexists\").\n",
    "    *   `partitionBy(*cols)`: Partitions the output data based on column values (more on this later).\n",
    "    *   `save(path)`: Specifies the output path in cloud storage.\n",
    "\n",
    "#### 2.2 Cloud Storage URIs\n",
    "\n",
    "PySpark uses specific URI schemes to identify and access different cloud storage systems:\n",
    "\n",
    "*   **AWS S3:** `s3a://<your-bucket-name>/<path>/<to>/<data>` (The `s3a` connector is recommended over older `s3n` or `s3` for performance and features).\n",
    "*   **Azure Data Lake Storage Gen2:** `abfss://<filesystem-name>@<storage-account-name>.dfs.core.windows.net/<path>/<to>/<data>` (ABFSS - Azure Blob File System driver, optimized for big data analytics).\n",
    "*   **Google Cloud Storage:** `gs://<your-bucket-name>/<path>/<to>/<data>`\n",
    "\n",
    "#### 2.3 Authentication and Authorization: The Crucial Step\n",
    "\n",
    "Connecting Spark to cloud storage requires proper authentication to ensure secure access. **Storing credentials directly in notebooks or code is highly discouraged.** Secure methods include:\n",
    "\n",
    "*   **Azure Databricks Secrets:** Store sensitive credentials (keys, tokens) securely within Databricks Secrets, referenced programmatically.\n",
    "*   **Instance Profiles (AWS) / Managed Identities (Azure):** Associate cloud IAM roles/identities with the Databricks cluster, granting permissions without embedding keys. This is often the most secure and recommended approach.\n",
    "*   **OAuth 2.0 / Service Principals (Azure AD):** Use application identities (Service Principals) with specific permissions granted on the storage account.\n",
    "*   **Account Keys / Access Keys / SAS Tokens:** Direct keys or temporary tokens. Use with extreme caution, preferably only for testing or via secure mechanisms like Databricks Secrets.\n",
    "*   **Mounting:** Databricks allows mounting cloud storage locations to the Databricks File System (DBFS). This simplifies path access (e.g., `/mnt/mydata`) and centralizes credential management at the mount point.\n",
    "\n",
    "We will demonstrate configuration using Spark session configurations and Databricks Secrets where appropriate.\n",
    "\n",
    "### 3. Integrating with Azure Data Lake Storage (ADLS) Gen2\n",
    "\n",
    "ADLS Gen2 is Microsoft's optimized cloud storage solution for big data analytics workloads, built on Azure Blob Storage with features like a hierarchical namespace.\n",
    "\n",
    "#### 3.1 Theory: Authentication Methods for ADLS Gen2\n",
    "\n",
    "1.  **Service Principal with OAuth 2.0 (Recommended):** Create an Azure Active Directory application registration (Service Principal), grant it appropriate RBAC roles (e.g., \"Storage Blob Data Contributor\") on the ADLS Gen2 account, and use its credentials (Tenant ID, Client ID, Client Secret).\n",
    "2.  **Account Access Key:** The master key for the storage account. Provides full access. Less secure; avoid embedding directly.\n",
    "3.  **Shared Access Signature (SAS) Token:** Grants temporary, scoped permissions. Useful for specific, time-limited tasks.\n",
    "4.  **Credential Passthrough (Azure Databricks Feature):** Allows users to authenticate automatically using their Azure AD identity when accessing ADLS Gen2 (requires specific cluster configurations).\n",
    "5.  **Mounting using Service Principal or Account Key:** Centralizes access control.\n",
    "\n",
    "#### 3.2 Code Example: Reading Parquet data from ADLS Gen2 using Service Principal\n",
    "\n",
    "**Setup (Run once per session or configure at cluster level):**\n",
    "\n",
    "```python\n",
    "# --- Configuration (Ideally use Databricks Secrets) ---\n",
    "# Replace with your actual values or retrieve from Databricks Secrets\n",
    "storage_account_name = \"your_adls_storage_account_name\"\n",
    "client_id            = dbutils.secrets.get(scope=\"your-secret-scope\", key=\"adls-client-id\")\n",
    "tenant_id            = dbutils.secrets.get(scope=\"your-secret-scope\", key=\"adls-tenant-id\")\n",
    "client_secret        = dbutils.secrets.get(scope=\"your-secret-scope\", key=\"adls-client-secret\")\n",
    "\n",
    "# --- Spark Session Configuration ---\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\")\n",
    "\n",
    "print(\"Spark configuration set for ADLS Gen2 access using Service Principal.\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `storage_account_name`, `client_id`, `tenant_id`, `client_secret`: Variables holding your ADLS Gen2 and Service Principal details. **Crucially**, `dbutils.secrets.get` is used to fetch sensitive values (like the client secret) from a pre-configured Databricks Secret Scope, avoiding hardcoding.\n",
    "2.  `spark.conf.set(...)`: These lines configure the Spark session to use the specified Service Principal for accessing the given ADLS Gen2 account via the `abfss` driver.\n",
    "    *   `fs.azure.account.auth.type...`: Sets the authentication type to OAuth.\n",
    "    *   `fs.azure.account.oauth.provider.type...`: Specifies the token provider class for client credentials flow.\n",
    "    *   `fs.azure.account.oauth2.client.id...`: Provides the Application (Client) ID.\n",
    "    *   `fs.azure.account.oauth2.client.secret...`: Provides the Client Secret. **Never hardcode this in production.**\n",
    "    *   `fs.azure.account.oauth2.client.endpoint...`: Specifies the Azure AD token endpoint, including the Tenant ID.\n",
    "\n",
    "**Reading Data:**\n",
    "\n",
    "```python\n",
    "# --- Reading Data ---\n",
    "# Define filesystem and path\n",
    "filesystem_name = \"your-filesystem-name\" # Container name in ADLS Gen2\n",
    "data_path = f\"abfss://{filesystem_name}@{storage_account_name}.dfs.core.windows.net/raw/sales/year=2023/\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to read Parquet data from: {data_path}\")\n",
    "    df_adls = spark.read.format(\"parquet\").load(data_path)\n",
    "\n",
    "    print(\"Successfully read data from ADLS Gen2.\")\n",
    "    df_adls.show(5)\n",
    "    print(f\"Number of records read: {df_adls.count()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from ADLS Gen2: {e}\")\n",
    "    # Consider more specific error handling based on potential exceptions\n",
    "    # (e.g., AnalysisException for path not found, authentication errors)\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `filesystem_name`: The name of the container (filesystem) within your ADLS Gen2 storage account.\n",
    "2.  `data_path`: Constructs the full `abfss` path to the target data directory. Note the hierarchical structure `/raw/sales/year=2023/`, which might represent partitioned data.\n",
    "3.  `spark.read.format(\"parquet\").load(data_path)`: Instructs Spark to read data using the Parquet format reader from the specified ADLS Gen2 path. Spark uses the session configurations set earlier for authentication.\n",
    "4.  `df_adls.show(5)`: Displays the first 5 rows of the loaded DataFrame.\n",
    "5.  `df_adls.count()`: Triggers an action to count the total records, verifying the read operation.\n",
    "6.  `try...except`: Basic error handling to catch potential issues during the read operation (e.g., path not found, permission errors).\n",
    "\n",
    "#### 3.3 Code Example: Writing Partitioned Data to ADLS Gen2\n",
    "\n",
    "```python\n",
    "# Assume df_adls is the DataFrame read previously or another processed DataFrame\n",
    "# Example: Add a processing timestamp column\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_to_write = df_adls.withColumn(\"processing_ts\", current_timestamp())\n",
    "\n",
    "# --- Writing Data ---\n",
    "output_filesystem = \"processed-data\" # Target filesystem\n",
    "output_base_path = f\"abfss://{output_filesystem}@{storage_account_name}.dfs.core.windows.net/sales_summary/\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to write DataFrame to: {output_base_path}\")\n",
    "    (df_to_write.write\n",
    "        .format(\"delta\") # Using Delta Lake format (recommended)\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"country\") # Partition data by year and country columns\n",
    "        .option(\"path\", output_base_path) # Explicitly specify path using option\n",
    "        # .option(\"mergeSchema\", \"true\") # Useful option for schema evolution with Delta\n",
    "        .save() # save() is used with .option(\"path\", ...)\n",
    "        # Alternatively: .save(output_base_path) without .option(\"path\", ...)\n",
    "     )\n",
    "\n",
    "    print(f\"Successfully wrote partitioned data to ADLS Gen2 at {output_base_path}\")\n",
    "\n",
    "    # Verify by listing the created directories (optional)\n",
    "    print(\"Listing partitions created:\")\n",
    "    dbutils.fs.ls(output_base_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to ADLS Gen2: {e}\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `df_to_write`: The DataFrame intended for writing (here, we added a timestamp column for demonstration).\n",
    "2.  `output_filesystem`, `output_base_path`: Define the target location in ADLS Gen2.\n",
    "3.  `df_to_write.write`: Initiates the DataFrameWriter API.\n",
    "4.  `.format(\"delta\")`: Specifies the output format as Delta Lake. Delta Lake provides ACID transactions, time travel, and schema enforcement/evolution, making it highly suitable for data lake workloads, especially in Databricks. Parquet (`.format(\"parquet\")`) is also common.\n",
    "5.  `.mode(\"overwrite\")`: If data exists at the target path, it will be completely replaced. Other modes: \"append\", \"ignore\", \"errorifexists\".\n",
    "6.  `.partitionBy(\"year\", \"country\")`: Instructs Spark to partition the output data physically based on the unique combinations of values in the `year` and `country` columns. This creates a directory structure like `.../sales_summary/year=2023/country=USA/`, `.../sales_summary/year=2023/country=CA/`, etc. This significantly improves query performance when filtering on partition columns.\n",
    "7.  `.option(\"path\", output_base_path)`: Specifies the output directory path.\n",
    "8.  `.save()`: Executes the write operation. When using `.option(\"path\", ...)`, `save()` is called without arguments. Alternatively, `.save(output_base_path)` can be used without the `.option(\"path\", ...)` line.\n",
    "9.  `dbutils.fs.ls(output_base_path)`: Uses Databricks utilities to list the contents of the output directory, showing the partition folders created.\n",
    "\n",
    "**Use Case:** Writing cleaned, transformed, or aggregated data back to ADLS Gen2 for downstream consumption by reporting tools, ML models, or other Spark jobs. Partitioning is crucial for optimizing subsequent reads that filter by `year` or `country`.\n",
    "\n",
    "### 4. Integrating with AWS S3\n",
    "\n",
    "Amazon Simple Storage Service (S3) is a widely used object storage service.\n",
    "\n",
    "#### 4.1 Theory: Authentication Methods for S3\n",
    "\n",
    "1.  **IAM Roles via Instance Profiles (Recommended in Databricks on AWS):** Associate an EC2 instance profile with an IAM role that has the necessary S3 permissions (e.g., `s3:GetObject`, `s3:PutObject`, `s3:ListBucket`) to the Databricks cluster. Spark automatically uses these permissions. This is the most secure method on AWS. *Note: When using Databricks on Azure, accessing S3 often requires explicit key configuration or potentially cross-cloud identity federation.*\n",
    "2.  **Access Keys (Access Key ID & Secret Access Key):** User-specific or role-specific keys. **Avoid embedding directly.** Use Databricks Secrets or configure via Spark session/cluster settings.\n",
    "3.  **Mounting using Instance Profile or Access Keys:** Similar to ADLS, S3 buckets can be mounted to DBFS.\n",
    "\n",
    "#### 4.2 Code Example: Reading CSV data from S3 using Access Keys (via Secrets)\n",
    "\n",
    "**Setup (Run once per session or configure at cluster level):**\n",
    "\n",
    "```python\n",
    "# --- Configuration (Using Databricks Secrets for Keys) ---\n",
    "# Ensure you have secrets named 'aws-access-key-id' and 'aws-secret-access-key'\n",
    "# in a scope named 'aws-secrets' (replace with your scope name)\n",
    "try:\n",
    "    aws_access_key_id = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-access-key-id\")\n",
    "    aws_secret_access_key = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-secret-access-key\")\n",
    "\n",
    "    # --- Spark Session Configuration for s3a connector ---\n",
    "    # Use temporary credentials provider if possible, otherwise direct key setting\n",
    "    # Direct key setting (less secure if keys are long-lived, but common)\n",
    "    spark.conf.set(\"fs.s3a.access.key\", aws_access_key_id)\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
    "    # Optional: Specify endpoint if needed (e.g., for region-specific or private endpoints)\n",
    "    # spark.conf.set(\"fs.s3a.endpoint\", \"s3.us-west-2.amazonaws.com\")\n",
    "\n",
    "    print(\"Spark configuration set for S3 access using Access Keys.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving secrets or setting Spark conf: {e}. Ensure secrets scope and keys exist.\")\n",
    "    # Handle the error appropriately, perhaps by stopping execution\n",
    "    dbutils.notebook.exit(\"Failed to configure S3 access\")\n",
    "\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `dbutils.secrets.get(...)`: Retrieves AWS access key ID and secret access key from Databricks Secrets. This prevents hardcoding credentials.\n",
    "2.  `spark.conf.set(\"fs.s3a.access.key\", ...)`: Sets the AWS Access Key ID for the `s3a` filesystem connector.\n",
    "3.  `spark.conf.set(\"fs.s3a.secret.key\", ...)`: Sets the AWS Secret Access Key for the `s3a` connector.\n",
    "4.  `try...except`: Essential for handling cases where secrets might not be configured correctly. `dbutils.notebook.exit` stops execution if configuration fails.\n",
    "\n",
    "**Reading Data:**\n",
    "\n",
    "```python\n",
    "# --- Reading Data ---\n",
    "s3_bucket_name = \"your-s3-bucket-name\"\n",
    "s3_data_path = f\"s3a://{s3_bucket_name}/landing/customer_data.csv\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to read CSV data from: {s3_data_path}\")\n",
    "    df_s3 = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\") # Treat the first row as header\n",
    "             .option(\"inferSchema\", \"true\") # Infer data types (can be slow, provide schema for performance)\n",
    "             .load(s3_data_path)\n",
    "            )\n",
    "\n",
    "    print(\"Successfully read data from S3.\")\n",
    "    df_s3.show(5)\n",
    "    df_s3.printSchema() # Show inferred schema\n",
    "    print(f\"Number of records read: {df_s3.count()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from S3: {e}\")\n",
    "\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `s3_bucket_name`, `s3_data_path`: Define the target S3 bucket and the full `s3a` path to the CSV file.\n",
    "2.  `spark.read.format(\"csv\")`: Specifies the CSV data format reader.\n",
    "3.  `.option(\"header\", \"true\")`: Informs Spark that the CSV file has a header row.\n",
    "4.  `.option(\"inferSchema\", \"true\")`: Asks Spark to automatically detect column data types by reading a sample of the data. **Note:** For large datasets or production jobs, providing an explicit schema using `.schema(your_schema)` is highly recommended for performance and reliability.\n",
    "5.  `.load(s3_data_path)`: Loads the data from the specified S3 path using the credentials configured in the Spark session.\n",
    "6.  `df_s3.printSchema()`: Displays the schema that Spark inferred (or that was provided).\n",
    "\n",
    "#### 4.3 Code Example: Writing Parquet Data to S3\n",
    "\n",
    "```python\n",
    "# Assume df_s3 is the DataFrame read previously or another processed DataFrame\n",
    "\n",
    "# --- Writing Data ---\n",
    "output_s3_path = f\"s3a://{s3_bucket_name}/processed/customer_profiles/\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to write DataFrame to: {output_s3_path}\")\n",
    "    (df_s3.write # Assuming df_s3 is the DataFrame to write\n",
    "        .format(\"parquet\") # Parquet is generally preferred over CSV for analytics\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"compression\", \"snappy\") # Specify compression codec (snappy is common for Parquet)\n",
    "        .partitionBy(\"signup_year\") # Example partitioning by a 'signup_year' column\n",
    "        .save(output_s3_path)\n",
    "     )\n",
    "\n",
    "    print(f\"Successfully wrote partitioned Parquet data to S3 at {output_s3_path}\")\n",
    "\n",
    "    # Verify (optional)\n",
    "    print(\"Listing partitions created:\")\n",
    "    dbutils.fs.ls(output_s3_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to S3: {e}\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `output_s3_path`: Defines the target S3 path for the output.\n",
    "2.  `.format(\"parquet\")`: Specifies writing data in the efficient, columnar Parquet format.\n",
    "3.  `.mode(\"overwrite\")`: Replaces existing data at the location.\n",
    "4.  `.option(\"compression\", \"snappy\")`: Sets the compression codec for the Parquet files. Snappy offers a good balance between compression ratio and CPU overhead. Gzip provides better compression but requires more CPU.\n",
    "5.  `.partitionBy(\"signup_year\")`: Partitions the output based on the values in the `signup_year` column.\n",
    "6.  `.save(output_s3_path)`: Executes the write operation to S3.\n",
    "\n",
    "**Use Case:** Storing processed customer data back into S3 in an optimized format (Parquet) and partitioned for efficient querying based on signup year.\n",
    "\n",
    "### 5. Integrating with Google Cloud Storage (GCS)\n",
    "\n",
    "GCS is Google Cloud's scalable object storage service.\n",
    "\n",
    "#### 5.1 Theory: Authentication Methods for GCS\n",
    "\n",
    "1.  **Service Account Key Files:** Download a JSON key file for a Google Cloud Service Account that has appropriate GCS permissions (e.g., `roles/storage.objectAdmin` or more granular roles). This key needs to be accessible by the Spark driver and executors. Often configured via Spark properties.\n",
    "2.  **GCE Service Accounts (Recommended when running on Google Cloud):** If Databricks (or Spark) runs on Google Compute Engine (GCE) instances, the instances' attached service account can be used automatically by the GCS connector.\n",
    "3.  **Mounting using Service Account Key:** GCS buckets can be mounted to DBFS using service account credentials.\n",
    "\n",
    "#### 5.2 Code Example: Reading JSON data from GCS using a Service Account Key File\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "*   **Prerequisite:** You need a Service Account JSON key file. For security, upload this key file securely to DBFS or another location accessible by the cluster *without* checking it into version control. **Using Databricks Secrets to store the key file content is a better approach.**\n",
    "\n",
    "**Method 1: Key File Path (Less Secure if path is accessible)**\n",
    "\n",
    "```python\n",
    "# --- Configuration ---\n",
    "# Assume key file is uploaded to DBFS\n",
    "key_file_path_dbfs = \"/dbfs/path/to/your-gcp-keyfile.json\" # Path accessible by driver/executors\n",
    "gcs_project_id = \"your-gcp-project-id\" # Optional but good practice\n",
    "\n",
    "# --- Spark Session Configuration ---\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", key_file_path_dbfs)\n",
    "# spark.conf.set(\"spark.hadoop.fs.gs.project.id\", gcs_project_id) # Set project ID if needed\n",
    "\n",
    "print(\"Spark configuration set for GCS access using JSON key file.\")\n",
    "\n",
    "# Verification (optional): Check if the key file exists from the driver's perspective\n",
    "import os\n",
    "if not os.path.exists(key_file_path_dbfs.replace(\"/dbfs\", \"\")): # Adjust path for direct OS access if needed\n",
    "    print(f\"Warning: Key file not found at {key_file_path_dbfs}. GCS access might fail.\")\n",
    "    # dbutils.notebook.exit(\"GCS Key file not found\")\n",
    "```\n",
    "\n",
    "**Method 2: Key Content via Databricks Secret (More Secure)**\n",
    "\n",
    "```python\n",
    "# --- Configuration (Using Databricks Secret for Key Content) ---\n",
    "# Prerequisite: Store the *entire content* of the JSON key file as a secret value.\n",
    "# Use multiline secret support in Databricks UI if needed.\n",
    "try:\n",
    "    key_file_content = dbutils.secrets.get(scope=\"gcp-secrets\", key=\"gcs-keyfile-json-content\")\n",
    "    gcs_project_id = dbutils.secrets.get(scope=\"gcp-secrets\", key=\"gcp-project-id\")\n",
    "\n",
    "    # Encode the key content to Base64 - the connector often expects this\n",
    "    import base64\n",
    "    encoded_key = base64.b64encode(key_file_content.encode()).decode()\n",
    "\n",
    "    # --- Spark Session Configuration ---\n",
    "    spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "    # Use the encoded key property instead of the file path\n",
    "    spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.key\", encoded_key)\n",
    "    spark.conf.set(\"spark.hadoop.fs.gs.project.id\", gcs_project_id) # Set project ID\n",
    "\n",
    "    print(\"Spark configuration set for GCS access using key content from Databricks Secrets.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving secrets or setting Spark conf for GCS: {e}.\")\n",
    "    dbutils.notebook.exit(\"Failed to configure GCS access\")\n",
    "\n",
    "```\n",
    "\n",
    "**Code Explanation (Method 2):**\n",
    "\n",
    "1.  `dbutils.secrets.get(...)`: Retrieves the JSON key file's *content* and the GCP project ID from Databricks Secrets.\n",
    "2.  `base64.b64encode(...).decode()`: Encodes the JSON key content into Base64 format, as required by the `google.cloud.auth.service.account.json.key` Hadoop configuration property.\n",
    "3.  `spark.conf.set(...)`: Configures Spark:\n",
    "    *   Enables service account authentication.\n",
    "    *   Provides the Base64-encoded key content directly via `...json.key`. This avoids exposing a key file path.\n",
    "    *   Sets the GCP project ID associated with the bucket/service account.\n",
    "\n",
    "**Reading Data:**\n",
    "\n",
    "```python\n",
    "# --- Reading Data ---\n",
    "gcs_bucket_name = \"your-gcs-bucket-name\"\n",
    "gcs_data_path = f\"gs://{gcs_bucket_name}/raw_logs/events.json\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to read JSON data from: {gcs_data_path}\")\n",
    "    # Reading multi-line JSON file (each line is a valid JSON object)\n",
    "    df_gcs = (spark.read\n",
    "              .format(\"json\")\n",
    "              # .option(\"multiline\", \"true\") # Use if the entire file is a single JSON array/object\n",
    "              .load(gcs_data_path)\n",
    "             )\n",
    "\n",
    "    print(\"Successfully read data from GCS.\")\n",
    "    df_gcs.show(5, truncate=False)\n",
    "    df_gcs.printSchema()\n",
    "    print(f\"Number of records read: {df_gcs.count()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from GCS: {e}\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `gcs_bucket_name`, `gcs_data_path`: Define the target GCS bucket and the full `gs://` path to the JSON data.\n",
    "2.  `spark.read.format(\"json\").load(gcs_data_path)`: Reads data using the JSON format reader. By default, it expects one JSON object per line. Use `.option(\"multiline\", \"true\")` if the entire file constitutes a single JSON object or array.\n",
    "3.  The read operation uses the GCS credentials configured previously in the Spark session.\n",
    "\n",
    "#### 5.3 Code Example: Writing Avro Data to GCS\n",
    "\n",
    "```python\n",
    "# Assume df_gcs is the DataFrame read previously or another processed DataFrame\n",
    "\n",
    "# --- Writing Data ---\n",
    "output_gcs_path = f\"gs://{gcs_bucket_name}/processed_logs/events_avro/\"\n",
    "\n",
    "# Note: Writing Avro often requires an external package\n",
    "# Ensure 'org.apache.spark:spark-avro_2.12:<spark_version>' is added to the cluster libraries\n",
    "# Replace <spark_version> with your cluster's Spark version (e.g., 3.3.0)\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to write DataFrame to: {output_gcs_path}\")\n",
    "    (df_gcs.write\n",
    "        .format(\"avro\") # Specify Avro format\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"event_date\") # Example partitioning by an 'event_date' column\n",
    "        .save(output_gcs_path)\n",
    "     )\n",
    "\n",
    "    print(f\"Successfully wrote partitioned Avro data to GCS at {output_gcs_path}\")\n",
    "\n",
    "    # Verify (optional)\n",
    "    print(\"Listing partitions created:\")\n",
    "    dbutils.fs.ls(output_gcs_path)\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch potential ClassNotFoundException if avro package isn't installed\n",
    "    if \"Class not found\" in str(e) and \"avro\" in str(e):\n",
    "         print(\"Error writing to GCS: Spark Avro library might be missing. Please install 'org.apache.spark:spark-avro_2.12:<spark_version>' on the cluster.\")\n",
    "    else:\n",
    "        print(f\"Error writing to GCS: {e}\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `output_gcs_path`: Defines the target GCS path.\n",
    "2.  `.format(\"avro\")`: Specifies the Avro output format. Avro is a schema-based binary format, good for schema evolution and integration with various systems. **Requires the `spark-avro` library to be installed on the cluster.**\n",
    "3.  `.mode(\"overwrite\")`: Overwrites existing data.\n",
    "4.  `.partitionBy(\"event_date\")`: Partitions the output based on the `event_date` column.\n",
    "5.  `.save(output_gcs_path)`: Executes the write operation to GCS.\n",
    "6.  The `except` block includes a check for a common error if the required Avro library is missing.\n",
    "\n",
    "**Use Case:** Storing processed event logs in Avro format on GCS, partitioned by date for efficient time-based analysis.\n",
    "\n",
    "### 6. Advanced Concepts and Best Practices\n",
    "\n",
    "#### 6.1 Secure Credential Management Recap\n",
    "\n",
    "*   **Priority 1: IAM Roles / Managed Identities / Credential Passthrough.** Where possible, use identity-based authentication tied to the compute resource (Databricks cluster) or the user.\n",
    "*   **Priority 2: Databricks Secrets.** Store keys, secrets, and tokens in Databricks Secret Scopes and access them using `dbutils.secrets.get()`. Grant appropriate permissions (e.g., READ) on the secret scope to users or groups who need access.\n",
    "*   **Avoid:** Hardcoding credentials in notebooks, configuration files checked into Git, or cluster environment variables directly visible in the UI.\n",
    "\n",
    "#### 6.2 Data Partitioning Strategy\n",
    "\n",
    "*   **Why Partition?** Partitioning data on write organizes data into subdirectories based on column values. When querying, Spark can prune partitions (skip reading irrelevant directories) if the query includes filters on the partition columns (e.g., `WHERE year = 2023 AND country = 'UK'`). This drastically reduces the amount of data scanned and improves query performance.\n",
    "*   **Choosing Partition Columns:**\n",
    "    *   Select columns frequently used in query `WHERE` clauses.\n",
    "    *   Avoid columns with very high cardinality (too many unique values), as this creates too many small directories/files, potentially degrading performance (filesystem listing overhead). Aim for a reasonable number of partitions (hundreds to low thousands is often manageable, but depends on the filesystem).\n",
    "    *   Consider data skew. If one partition value has vastly more data, it can become a bottleneck.\n",
    "*   **Syntax:** `df.write.partitionBy(\"col1\", \"col2\").save(...)`\n",
    "\n",
    "#### 6.3 Performance Tuning for Cloud Storage I/O\n",
    "\n",
    "*   **File Formats:**\n",
    "    *   **Delta Lake (Recommended in Databricks):** Provides ACID transactions, time travel, schema enforcement/evolution, and optimizations like Z-Ordering (multi-dimensional clustering) and optimized writes. Built on Parquet. Solves the \"small file problem\" better than raw Parquet.\n",
    "    *   **Parquet:** Excellent general-purpose columnar format. Good compression, supports predicate pushdown. Default choice if not using Delta.\n",
    "    *   **ORC:** Another columnar format, often used in the Hadoop ecosystem.\n",
    "    *   **Avro:** Row-based, good for schema evolution and ingestion pipelines.\n",
    "    *   **Avoid:** Using plain text formats (CSV, JSON) for large analytical datasets due to performance limitations (no predicate pushdown, slower parsing, larger size). Use them primarily for ingestion/egress if required by external systems.\n",
    "*   **File Size:** Aim for reasonably sized files (e.g., 128MB - 1GB).\n",
    "    *   *Too many small files:* Increases metadata overhead (listing files in cloud storage can be slow), reduces I/O throughput. Use `.repartition()` or `.coalesce()` on the DataFrame *before* writing if necessary, or leverage Delta Lake's `OPTIMIZE` command (especially with `ZORDER BY`).\n",
    "    *   *Too few large files:* Reduces parallelism during reads. Spark reads files in parallel based on task slots.\n",
    "*   **Compression:** Use splittable compression codecs for columnar formats like Parquet (e.g., Snappy, Gzip - though Gzip isn't splittable itself, Parquet's internal structure often allows splitting). Compression reduces storage costs and network I/O but adds CPU overhead during read/write. Snappy is often a good default.\n",
    "*   **Databricks I/O (DBIO) Cache:** Databricks clusters can cache data read from cloud storage on local SSDs (if available on the worker nodes). Subsequent reads of the same data can be served from the cache, significantly speeding up access. This happens automatically for certain instance types.\n",
    "*   **Commit Protocols:** Spark uses commit protocols to ensure write operations to distributed filesystems are atomic. Different protocols exist (e.g., `DirectParquetOutputCommitter`). Databricks often optimizes these for cloud storage, especially with Delta Lake, reducing commit times for large jobs.\n",
    "\n",
    "#### 6.4 Choosing the Right File Format\n",
    "\n",
    "| Feature          | Delta Lake                       | Parquet                         | Avro                            | CSV / JSON                      |\n",
    "| :--------------- | :------------------------------- | :------------------------------ | :------------------------------ | :------------------------------ |\n",
    "| **Type**         | Table Format (on Parquet)        | Columnar                        | Row-based (Binary)              | Text (Row-based)                |\n",
    "| **Schema**       | Enforced, Evolution              | Embedded, Evolution possible    | Embedded, Evolution built-in  | Optional / Inferred             |\n",
    "| **Transactions** | ACID                             | No                              | No                              | No                              |\n",
    "| **Time Travel**  | Yes                              | No                              | No                              | No                              |\n",
    "| **Performance**  | Very High (Optimized Parquet)    | High (Columnar)                 | Good (Schema helps)             | Low (Parsing, no pushdown)    |\n",
    "| **Compression**  | Good (Inherited from Parquet)    | Good                            | Good                            | Moderate (Text)                 |\n",
    "| **Splittable**   | Yes                              | Yes                             | Yes                             | Yes (CSV), Sometimes (JSON)     |\n",
    "| **Use Case**     | Data Lakes, Reliable Pipelines | General Analytics, Data Lakes | Ingestion, Schema Evolution | Data Exchange, Simple Logs    |\n",
    "| **DBX Feature**  | `OPTIMIZE`, `ZORDER`, Caching    | Caching                         | Caching                         | Caching                         |\n",
    "\n",
    "**Recommendation:** Use **Delta Lake** as the default for storing analytical data within Azure Databricks due to its reliability, performance features, and seamless integration. Use Parquet if Delta is not an option or for interchange outside the Delta ecosystem. Use Avro for schema-driven ingestion/egress. Use CSV/JSON primarily for initial ingestion or final output to systems requiring text formats.\n",
    "\n",
    "### 7. Platform Comparison: Databricks vs EMR vs Synapse (Cloud Storage Focus)\n",
    "\n",
    "| Feature                   | Azure Databricks                                    | AWS EMR                                             | Azure Synapse Analytics (Spark Pools)            |\n",
    "| :------------------------ | :-------------------------------------------------- | :-------------------------------------------------- | :----------------------------------------------- |\n",
    "| **Primary Cloud**         | Azure (also on AWS, GCP)                            | AWS                                                 | Azure                                            |\n",
    "| **Primary Storage**       | ADLS Gen2 (optim., native), S3, GCS                 | S3 (highly optim., native IAM), HDFS (optional)     | ADLS Gen2 (highly optim., native), Azure Blob  |\n",
    "| **ADLS Gen2 Integration** | Excellent (ABFSS, Mounting, Secrets, Passthrough) | Possible via connectors, less native than DBX/Synapse | Excellent (ABFSS, Managed Identity, Native)    |\n",
    "| **S3 Integration**        | Good (s3a, Mounting, Secrets)                     | Excellent (s3a, EMRFS, Native IAM Roles)            | Possible via connectors, less native           |\n",
    "| **GCS Integration**       | Good (gs connector, Secrets, Mounting)              | Good (gs connector, Service Accounts)               | Possible via connectors, less native           |\n",
    "| **Auth Preferred**        | Service Principals, Managed ID, Passthrough (Azure) | IAM Roles (Instance Profiles) (AWS)                 | Managed Identities, Service Principals (Azure) |\n",
    "| **Credential Mgmt**       | Databricks Secrets                                  | IAM Roles, AWS Secrets Manager                      | Azure Key Vault Integration, Managed ID        |\n",
    "| **Optimized I/O**         | DBIO Cache, Delta Optimizations                     | EMRFS (S3 optimizations), Runtime optimizations     | Runtime optimizations, Native ADLS connector   |\n",
    "| **Mounting Abstraction**  | Yes (DBFS Mounts for ADLS, S3, GCS)                 | Less common; direct paths or HDFS overlays used     | Less common; direct paths used                 |\n",
    "| **Ease of Use (Storage)** | High (Unified interface, mounting)                  | Moderate-High (Excellent S3, others require config) | High (Excellent ADLS, others require config)   |\n",
    "| **Format Focus**          | Delta Lake (Heavily promoted & optimized)           | Parquet, ORC, Avro (Open source focus)              | Delta Lake, Parquet, CSV (Integrated focus)    |\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "*   **Databricks:** Offers a consistent, user-friendly experience across clouds (though best on its primary cloud, Azure). Strong focus on Delta Lake and provides helpful abstractions like DBFS mounts and integrated secret management. DBIO caching is a key performance benefit.\n",
    "*   **EMR:** Deepest integration with AWS services, particularly S3 and IAM roles. Highly flexible with Hadoop ecosystem components. Configuration for non-AWS storage requires more manual setup.\n",
    "*   **Synapse:** Best-in-class integration with Azure Data Lake Storage Gen2 and other Azure services via Managed Identities. Optimized for Azure ecosystem, less native support for S3/GCS compared to Databricks/EMR in their respective clouds.\n",
    "\n",
    "Choose the platform based on your primary cloud ecosystem, required storage integrations, and desired level of managed service vs. flexibility.\n",
    "\n",
    "### 8. Conclusion\n",
    "\n",
    "Integrating PySpark with cloud storage is fundamental to building scalable big data pipelines. Azure Databricks provides robust and often optimized mechanisms for connecting to ADLS Gen2, S3, and GCS. Key takeaways include:\n",
    "\n",
    "*   Always prioritize secure authentication methods (IAM/Managed Identities, Service Principals, Databricks Secrets).\n",
    "*   Use the appropriate URI schemes (`abfss://`, `s3a://`, `gs://`).\n",
    "*   Leverage the DataFrameReader/Writer API with correct formats and options.\n",
    "*   Use partitioning (`partitionBy`) strategically to optimize query performance.\n",
    "*   Choose appropriate file formats (Delta Lake or Parquet preferred for analytics).\n",
    "*   Be mindful of file sizes and compression for I/O efficiency.\n",
    "*   Understand the strengths of each platform (Databricks, EMR, Synapse) concerning storage integration in their respective cloud environments.\n",
    "\n",
    "By mastering these concepts, developers can effectively leverage the power of PySpark and the scalability of cloud storage for their data processing needs.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
