{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534b1354",
   "metadata": {},
   "source": [
    "# Lesson 18 - Streaming Transformations and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e2397",
   "metadata": {},
   "source": [
    "Okay, here are detailed technical notes on PySpark Structured Streaming, focusing on transformations, aggregations, joins with static data, and various sinks. These notes are designed for professional learners and can be adapted for training materials or reference documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Structured Streaming: Transformations, Joins, and Sinks\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "PySpark Structured Streaming provides a high-level API for building scalable, fault-tolerant, end-to-end streaming applications. It leverages the Spark SQL engine, allowing developers to express streaming computations in the same way they express batch computations on static data using DataFrames and Datasets. The core abstraction is the DataFrame/Dataset, representing an unbounded table where new data continuously arrives. This model enables powerful transformations, aggregations, and joins on streaming data.\n",
    "\n",
    "This document delves into key aspects of processing streaming data: performing aggregations, joining streams with static datasets, and writing results to various external systems (sinks).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Aggregations in Streaming DataFrames\n",
    "\n",
    "**Theory**\n",
    "\n",
    "Aggregating data over a stream is fundamentally different from batch aggregation because the dataset is unbounded. Structured Streaming handles this through **stateful processing**. When performing aggregations (e.g., counts, sums, averages), Spark maintains intermediate state across micro-batches.\n",
    "\n",
    "*   **State Management:** Spark automatically manages the running state (e.g., the current count for each group) required for the aggregation. This state is stored reliably (using checkpointing) to ensure fault tolerance. However, without constraints, this state can grow indefinitely as new keys/groups arrive.\n",
    "*   **Watermarking:** To manage potentially unbounded state, Structured Streaming introduces **watermarking**. A watermark defines a threshold for how \"late\" data can be before it's ignored for stateful operations like aggregations. It allows Spark to track the progress of event time and automatically clean up old state associated with groups whose event time is significantly older than the watermark. Watermarks are crucial for limiting memory/disk usage for long-running streaming aggregations. They are typically defined on an event-time column.\n",
    "*   **Output Modes:** The way aggregation results are written to a sink depends on the chosen output mode:\n",
    "    *   `OutputMode.Complete()`: The entire updated result table (all aggregated groups) is written to the sink in every trigger interval. This is typically used for aggregations *without* watermarking, but can be resource-intensive as the full state is outputted each time. Not supported for aggregations with watermarking.\n",
    "    *   `OutputMode.Append()`: Only *new* rows added to the result table since the last trigger are written to the sink. This is applicable only when rows added to the result table will never be updated again. For aggregations, this usually requires watermarking, ensuring that once a window's aggregation is finalized (passed the watermark), it won't change and can be appended.\n",
    "    *   `OutputMode.Update()`: Only rows that were *updated* in the result table since the last trigger are written to the sink. If watermarking is used, rows corresponding to older, cleaned-up state are *not* outputted again. This is the default mode for aggregations with or without watermarking (when `Complete` isn't applicable).\n",
    "\n",
    "**Code Example: Windowed Event Count with Watermarking**\n",
    "\n",
    "Let's consider a stream of events with timestamps. We want to count events within 10-minute tumbling windows, updating every 5 minutes, and handle data arriving up to 1 hour late.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingAggregationExample\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) # Example config for parallelism\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") # Reduce verbosity\n",
    "\n",
    "# 2. Define schema for incoming data (e.g., from Kafka or socket)\n",
    "# Assuming data like: {\"timestamp\": \"2023-10-27T10:00:00Z\", \"device\": \"A\"}\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"device\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 3. Create a Streaming DataFrame (using Rate source for simplicity)\n",
    "# In a real scenario, replace 'rate' with 'kafka', 'socket', etc.\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(timestamp AS TIMESTAMP) as event_time\", # Rate source timestamp\n",
    "                \"CONCAT('device_', CAST(value % 3 AS STRING)) as device_id\") # Simulate device IDs\n",
    "\n",
    "# Add a small delay for demonstration if needed (optional)\n",
    "# streaming_df = streaming_df.withColumn(\"event_time\", col(\"event_time\") - expr(\"INTERVAL 2 MINUTES\"))\n",
    "\n",
    "# 4. Define Watermark and Perform Aggregation\n",
    "# Watermark: Allow data to be 1 hour late based on 'event_time'\n",
    "# Window: Tumble windows of 10 minutes duration\n",
    "# GroupBy: Group by window and device_id\n",
    "# Aggregation: Count events in each group\n",
    "windowed_counts = streaming_df \\\n",
    "    .withWatermark(\"event_time\", \"1 hour\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\"), # Tumbling window\n",
    "        col(\"device_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\") # Optional: Order output for clarity\n",
    "\n",
    "# 5. Define the Streaming Sink (Console for demonstration)\n",
    "# Output Mode: Update - only write updated rows to the console\n",
    "query = windowed_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='5 seconds') # Process data every 5 seconds\n",
    "    .start()\n",
    "\n",
    "# 6. Keep the query running\n",
    "query.awaitTermination()\n",
    "\n",
    "# 7. Stop the SparkSession (won't be reached in awaitTermination)\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `SparkSession.builder...getOrCreate()`: Standard way to create or get a SparkSession, the entry point for Spark functionality. `spark.sql.shuffle.partitions` is configured to control parallelism during shuffles (like in `groupBy`).\n",
    "2.  `StructType([...])`: Defines the expected structure and data types of the incoming streaming data. Crucial for schema enforcement.\n",
    "3.  `spark.readStream...load()`: Creates the initial streaming DataFrame. We use the `rate` source for generating test data with timestamps. `selectExpr` is used to cast the timestamp and create a simulated device ID column.\n",
    "4.  `.withWatermark(\"event_time\", \"1 hour\")`: Specifies that the `event_time` column should be used for watermarking, and the system should tolerate data arriving up to 1 hour late relative to the maximum event time seen so far.\n",
    "5.  `.groupBy(window(...), col(...))`: Groups the data. `window(col(\"event_time\"), \"10 minutes\")` creates 10-minute tumbling windows based on the `event_time`. We also group by `device_id`.\n",
    "6.  `.count()`: Performs the aggregation (counting rows) within each group (window and device).\n",
    "7.  `.orderBy(\"window\")`: Sorts the output micro-batch for better readability in the console (optional).\n",
    "8.  `windowed_counts.writeStream...start()`: Configures and starts the streaming query.\n",
    "    *   `.outputMode(\"update\")`: Specifies that only rows whose counts have changed since the last trigger should be written. With watermarking, this ensures that old, finalized windows are eventually removed from the output as their state is cleaned up.\n",
    "    *   `.format(\"console\")`: Specifies the sink type â€“ print output to the console.\n",
    "    *   `.option(\"truncate\", \"false\")`: Prevents truncation of long column values in the console output.\n",
    "    *   `.trigger(processingTime='5 seconds')`: Defines how often the stream should be processed (trigger interval).\n",
    "    *   `.start()`: Starts the query execution asynchronously.\n",
    "9.  `query.awaitTermination()`: Blocks the main thread, keeping the application alive until the streaming query is stopped (manually or due to an error).\n",
    "\n",
    "**Practical Use Cases:**\n",
    "\n",
    "*   Real-time monitoring dashboards (e.g., requests per minute per endpoint).\n",
    "*   Anomaly detection (e.g., sudden spikes in error counts within time windows).\n",
    "*   Sessionization of user activity based on event time.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Joining Streaming DataFrames with Static DataFrames\n",
    "\n",
    "**Theory**\n",
    "\n",
    "A common pattern in streaming applications is enriching incoming event data with relatively static reference or dimensional data. For example, joining a stream of user clicks (containing user IDs) with a static user profile table (containing user demographics).\n",
    "\n",
    "Structured Streaming supports joining a streaming DataFrame with a static DataFrame (or Dataset).\n",
    "\n",
    "*   **Supported Join Types:**\n",
    "    *   Inner Join\n",
    "    *   Left Outer Join: `streamingDF.join(staticDF, ...)` (stream rows must have a match in static data, or nulls are added for static columns).\n",
    "    *   *Right Outer and Full Outer joins are NOT supported* because they would require the static DataFrame to update the results indefinitely as new streaming data arrives without matches, which complicates state management.\n",
    "*   **Execution:** Spark's optimizer handles this join efficiently. If the static DataFrame is small enough (controlled by `spark.sql.autoBroadcastJoinThreshold`), it will be broadcast to all executors, avoiding expensive shuffles. For larger static DataFrames, a standard shuffle-based join will occur, but only involving the data from the current micro-batch of the stream. The static DataFrame is read once per micro-batch execution unless caching strategies are employed.\n",
    "\n",
    "**Code Example: Enriching Event Stream with User Metadata**\n",
    "\n",
    "Assume we have the same event stream as before (`streaming_df`) containing `device_id`, and a static DataFrame containing metadata for each device.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "\n",
    "# 1. Initialize Spark Session (reuse from previous example or create new)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingStaticJoinExample\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\") # Increase threshold if static data is larger but fits in memory\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. Create Static DataFrame (Device Metadata)\n",
    "# In a real scenario, load this from a file, database, etc.\n",
    "static_data = [\n",
    "    (\"device_0\", \"Factory A\", \"Sensor Model X\"),\n",
    "    (\"device_1\", \"Factory B\", \"Sensor Model Y\"),\n",
    "    (\"device_2\", \"Factory A\", \"Sensor Model X\")\n",
    "]\n",
    "static_schema = StructType([\n",
    "    StructField(\"static_device_id\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"model\", StringType(), False)\n",
    "])\n",
    "static_df = spark.createDataFrame(data=static_data, schema=static_schema)\n",
    "\n",
    "# Optional: Cache the static DataFrame for potentially faster access in repeated micro-batches\n",
    "static_df.cache()\n",
    "static_df.count() # Action to trigger caching\n",
    "\n",
    "# 3. Create Streaming DataFrame (reuse from previous example or define again)\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(timestamp AS TIMESTAMP) as event_time\",\n",
    "                \"CONCAT('device_', CAST(value % 4 AS STRING)) as device_id\") # Simulate some missing devices\n",
    "\n",
    "# 4. Perform the Join\n",
    "# Join Condition: streaming_df.device_id == static_df.static_device_id\n",
    "# Join Type: Left Outer (keep all stream events, add metadata if available)\n",
    "enriched_stream_df = streaming_df.join(\n",
    "    static_df,\n",
    "    streaming_df.device_id == static_df.static_device_id,\n",
    "    \"left_outer\"\n",
    ")\n",
    "\n",
    "# Select desired columns after join\n",
    "final_stream_df = enriched_stream_df.select(\n",
    "    \"event_time\",\n",
    "    \"device_id\",\n",
    "    \"location\", # This will be null if no match in static_df\n",
    "    \"model\"     # This will be null if no match in static_df\n",
    ")\n",
    "\n",
    "# 5. Define the Streaming Sink (Console)\n",
    "query = final_stream_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") # Append is suitable for simple transformations/joins\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    .start()\n",
    "\n",
    "# 6. Keep the query running\n",
    "query.awaitTermination()\n",
    "\n",
    "# 7. Stop SparkSession\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `SparkSession.builder...getOrCreate()`: Initializes Spark. `spark.sql.autoBroadcastJoinThreshold` is set to potentially broadcast the static DataFrame if it's under 100MB.\n",
    "2.  `static_data`, `static_schema`, `spark.createDataFrame()`: Creates a sample static DataFrame containing device metadata. In practice, this would be loaded from persistent storage (`spark.read.parquet(...)`, `spark.read.jdbc(...)`, etc.).\n",
    "3.  `static_df.cache()`: Suggests to Spark that this DataFrame should be kept in memory after the first computation. `static_df.count()` is an action that forces the DataFrame to be computed and cached. This can improve performance if the static data is accessed repeatedly across micro-batches.\n",
    "4.  `spark.readStream...load()`: Defines the streaming source DataFrame (similar to the aggregation example).\n",
    "5.  `streaming_df.join(static_df, join_condition, \"left_outer\")`: Performs the join.\n",
    "    *   `static_df`: The DataFrame to join with.\n",
    "    *   `streaming_df.device_id == static_df.static_device_id`: The condition defining how rows are matched between the two DataFrames.\n",
    "    *   `\"left_outer\"`: Specifies the join type. All rows from the `streaming_df` (left side) are kept. If a match is found in `static_df`, the corresponding columns (`location`, `model`) are added. If no match is found, these columns will have `null` values.\n",
    "6.  `enriched_stream_df.select(...)`: Selects and potentially renames columns for the final output stream.\n",
    "7.  `final_stream_df.writeStream...start()`: Configures and starts the streaming query.\n",
    "    *   `.outputMode(\"append\")`: Since this is a simple transformation/join without aggregation or watermarking impacting existing rows, `append` mode is suitable. Each micro-batch's results are simply appended to the sink.\n",
    "    *   `.format(\"console\")`: Writing output to the console.\n",
    "    *   `.trigger(processingTime='10 seconds')`: Processing interval.\n",
    "8.  `query.awaitTermination()`: Keeps the application running.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "\n",
    "*   Real-time ad targeting: Joining an impression stream (user ID, ad ID) with user segment data (static).\n",
    "*   IoT data enrichment: Joining sensor readings (sensor ID, value) with sensor metadata (location, calibration info).\n",
    "*   Fraud detection: Joining a transaction stream (account ID, amount) with account risk scores (static).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Streaming Sinks: Writing Streaming Data\n",
    "\n",
    "**Theory**\n",
    "\n",
    "A streaming sink is the destination where the results of a streaming query are written. Structured Streaming provides built-in support for various sinks, each with different characteristics and configuration options. A crucial requirement for most production sinks is **fault tolerance** and **exactly-once semantics** (or at-least-once). This is typically achieved using **checkpointing**.\n",
    "\n",
    "*   **Checkpointing:** Structured Streaming periodically saves the progress of the query (processed offsets, running state for aggregations) to a reliable distributed filesystem (like HDFS, S3, ADLS). If the query fails, it can restart from the last checkpoint, ensuring no data loss and preventing duplicate processing (for idempotent sinks or transactional sinks). `option(\"checkpointLocation\", \"/path/to/checkpoint/dir\")` is essential for most non-console sinks in production.\n",
    "\n",
    "**Common Sinks**\n",
    "\n",
    "**a) Console Sink**\n",
    "\n",
    "*   **Purpose:** Primarily for debugging and development. Prints output to the driver's console.\n",
    "*   **Fault Tolerance:** Not fault-tolerant. Does not support checkpointing directly for output (though internal state might be checkpointed if needed).\n",
    "*   **Semantics:** At-most-once (if driver fails, output might be lost).\n",
    "*   **Key Options:**\n",
    "    *   `numRows`: Number of rows to print (default 20).\n",
    "    *   `truncate`: Whether to truncate long string values (default true).\n",
    "\n",
    "```python\n",
    "# (Assuming 'result_stream_df' is the final streaming DataFrame)\n",
    "\n",
    "console_query = result_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") # Or \"append\", \"complete\" depending on the transformation\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 50) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# console_query.awaitTermination()\n",
    "```\n",
    "\n",
    "**b) File Sink**\n",
    "\n",
    "*   **Purpose:** Writing results to files in a directory on a distributed filesystem (HDFS, S3, Azure Blob Storage, etc.). Suitable for archiving or feeding downstream batch processes.\n",
    "*   **Fault Tolerance:** Fault-tolerant using checkpointing.\n",
    "*   **Semantics:** Exactly-once (achieved through checkpointing and transactional writes to a metadata log within the output directory).\n",
    "*   **Output Format:** Supports various file formats (`parquet`, `orc`, `json`, `csv`, etc.). Parquet is generally recommended for efficiency.\n",
    "*   **Key Options:**\n",
    "    *   `path`: The output directory path.\n",
    "    *   `checkpointLocation`: Path to the checkpoint directory (mandatory for fault tolerance).\n",
    "    *   `format`: File format (e.g., `\"parquet\"`, `\"json\"`).\n",
    "    *   `partitionBy`: (Optional) Partition the output data into subdirectories based on column values (e.g., `partitionBy(\"date\", \"hour\")`). Improves read performance for consumers filtering by partition columns.\n",
    "\n",
    "```python\n",
    "# (Assuming 'result_stream_df' is the final streaming DataFrame)\n",
    "\n",
    "file_query = result_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") # Recommended format\n",
    "    .outputMode(\"append\") # Append is common for file sinks\n",
    "    .option(\"path\", \"s3a://my-bucket/output/data\") # Output path\n",
    "    .option(\"checkpointLocation\", \"s3a://my-bucket/output/checkpoints\") # Checkpoint path\n",
    "    .partitionBy(\"event_date\") # Example partitioning column (must exist in result_stream_df)\n",
    "    .trigger(processingTime='1 minute') # Trigger less frequently for file sinks typically\n",
    "    .start()\n",
    "\n",
    "# file_query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation (File Sink):**\n",
    "\n",
    "1.  `.format(\"parquet\")`: Specifies the output file format.\n",
    "2.  `.outputMode(\"append\")`: New rows generated in each micro-batch are added to the output directory.\n",
    "3.  `.option(\"path\", ...)`: Defines the target directory where Parquet files will be written. Spark creates subdirectories for each micro-batch output.\n",
    "4.  `.option(\"checkpointLocation\", ...)`: **Crucial**. Specifies the directory for storing checkpoint information. This enables recovery from failures. Must be a reliable, distributed filesystem path.\n",
    "5.  `.partitionBy(\"event_date\")`: Instructs Spark to create subdirectories based on the unique values in the `event_date` column (e.g., `/path/event_date=2023-10-27/...`). This significantly speeds up reads that filter on `event_date`.\n",
    "6.  `.trigger(processingTime='1 minute')`: Sets the processing interval. File sinks often use longer intervals to avoid creating too many small files.\n",
    "\n",
    "**c) Kafka Sink**\n",
    "\n",
    "*   **Purpose:** Publishing results to Apache Kafka topics. Ideal for integrating with downstream real-time applications or microservices.\n",
    "*   **Fault Tolerance:** Fault-tolerant using checkpointing.\n",
    "*   **Semantics:** At-least-once by default. Exactly-once can be achieved if Kafka producer is configured idempotently (enabled by default in recent Kafka versions) and transactions are used (though Spark's Kafka sink doesn't use Kafka transactions explicitly, its checkpointing mechanism provides effective exactly-once guarantees when restarting).\n",
    "*   **Serialization:** Data needs to be serialized into a format Kafka understands, typically as key/value byte arrays. Often requires selecting specific columns and casting them (e.g., to JSON strings or binary formats like Avro).\n",
    "*   **Key Options:**\n",
    "    *   `kafka.bootstrap.servers`: List of Kafka broker addresses.\n",
    "    *   `topic`: The Kafka topic to write to.\n",
    "    *   `checkpointLocation`: Path to the checkpoint directory (mandatory for fault tolerance).\n",
    "    *   (Optional) `key.serializer`, `value.serializer` (if needed beyond default StringSerializer).\n",
    "    *   (Optional) Other Kafka producer configurations can be passed via `option(\"kafka.producer.some_config\", \"value\")`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "# (Assuming 'result_stream_df' has columns like 'key_col', 'value_col', 'other_data')\n",
    "\n",
    "# Prepare data for Kafka: often requires selecting key/value and serializing\n",
    "# Example: Serialize entire row as JSON string in the 'value'\n",
    "kafka_df = result_stream_df \\\n",
    "    .select(\n",
    "        col(\"key_col\").cast(\"string\").alias(\"key\"), # Kafka key (optional, string)\n",
    "        to_json(struct(\"*\")).alias(\"value\") # Kafka value (JSON string of all columns)\n",
    "    )\n",
    "# Alternative: select specific columns for value\n",
    "# kafka_df = result_stream_df.select(col(\"id\").cast(\"string\").alias(\"key\"), col(\"data_payload\").cast(\"string\").alias(\"value\"))\n",
    "\n",
    "\n",
    "kafka_query = kafka_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") # Usually append for Kafka unless updates are meaningful downstream\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-broker1:9092,kafka-broker2:9092\") # Kafka brokers\n",
    "    .option(\"topic\", \"output-topic\") # Target Kafka topic\n",
    "    .option(\"checkpointLocation\", \"/path/to/kafka/checkpoints\") # Checkpoint path\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    .start()\n",
    "\n",
    "# kafka_query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation (Kafka Sink):**\n",
    "\n",
    "1.  `.select(...)`: Prepares the DataFrame for Kafka. Kafka messages are key-value pairs.\n",
    "    *   `col(\"key_col\").cast(\"string\").alias(\"key\")`: Selects a column to be used as the Kafka message key and casts it to string. The `.alias(\"key\")` is important for the Kafka sink to identify it. Can be omitted if no key is needed.\n",
    "    *   `to_json(struct(\"*\")).alias(\"value\")`: Selects the Kafka message value. Here, `struct(\"*\")` creates a struct containing all columns from `result_stream_df`, and `to_json` converts this struct into a JSON string. `.alias(\"value\")` identifies this column as the value for the Kafka sink.\n",
    "2.  `.format(\"kafka\")`: Specifies the Kafka sink type.\n",
    "3.  `.outputMode(\"append\")`: Each processed row is sent as a new message to Kafka.\n",
    "4.  `.option(\"kafka.bootstrap.servers\", ...)`: Provides the connection string for the Kafka cluster.\n",
    "5.  `.option(\"topic\", ...)`: Specifies the Kafka topic name.\n",
    "6.  `.option(\"checkpointLocation\", ...)`: **Crucial** for fault tolerance and ensuring messages aren't missed or duplicated upon restarts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Advanced Considerations and Performance Tuning\n",
    "\n",
    "*   **State Store Management:** For aggregations with large state (many groups), the default in-memory state store might cause memory issues. Consider using the RocksDB-based state store for better scalability by spilling state to local disk:\n",
    "    ```python\n",
    "    spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\",\n",
    "                   \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\")\n",
    "    ```\n",
    "    This requires adding the `spark-sql-streaming-state-store-rocksdb` dependency.\n",
    "*   **Shuffle Partitions:** Tune `spark.sql.shuffle.partitions` based on cluster size and data volume. Too few partitions limit parallelism; too many can cause overhead with small tasks. Monitor task durations in the Spark UI.\n",
    "*   **Trigger Interval:** Choose `Trigger.ProcessingTime` based on latency requirements vs. resource usage. Smaller intervals mean lower latency but higher overhead. `Trigger.Once()` processes all available data in one batch (useful for quasi-batch processing). `Trigger.Continuous()` offers the lowest latency (experimental, sub-millisecond) but has more constraints.\n",
    "*   **File Sink Optimization:**\n",
    "    *   **Partitioning:** Use `partitionBy` wisely based on common query filters for downstream consumers. Avoid partitioning on high-cardinality columns.\n",
    "    *   **Small Files:** Very frequent triggers with file sinks can create many small files, which is inefficient for HDFS/S3. Consider longer trigger intervals or using Delta Lake as a sink, which handles small files better through compaction.\n",
    "*   **Data Skew:** In aggregations or joins, if certain keys are much more frequent than others, they can create straggler tasks. Techniques like salting (adding a random prefix/suffix to skewed keys) can sometimes help, but are more complex to implement correctly in streaming.\n",
    "*   **Early Filtering/Projection:** Filter data and select only necessary columns as early as possible in the streaming query plan to reduce data shuffled and processed.\n",
    "*   **Static Join Optimization:** Ensure `spark.sql.autoBroadcastJoinThreshold` is set appropriately for stream-static joins, or explicitly broadcast the static DataFrame (`from pyspark.sql.functions import broadcast; streaming_df.join(broadcast(static_df), ...)`) if it fits comfortably in executor memory. Cache the static DataFrame if it's read repeatedly.\n",
    "*   **Monitoring:** Use the Spark UI (especially the Structured Streaming tab) to monitor input/processing rates, batch durations, state store memory usage, and potential bottlenecks. Ganglia, Prometheus, and Grafana can provide cluster-level monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "PySpark Structured Streaming offers a robust framework for complex stream processing. Understanding how to implement stateful aggregations using watermarking, perform efficient joins with static data, and correctly configure sinks with checkpointing is crucial for building reliable and scalable real-time applications. Performance tuning often involves balancing latency, throughput, and resource utilization by adjusting configurations related to parallelism, state management, trigger intervals, and sink-specific options. Careful design and monitoring are key to successful streaming deployments.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
