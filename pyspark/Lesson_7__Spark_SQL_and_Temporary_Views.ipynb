{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27df29f5",
   "metadata": {},
   "source": [
    "# Lesson 7 - Spark SQL and Temporary Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42c514",
   "metadata": {},
   "source": [
    "Okay, let's structure the technical notes for Lesson 7, focusing on Spark SQL, Temporary Views, and UDFs in PySpark for professional learners.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Lesson 7 - Spark SQL, Temporary Views, and User-Defined Functions\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Apache Spark's power lies not only in its distributed processing capabilities but also in its versatile APIs. Spark SQL is a critical module that bridges the gap between traditional relational database querying and Spark's distributed data processing. It allows developers and data analysts to leverage familiar SQL syntax or DataFrame API operations to interact with structured and semi-structured data at scale. This lesson explores how to execute SQL queries directly within PySpark, utilize temporary views for SQL accessibility on DataFrames, and extend Spark's functionality with User-Defined Functions (UDFs).\n",
    "\n",
    "### Spark SQL Fundamentals\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Spark SQL enables running SQL queries programmatically on various data sources, including existing DataFrames, Hive tables, JSON, Parquet, etc. The core entry point for Spark SQL functionality, as with most Spark operations since Spark 2.0, is the `SparkSession`. Once you have a DataFrame, Spark SQL provides mechanisms to query it using standard SQL syntax. This is particularly beneficial for:\n",
    "\n",
    "1.  **Leveraging Existing SQL Skills:** Data professionals comfortable with SQL can immediately become productive with Spark.\n",
    "2.  **Integrating with BI Tools:** Many business intelligence tools can connect to Spark via JDBC/ODBC and execute SQL queries.\n",
    "3.  **Hybrid Approaches:** Combining the declarative nature of SQL with the programmatic power of the DataFrame API within a single application.\n",
    "\n",
    "Under the hood, Spark SQL uses the **Catalyst Optimizer**. When a SQL query is submitted or a DataFrame operation is defined:\n",
    "1.  It's parsed into an **Unresolved Logical Plan**.\n",
    "2.  The Catalyst Analyzer resolves attributes and relations using the catalog (metadata).\n",
    "3.  The **Logical Optimizer** applies rule-based optimizations (e.g., predicate pushdown, constant folding).\n",
    "4.  Multiple **Physical Plans** are generated.\n",
    "5.  A **Cost Model** selects the most optimal Physical Plan for execution on the cluster.\n",
    "\n",
    "**Code Example: Basic Setup**\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLExample\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .enableHiveSupport() # Optional: If Hive interaction is needed\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 1, \"HR\"),\n",
    "        (\"Bob\", 2, \"Engineering\"),\n",
    "        (\"Charlie\", 3, \"Engineering\"),\n",
    "        (\"David\", 4, \"HR\"),\n",
    "        (\"Eve\", 5, \"Finance\")]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "employees_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "# Stop the SparkSession at the end\n",
    "# spark.stop() # Usually at the very end of the script/notebook\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `from pyspark.sql import SparkSession`: Imports the main entry point for DataFrame and SQL functionality.\n",
    "2.  `from pyspark.sql.types import ...`: Imports data types needed for defining the DataFrame schema.\n",
    "3.  `spark = SparkSession.builder...getOrCreate()`: Creates or retrieves an existing `SparkSession`.\n",
    "    *   `.appName()`: Sets a name for the Spark application (useful for monitoring).\n",
    "    *   `.config(\"spark.sql.warehouse.dir\", ...)`: Specifies a directory for Spark SQL to persist metadata for Hive tables (even if not using persistent Hive tables, it's often good practice).\n",
    "    *   `.enableHiveSupport()`: (Optional) Allows Spark to connect to a Hive metastore if available.\n",
    "    *   `.getOrCreate()`: Returns the SparkSession instance.\n",
    "4.  `data = [...]`: Defines sample data as a list of tuples.\n",
    "5.  `schema = StructType([...])`: Defines the structure and data types for the DataFrame columns. This provides clarity and avoids schema inference overhead for simple examples.\n",
    "6.  `employees_df = spark.createDataFrame(data, schema)`: Creates a Spark DataFrame from the sample data and schema.\n",
    "7.  `employees_df.show()`: Displays the contents of the DataFrame.\n",
    "\n",
    "### Registering Temporary Views\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "A **Temporary View** in Spark SQL is essentially an alias or a pointer to a DataFrame within a specific `SparkSession`. It allows you to refer to the DataFrame using a SQL table name in `spark.sql()` queries. Key characteristics include:\n",
    "\n",
    "1.  **Session-Scoped:** Temporary views are tied to the `SparkSession` that created them. They are automatically dropped when the session terminates. They are *not* accessible from other sessions.\n",
    "2.  **Namespace:** They reside in a session-specific namespace and do not conflict with permanent tables in the underlying metastore (like Hive).\n",
    "3.  **Lazy Evaluation:** Creating a view doesn't trigger any computation; it simply registers the logical plan of the underlying DataFrame under a name. The actual computation happens when a query is executed against the view.\n",
    "4.  **No Data Duplication:** A view does not copy the data. It's purely a metadata construct pointing to the DataFrame's plan.\n",
    "\n",
    "**Why use Temporary Views?**\n",
    "\n",
    "*   To easily query DataFrames using standard SQL syntax.\n",
    "*   To simplify complex query logic by breaking it down using intermediate views.\n",
    "*   To allow users familiar with SQL to interact with data prepared via the DataFrame API.\n",
    "\n",
    "**Creating and Querying Temporary Views:**\n",
    "\n",
    "The primary method to create a temporary view is `DataFrame.createOrReplaceTempView(\"view_name\")`.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "# Assuming employees_df is created as shown previously\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "employees_df.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "print(\"Temporary view 'employees_view' created.\")\n",
    "\n",
    "# Now query the view using spark.sql()\n",
    "engineering_employees = spark.sql(\"SELECT name, id FROM employees_view WHERE department = 'Engineering'\")\n",
    "\n",
    "print(\"Querying the temporary view:\")\n",
    "engineering_employees.show()\n",
    "\n",
    "# You can also perform more complex SQL queries\n",
    "hr_employee_count = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as employee_count\n",
    "    FROM employees_view\n",
    "    WHERE department = 'HR'\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "print(\"Aggregation query on the temporary view:\")\n",
    "hr_employee_count.show()\n",
    "\n",
    "# Demonstrate that the view is session-scoped (conceptual - cannot run in separate script easily)\n",
    "# If you start a *new* SparkSession, 'employees_view' will not exist.\n",
    "\n",
    "# Clean up the view (optional, as it's dropped with session)\n",
    "# spark.catalog.dropTempView(\"employees_view\")\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `employees_df.createOrReplaceTempView(\"employees_view\")`: This line takes the existing DataFrame `employees_df` and registers it as a temporary SQL view named `employees_view`. If a view with this name already exists in the current session, it will be replaced.\n",
    "2.  `spark.sql(\"SELECT ... FROM employees_view ...\")`: This demonstrates how to use the `spark.sql()` method, which takes a standard SQL query string as input. The query references the temporary view `employees_view` just like a regular SQL table.\n",
    "3.  `engineering_employees = spark.sql(...)`: The result of `spark.sql()` is another Spark DataFrame.\n",
    "4.  `engineering_employees.show()`: Displays the result of the SQL query.\n",
    "5.  The second `spark.sql()` example shows a more complex query with aggregation (`COUNT`, `GROUP BY`) executed against the same temporary view.\n",
    "6.  `spark.catalog.dropTempView(\"...\")`: (Commented out) Shows how to explicitly drop a temporary view if needed before the session ends. `spark.catalog` provides methods to manage metadata like tables, views, databases, and functions.\n",
    "\n",
    "**Global Temporary Views:**\n",
    "\n",
    "Spark also supports **Global Temporary Views** (`createOrReplaceGlobalTempView`). These are tied to the Spark *application* rather than a specific session.\n",
    "\n",
    "*   **Scope:** Visible across all sessions within the same Spark application.\n",
    "*   **Namespace:** Registered in a special database `global_temp`. Must be referenced using the qualified name `global_temp.view_name`.\n",
    "*   **Lifecycle:** Dropped when the Spark application terminates.\n",
    "\n",
    "```python\n",
    "# Create a global temporary view\n",
    "employees_df.createOrReplaceGlobalTempView(\"global_employees_view\")\n",
    "\n",
    "print(\"Global temporary view 'global_employees_view' created.\")\n",
    "\n",
    "# Query the global temporary view using the qualified name\n",
    "global_eng_employees = spark.sql(\"SELECT name FROM global_temp.global_employees_view WHERE department = 'Engineering'\")\n",
    "\n",
    "print(\"Querying the global temporary view:\")\n",
    "global_eng_employees.show()\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.catalog.dropGlobalTempView(\"global_employees_view\")\n",
    "```\n",
    "\n",
    "**Use Case:** Global views are useful when you need to share a temporary data reference across different sessions spawned within the *same* Spark application (e.g., in some multi-user or complex workflow scenarios), but this is less common than session-scoped temporary views.\n",
    "\n",
    "### User-Defined Functions (UDFs)\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "While Spark SQL and the DataFrame API provide a rich set of built-in functions (`pyspark.sql.functions`), sometimes you need custom logic that isn't readily available. User-Defined Functions (UDFs) allow you to define your own functions in Python (or Scala/Java) and apply them to Spark DataFrame columns.\n",
    "\n",
    "**How UDFs Work:**\n",
    "\n",
    "1.  **Define:** You write a standard Python function that takes one or more arguments (representing column values for a single row) and returns a single value.\n",
    "2.  **Register:** You register this Python function with Spark using `pyspark.sql.functions.udf()`, crucially specifying the **return type** of the function using Spark's SQL data types (`pyspark.sql.types`).\n",
    "3.  **Apply:** You can then use the registered UDF object within DataFrame operations (like `withColumn` or `select`) or register it for use in SQL queries using `spark.udf.register()`.\n",
    "\n",
    "**Important Performance Considerations:**\n",
    "\n",
    "*   **Serialization/Deserialization:** When a PySpark UDF executes, data for each row being processed must be serialized from the JVM (where Spark stores DataFrames) to the Python process, processed by your Python function, and then the result must be serialized back to the JVM. This back-and-forth introduces significant overhead.\n",
    "*   **Optimizer Limitations:** The Catalyst Optimizer cannot \"see inside\" a Python UDF. It treats the UDF as a black box. This means it cannot perform optimizations like predicate pushdown or code generation *within* the UDF logic.\n",
    "*   **Python vs. JVM:** Python code execution is generally slower than optimized JVM code executed by Spark's built-in functions.\n",
    "\n",
    "**Best Practice:** **Always prefer built-in Spark SQL functions whenever possible.** They run entirely within the JVM and are fully optimized by Catalyst. Use UDFs only when the required logic cannot be expressed using built-in functions. Consider alternatives like Pandas UDFs (Vectorized UDFs) for better performance if significant custom Python logic is needed, as they operate on Pandas Series/DataFrames batches, reducing serialization overhead.\n",
    "\n",
    "**Creating and Using UDFs:**\n",
    "\n",
    "**Method 1: Using with DataFrame API (`pyspark.sql.functions.udf`)**\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Sample DataFrame (re-using employees_df)\n",
    "employees_df.show()\n",
    "\n",
    "# 1. Define the Python function\n",
    "def get_name_length(name):\n",
    "  \"\"\"Calculates the length of a string.\"\"\"\n",
    "  if name is None:\n",
    "    return 0\n",
    "  return len(name)\n",
    "\n",
    "# 2. Register the Python function as a UDF, specifying the return type\n",
    "#    The function itself takes Python types, but Spark needs to know the SQL type it returns.\n",
    "get_name_length_udf = udf(get_name_length, IntegerType())\n",
    "\n",
    "# 3. Apply the UDF to the DataFrame\n",
    "employees_with_name_length = employees_df.withColumn(\"name_length\", get_name_length_udf(employees_df[\"name\"]))\n",
    "\n",
    "print(\"DataFrame with name length calculated using UDF:\")\n",
    "employees_with_name_length.show()\n",
    "\n",
    "# Example with multiple arguments and different return type\n",
    "def create_employee_label(name, id):\n",
    "    \"\"\"Creates a label string combining name and ID.\"\"\"\n",
    "    if name is None or id is None:\n",
    "        return None\n",
    "    return f\"{name} (ID: {id})\"\n",
    "\n",
    "create_label_udf = udf(create_employee_label, StringType())\n",
    "\n",
    "employees_with_label = employees_df.withColumn(\"label\", create_label_udf(employees_df[\"name\"], employees_df[\"id\"]))\n",
    "\n",
    "print(\"DataFrame with label created using UDF:\")\n",
    "employees_with_label.show()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `from pyspark.sql.functions import udf`: Imports the `udf` function factory.\n",
    "2.  `from pyspark.sql.types import StringType, IntegerType`: Imports Spark SQL data types needed for specifying UDF return types.\n",
    "3.  `def get_name_length(name): ...`: A standard Python function taking one argument (`name`). Includes handling for potential `None` values.\n",
    "4.  `get_name_length_udf = udf(get_name_length, IntegerType())`:\n",
    "    *   `udf()`: Registers the Python function `get_name_length`.\n",
    "    *   `IntegerType()`: Crucially tells Spark that this UDF will return an integer value for each row. **This must match the actual return type of the Python function.** Mismatches can lead to runtime errors or incorrect results.\n",
    "5.  `employees_df.withColumn(\"name_length\", ...)`: Adds a new column named \"name\\_length\".\n",
    "6.  `get_name_length_udf(employees_df[\"name\"])`: Applies the registered UDF. `employees_df[\"name\"]` selects the 'name' column, and the UDF is applied row by row to the values in this column.\n",
    "7.  The second example (`create_employee_label`, `create_label_udf`) demonstrates a UDF taking multiple column arguments (`name`, `id`) and returning a `StringType`.\n",
    "\n",
    "**Method 2: Registering for SQL Use (`spark.udf.register`)**\n",
    "\n",
    "If you want to use your custom function directly within `spark.sql` queries, you need to register it with Spark's SQL context.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "# Define the Python function (can reuse get_name_length from above)\n",
    "# def get_name_length(name): ...\n",
    "\n",
    "# Register the UDF for use in SQL queries\n",
    "# The arguments are: SQL function name, Python function, return type\n",
    "spark.udf.register(\"sql_get_name_length\", get_name_length, IntegerType())\n",
    "\n",
    "print(\"UDF 'sql_get_name_length' registered for SQL use.\")\n",
    "\n",
    "# Use the registered UDF within a spark.sql query\n",
    "# Make sure the temporary view 'employees_view' still exists\n",
    "employees_df.createOrReplaceTempView(\"employees_view\") # Recreate if needed\n",
    "\n",
    "result_sql_udf = spark.sql(\"\"\"\n",
    "    SELECT name, department, sql_get_name_length(name) AS name_len\n",
    "    FROM employees_view\n",
    "\"\"\")\n",
    "\n",
    "print(\"Using UDF within spark.sql:\")\n",
    "result_sql_udf.show()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  `spark.udf.register(\"sql_get_name_length\", get_name_length, IntegerType())`: Registers the Python function `get_name_length` under the name `sql_get_name_length` for use in SQL queries within the current SparkSession. It also requires the return type (`IntegerType()`).\n",
    "2.  `spark.sql(\"... sql_get_name_length(name) ...\")`: The SQL query now directly calls the registered UDF `sql_get_name_length` as if it were a built-in SQL function, passing the `name` column as an argument.\n",
    "3.  `AS name_len`: Assigns an alias to the result column generated by the UDF.\n",
    "\n",
    "**Practical UDF Use Cases:**\n",
    "\n",
    "*   Implementing complex, proprietary business logic.\n",
    "*   Parsing intricate string formats not handled by built-in functions.\n",
    "*   Integrating external libraries (e.g., specialized geospatial calculations, complex statistical models) on a row-by-row basis.\n",
    "*   Applying machine learning model inference (though dedicated libraries like MLlib or Pandas UDFs are often better).\n",
    "\n",
    "### Advanced Considerations and Best Practices\n",
    "\n",
    "1.  **Performance Tuning:**\n",
    "    *   **Minimize UDF Usage:** Always benchmark UDFs against equivalent built-in functions or DataFrame API constructs. The performance difference can be orders of magnitude.\n",
    "    *   **Select Correct Return Types:** Specifying the most precise return type (`IntegerType` vs. `LongType`, `FloatType` vs. `DoubleType`) can sometimes help Spark optimize storage, although the main overhead is serialization/Python execution.\n",
    "    *   **Explore Pandas UDFs (Vectorized UDFs):** For computationally intensive tasks or operations that benefit from vectorized execution (like NumPy/Pandas), Pandas UDFs (`pyspark.sql.functions.pandas_udf`) offer significantly better performance by processing data in batches using Apache Arrow.\n",
    "    *   **Analyze Query Plans:** Use `DataFrame.explain()` or `spark.sql(\"EXPLAIN ...\").show(truncate=False)` to understand how Spark translates your SQL query or DataFrame operations (including UDFs) into a physical execution plan. Look for bottlenecks or areas where optimizations might be hindered (e.g., UDFs often appear as `PythonUDF` nodes).\n",
    "\n",
    "2.  **Partitioning:**\n",
    "    *   The performance of SQL queries on views and operations involving UDFs is heavily influenced by the partitioning of the underlying DataFrame.\n",
    "    *   Ensure the base DataFrame is appropriately partitioned (e.g., using `repartition()` or `partitionBy()` on write) based on common filter or join keys used in your SQL queries or relevant to your UDF logic. UDFs operate independently within each partition task. Poor partitioning can lead to data skew and inefficient UDF execution.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Spark SQL provides a powerful and familiar interface for querying structured data within the Spark ecosystem. Temporary Views act as convenient aliases for DataFrames, enabling seamless SQL querying on data manipulated programmatically. While UDFs offer extensibility for custom logic, they come with significant performance caveats due to serialization and optimizer limitations. Professionals should prioritize built-in functions and explore alternatives like Pandas UDFs before resorting to standard Python UDFs, always keeping performance implications in mind. Understanding how these components interact with the Catalyst optimizer and data partitioning is key to building efficient and scalable PySpark applications.\n",
    "\n",
    "---\n",
    "**End of Lesson 7 Notes**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
