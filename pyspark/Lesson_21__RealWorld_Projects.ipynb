{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b69ade",
   "metadata": {},
   "source": [
    "# Lesson 21 - Real-World Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bceebd2",
   "metadata": {},
   "source": [
    "Okay, here are detailed technical notes on PySpark, structured for professional learners and suitable for training material or reference documentation. The notes focus on PySpark functionality, architecture, coding practices, and advanced concepts, using real-world project contexts as examples.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes for Professional Learners\n",
    "\n",
    "### Introduction to Apache Spark and PySpark\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python (PySpark), and R, along with an optimized engine that supports general execution graphs. Key characteristics include:\n",
    "\n",
    "1.  **Speed:** Spark can be significantly faster than Hadoop MapReduce due to in-memory computation capabilities and optimized execution graphs (DAGs).\n",
    "2.  **Ease of Use:** Rich APIs (like DataFrames and SQL) simplify complex data processing tasks.\n",
    "3.  **Generality:** Spark combines SQL, streaming analytics, machine learning (MLlib), and graph processing (GraphX) within a single framework.\n",
    "4.  **Runs Everywhere:** Spark can run on Hadoop YARN, Apache Mesos, Kubernetes, standalone, or in the cloud.\n",
    "\n",
    "**PySpark** is the Python API for Apache Spark. It allows data scientists and engineers to leverage the power of Spark using the familiar Python language and its rich ecosystem of libraries (like Pandas, NumPy, scikit-learn), while Spark handles the distributed computation across a cluster.\n",
    "\n",
    "**Core Abstraction: Resilient Distributed Datasets (RDDs)**\n",
    "While historically foundational, modern PySpark development primarily uses higher-level abstractions like DataFrames. RDDs represent an immutable, partitioned collection of elements that can be operated on in parallel. They offer fault tolerance through lineage information (how an RDD was derived from others). You might encounter RDDs when needing low-level control or dealing with unstructured data, but DataFrames are generally preferred for structured/semi-structured data due to performance optimizations.\n",
    "\n",
    "**Primary Abstraction: DataFrames**\n",
    "Built on top of RDDs, DataFrames organize data into named columns, similar to tables in relational databases or pandas DataFrames. They benefit from Spark's Catalyst optimizer and Tungsten execution engine, providing significant performance improvements over RDDs for structured data processing. DataFrames can be manipulated using domain-specific language (DSL) methods (e.g., `select`, `filter`, `groupBy`) or standard SQL queries.\n",
    "\n",
    "### Spark Architecture Fundamentals\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Understanding Spark's architecture is crucial for writing efficient applications and debugging performance issues.\n",
    "\n",
    "1.  **Driver Program:** The process running the `main()` function of your application (e.g., your PySpark script). It creates the `SparkContext` (or `SparkSession`), which coordinates the execution.\n",
    "    *   Responsibilities: Hosts the application logic, analyzes, distributes, and schedules work across the Executors, maintains metadata about the application.\n",
    "2.  **Cluster Manager:** An external service responsible for acquiring resources on the cluster (e.g., YARN, Mesos, Kubernetes, Spark Standalone). The Driver requests resources from the Cluster Manager.\n",
    "3.  **Executor:** A process launched for an application on a worker node.\n",
    "    *   Responsibilities: Executes tasks assigned by the Driver, holds data partitions in memory or on disk (cache), reports results and status back to the Driver. Each application has its own Executors.\n",
    "4.  **SparkSession:** The unified entry point for Spark functionality since Spark 2.0. It encapsulates `SparkContext`, `SQLContext`, `HiveContext`, and `StreamingContext`.\n",
    "5.  **Job:** A parallel computation consisting of multiple tasks, triggered by a Spark *action* (e.g., `count()`, `collect()`, `save()`).\n",
    "6.  **Stage:** Each job is divided into smaller sets of tasks called *stages*, separated by *shuffle* operations (wide transformations). Tasks within a stage can run in parallel without data shuffling.\n",
    "7.  **Task:** A unit of work sent by the Driver to an Executor to be executed on a specific data partition.\n",
    "\n",
    "**Lazy Evaluation:** Spark transformations (e.g., `select`, `filter`, `map`) are *lazy*. They don't execute immediately. Instead, Spark builds a Directed Acyclic Graph (DAG) of transformations. The computation starts only when an *action* (e.g., `show`, `count`, `collect`, `save`) is called. This allows Spark to optimize the entire execution plan.\n",
    "\n",
    "**Code Example: Initializing SparkSession**\n",
    "\n",
    "```python\n",
    "# Import the SparkSession module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession instance\n",
    "# .appName(): Sets a name for the application, visible in the Spark UI.\n",
    "# .master(): Specifies the cluster manager URL ('local[*]' runs locally using all available cores).\n",
    "#           For cluster deployment, this would be 'yarn', 'k8s://<api_server>', etc.\n",
    "# .config(): Allows setting various Spark configuration properties.\n",
    "# .getOrCreate(): Gets an existing SparkSession or creates a new one if none exists.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkFundamentals\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext (though often not needed directly with DataFrames)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\") # Reduce log verbosity\n",
    "\n",
    "print(f\"SparkSession initialized. Spark version: {spark.version}\")\n",
    "\n",
    "# Example: Stop the SparkSession when done (important in scripts)\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   `from pyspark.sql import SparkSession`: Imports the necessary class.\n",
    "*   `SparkSession.builder`: Accesses the builder pattern to configure the session.\n",
    "*   `.appName(\"...\")`: Assigns a name for tracking the application.\n",
    "*   `.master(\"local[*]\")`: Tells Spark to run locally using as many worker threads as logical cores on the machine. For production, this would point to your cluster manager (e.g., `yarn`).\n",
    "*   `.config(\"spark.driver.memory\", \"2g\")`: Sets the memory allocated to the Driver process.\n",
    "*   `.config(\"spark.executor.memory\", \"4g\")`: Sets the memory allocated to each Executor process. These are crucial tuning parameters.\n",
    "*   `.getOrCreate()`: Constructs the `SparkSession` object.\n",
    "*   `spark.sparkContext`: Accesses the underlying `SparkContext` if needed for RDD operations or lower-level configuration.\n",
    "*   `sc.setLogLevel(\"WARN\")`: Filters log messages to show only warnings and errors, making output cleaner.\n",
    "*   `spark.stop()`: Releases the resources used by the Spark application. Essential to call at the end of a script.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Example 1: Retail Sales Dashboard Data Aggregation\n",
    "\n",
    "**Goal:** Process raw retail sales data (e.g., CSV files) to compute daily/monthly sales summaries per store or product category, suitable for feeding a dashboard.\n",
    "\n",
    "**PySpark Concepts Illustrated:** Data Ingestion (CSV), Schema Inference/Definition, DataFrame Transformations (`select`, `withColumn`, `filter`, `groupBy`, `agg`), Data Output (Parquet).\n",
    "\n",
    "**Theory: DataFrames and Transformations**\n",
    "\n",
    "DataFrames provide a structured view of data. Common operations include:\n",
    "\n",
    "*   **Reading Data:** `spark.read.format(...).load(...)` or shortcuts like `spark.read.csv(...)`. Schema can be inferred or explicitly defined for robustness and performance.\n",
    "*   **Selecting Columns:** `df.select(\"col1\", \"col2\")`\n",
    "*   **Filtering Rows:** `df.filter(df[\"col_name\"] > value)` or `df.where(\"col_name > value\")` (using SQL syntax).\n",
    "*   **Adding/Modifying Columns:** `df.withColumn(\"new_col\", expression)`\n",
    "*   **Grouping and Aggregating:** `df.groupBy(\"key_col\").agg(agg_function(\"value_col\"))`\n",
    "*   **Transformations:**\n",
    "    *   **Narrow:** Input partitions map one-to-one to output partitions (e.g., `select`, `filter`, `withColumn`). No data shuffling across the network is required. Efficient.\n",
    "    *   **Wide:** Input partitions contribute to multiple output partitions (e.g., `groupBy`, `join` on non-partitioned keys). Requires shuffling data between executors, which is expensive.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, date_format, month, year\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize SparkSession (assuming already done from previous section)\n",
    "# spark = SparkSession.builder.appName(\"RetailSales\").getOrCreate()\n",
    "\n",
    "# 1. Data Ingestion with Explicit Schema\n",
    "# Define the schema for better performance and data integrity\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"Timestamp\", TimestampType(), True),\n",
    "    StructField(\"StoreID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Load data from CSV\n",
    "# header=True: Uses the first line as header.\n",
    "# schema=schema: Applies the defined schema.\n",
    "# timestampFormat: Specifies the format for parsing timestamp strings.\n",
    "sales_df = spark.read.csv(\n",
    "    \"path/to/your/sales_data.csv\",\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    timestampFormat=\"yyyy-MM-dd HH:mm:ss\"\n",
    ")\n",
    "\n",
    "# 2. Initial Exploration (Optional but Recommended)\n",
    "print(\"Sales Data Schema:\")\n",
    "sales_df.printSchema()\n",
    "print(\"Sample Sales Data:\")\n",
    "sales_df.show(5, truncate=False)\n",
    "\n",
    "# 3. Data Cleaning / Preparation (Example: Handle potential nulls if necessary)\n",
    "# For this example, assume data is clean or schema handles nullability\n",
    "\n",
    "# 4. Feature Engineering & Transformation\n",
    "# Calculate total sale amount per transaction\n",
    "sales_df = sales_df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# Extract date components for aggregation\n",
    "sales_df = sales_df.withColumn(\"SaleDate\", date_format(col(\"Timestamp\"), \"yyyy-MM-dd\"))\n",
    "sales_df = sales_df.withColumn(\"SaleMonth\", date_format(col(\"Timestamp\"), \"yyyy-MM\"))\n",
    "# Alternative using month() and year() functions\n",
    "# sales_df = sales_df.withColumn(\"SaleYear\", year(col(\"Timestamp\")))\n",
    "# sales_df = sales_df.withColumn(\"SaleMonthNum\", month(col(\"Timestamp\")))\n",
    "\n",
    "# 5. Aggregation - Daily Sales per Store\n",
    "daily_sales_summary = sales_df.groupBy(\"StoreID\", \"SaleDate\") \\\n",
    "    .agg(\n",
    "        sum(\"TotalAmount\").alias(\"TotalDailySales\"),\n",
    "        count(\"TransactionID\").alias(\"TotalDailyTransactions\")\n",
    "    ) \\\n",
    "    .orderBy(\"StoreID\", \"SaleDate\")\n",
    "\n",
    "print(\"Daily Sales Summary per Store:\")\n",
    "daily_sales_summary.show(10)\n",
    "\n",
    "# 6. Aggregation - Monthly Sales per Store\n",
    "monthly_sales_summary = sales_df.groupBy(\"StoreID\", \"SaleMonth\") \\\n",
    "    .agg(\n",
    "        sum(\"TotalAmount\").alias(\"TotalMonthlySales\"),\n",
    "        count(\"TransactionID\").alias(\"TotalMonthlyTransactions\")\n",
    "    ) \\\n",
    "    .orderBy(\"StoreID\", \"SaleMonth\")\n",
    "\n",
    "print(\"Monthly Sales Summary per Store:\")\n",
    "monthly_sales_summary.show(10)\n",
    "\n",
    "\n",
    "# 7. Data Output\n",
    "# Save the aggregated results, often in an optimized format like Parquet\n",
    "# mode(\"overwrite\"): If the destination exists, replace its contents.\n",
    "# partitionBy(): Optional - physically partitions data on disk for faster reads on filtered queries.\n",
    "daily_sales_summary.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"StoreID\") \\\n",
    "    .parquet(\"path/to/output/daily_sales_summary.parquet\")\n",
    "\n",
    "monthly_sales_summary.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"path/to/output/monthly_sales_summary.parquet\")\n",
    "\n",
    "# Remember to stop the SparkSession\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **`schema = StructType([...])`**: Defines the expected structure and data types of the input CSV. Using an explicit schema avoids a potentially slow and error-prone schema inference step, and ensures data type correctness.\n",
    "*   **`spark.read.csv(...)`**: Reads the CSV file into a DataFrame using the specified options and schema.\n",
    "*   **`printSchema()`**: Displays the DataFrame's schema (column names and types).\n",
    "*   **`show(5, truncate=False)`**: Displays the first 5 rows without truncating column content. Useful for quick inspection.\n",
    "*   **`withColumn(\"TotalAmount\", ...)`**: Adds a new column `TotalAmount` calculated by multiplying `Quantity` and `UnitPrice`. `col()` is used to refer to existing columns.\n",
    "*   **`withColumn(\"SaleDate\", ...)`**: Extracts the date part from the `Timestamp` column using `date_format`.\n",
    "*   **`groupBy(\"StoreID\", \"SaleDate\")`**: Groups the DataFrame rows based on unique combinations of `StoreID` and `SaleDate`. This is a wide transformation, potentially involving data shuffling.\n",
    "*   **`.agg(...)`**: Performs aggregations on each group.\n",
    "    *   `sum(\"TotalAmount\").alias(\"...\")`: Calculates the sum of `TotalAmount` for each group and names the resulting column `TotalDailySales`.\n",
    "    *   `count(\"TransactionID\").alias(\"...\")`: Counts the number of transactions in each group.\n",
    "*   **`.orderBy(...)`**: Sorts the results for better readability (can be expensive on large data).\n",
    "*   **`write.mode(\"overwrite\")...`**: Specifies how to save the DataFrame. `overwrite` replaces existing data. Other modes include `append`, `ignore`, `errorifexists`.\n",
    "*   **`.partitionBy(\"StoreID\")`**: Instructs Spark to partition the output data into separate directories based on the values in the `StoreID` column. This significantly speeds up queries that filter by `StoreID` (e.g., `spark.read.parquet(...).filter(col(\"StoreID\") == \"S101\")`).\n",
    "*   **`.parquet(...)`**: Saves the DataFrame in the Parquet format. Parquet is a columnar storage format optimized for analytical queries, offering good compression and performance.\n",
    "\n",
    "**Use Cases:** Business intelligence reporting, feeding dashboards, data warehousing ETL.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Example 2: Fraud Detection Pipeline (Feature Engineering)\n",
    "\n",
    "**Goal:** Process transaction data and user activity logs to engineer features that can be used by a machine learning model to detect potentially fraudulent activities.\n",
    "\n",
    "**PySpark Concepts Illustrated:** Joins, Window Functions, UDFs (and alternatives), Caching/Persistence.\n",
    "\n",
    "**Theory: Joins, Window Functions, and Performance**\n",
    "\n",
    "*   **Joins:** Combine rows from two DataFrames based on a related column. Spark supports various join types (`inner`, `left_outer`, `right_outer`, `full_outer`, `left_semi`, `left_anti`). Performance depends heavily on data size, partitioning, and whether a *Broadcast Join* can be used (if one DataFrame is small enough to fit in each Executor's memory).\n",
    "*   **Window Functions:** Perform calculations across a set of table rows that are somehow related to the current row (similar to SQL window functions). Unlike `groupBy`, they don't collapse rows; they return a value for *each* row based on a \"window\" of related rows defined by `PARTITION BY` and `ORDER BY` clauses. Examples: calculating running totals, ranking, accessing previous/next row's value.\n",
    "*   **User-Defined Functions (UDFs):** Allow executing custom Python code on DataFrame columns. While flexible, UDFs can be performance bottlenecks because:\n",
    "    *   Spark cannot optimize the Python code within the UDF.\n",
    "    *   Data must be serialized/deserialized between the JVM (Spark) and the Python interpreter.\n",
    "    *   Whenever possible, use built-in Spark SQL functions, which operate directly on Spark's internal optimized format (Tungsten). Use UDFs only when the required logic cannot be expressed using built-in functions. Pandas UDFs (Vectorized UDFs) offer better performance by working on Pandas Series/DataFrames via Apache Arrow.\n",
    "*   **Caching (`.cache()`, `.persist()`):** Stores the DataFrame's partitions in memory (or disk, depending on the storage level) on the Executors. This is beneficial when a DataFrame is used multiple times in the execution plan (e.g., in iterative algorithms or when branching logic uses the same base data). `cache()` is a shorthand for `persist(StorageLevel.MEMORY_ONLY)`. Use judiciously, as memory is a finite resource.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, avg, lag, lead, rank, datediff, expr, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "# from pyspark.sql.pandas.functions import pandas_udf # Alternative for better performance\n",
    "\n",
    "# Assume 'spark' session is available and 'sales_df' from Project 1 exists\n",
    "# Let's assume another DataFrame 'user_activity_df' exists\n",
    "# Schema: UserID, ActivityTimestamp, ActivityType, DeviceID\n",
    "\n",
    "# Sample DataFrames (Replace with actual loading)\n",
    "# sales_df = spark.read.parquet(\"path/to/sales_data.parquet\") # Assuming it includes UserID\n",
    "# user_activity_df = spark.read.parquet(\"path/to/user_activity.parquet\")\n",
    "\n",
    "# For demonstration, let's create dummy DataFrames\n",
    "data_sales = [(\"T1\", \"U1\", \"2023-10-26 10:00:00\", 100.0),\n",
    "              (\"T2\", \"U2\", \"2023-10-26 10:05:00\", 50.0),\n",
    "              (\"T3\", \"U1\", \"2023-10-26 11:15:00\", 200.0),\n",
    "              (\"T4\", \"U1\", \"2023-10-27 09:30:00\", 10.0),\n",
    "              (\"T5\", \"U2\", \"2023-10-27 14:00:00\", 75.0),\n",
    "              (\"T6\", \"U1\", \"2023-10-27 14:05:00\", 300.0)] # High value txn shortly after low value\n",
    "columns_sales = [\"TransactionID\", \"UserID\", \"Timestamp\", \"Amount\"]\n",
    "sales_df = spark.createDataFrame(data_sales, columns_sales) \\\n",
    "    .withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "data_activity = [(\"U1\", \"2023-10-26 09:58:00\", \"login\", \"DevA\"),\n",
    "                 (\"U2\", \"2023-10-26 10:03:00\", \"login\", \"DevB\"),\n",
    "                 (\"U1\", \"2023-10-26 11:13:00\", \"update_profile\", \"DevA\"),\n",
    "                 (\"U1\", \"2023-10-27 09:00:00\", \"login\", \"DevC\"), # Login from new device\n",
    "                 (\"U2\", \"2023-10-27 13:55:00\", \"login\", \"DevB\")]\n",
    "columns_activity = [\"UserID\", \"ActivityTimestamp\", \"ActivityType\", \"DeviceID\"]\n",
    "user_activity_df = spark.createDataFrame(data_activity, columns_activity) \\\n",
    "    .withColumn(\"ActivityTimestamp\", col(\"ActivityTimestamp\").cast(\"timestamp\"))\n",
    "\n",
    "\n",
    "# 1. Feature Engineering using Window Functions\n",
    "# Calculate average transaction amount for the user *up to the current transaction*\n",
    "# Define the window specification: Partition by UserID, order by Timestamp\n",
    "user_window = Window.partitionBy(\"UserID\").orderBy(\"Timestamp\") \\\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow) # Or Window.currentRow for current row only avg\n",
    "\n",
    "sales_with_avg = sales_df.withColumn(\n",
    "    \"UserAvgTxnAmount\",\n",
    "    avg(\"Amount\").over(user_window)\n",
    ")\n",
    "\n",
    "# Calculate time difference between consecutive transactions for the same user\n",
    "user_window_prev = Window.partitionBy(\"UserID\").orderBy(\"Timestamp\")\n",
    "sales_with_time_diff = sales_with_avg.withColumn(\n",
    "    \"PrevTxnTimestamp\",\n",
    "    lag(\"Timestamp\", 1).over(user_window_prev) # Get timestamp of previous row in window\n",
    ")\n",
    "\n",
    "# Calculate time diff in seconds\n",
    "sales_with_time_diff = sales_with_time_diff.withColumn(\n",
    "    \"TimeSinceLastTxn_sec\",\n",
    "    col(\"Timestamp\").cast(\"long\") - col(\"PrevTxnTimestamp\").cast(\"long\")\n",
    ")\n",
    "sales_with_time_diff = sales_with_time_diff.fillna(0, subset=[\"TimeSinceLastTxn_sec\"]) # Handle first transaction\n",
    "\n",
    "print(\"Sales with Window Features:\")\n",
    "sales_with_time_diff.show(truncate=False)\n",
    "\n",
    "# 2. Feature Engineering using Joins\n",
    "# Add user's last login information before the transaction\n",
    "# Use a non-equi join condition (requires Spark 3.0+) or a more complex approach for older versions\n",
    "# For simplicity here, let's find the *closest prior* login (can be complex, often approximated)\n",
    "\n",
    "# A common pattern: Find the latest activity *before* the transaction time for each user\n",
    "# This often involves joins and aggregations or window functions on the activity table first.\n",
    "\n",
    "# Simpler Example: Join sales with the *latest* user activity overall (less precise for fraud)\n",
    "latest_activity = user_activity_df.groupBy(\"UserID\") \\\n",
    "                                .agg(expr(\"max(ActivityTimestamp)\").alias(\"LastActivityTimestamp\"))\n",
    "\n",
    "sales_joined = sales_with_time_diff.join(\n",
    "    latest_activity,\n",
    "    on=\"UserID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Sales joined with latest activity timestamp:\")\n",
    "sales_joined.show(truncate=False)\n",
    "\n",
    "# More relevant: Join transaction with activity *immediately preceding* it.\n",
    "# This often requires careful handling of timestamps and potentially complex joins or window functions.\n",
    "\n",
    "# Example: Feature - Transaction Amount compared to User's Average\n",
    "# This was already computed (`UserAvgTxnAmount`)\n",
    "\n",
    "\n",
    "# 3. Feature Engineering using UDF (Use with Caution - Prefer built-in functions)\n",
    "# Example: Calculate a custom risk score based on amount and time (Illustrative only)\n",
    "def calculate_risk_score(amount, avg_amount, time_since_last):\n",
    "    if avg_amount is None or avg_amount == 0 or time_since_last is None:\n",
    "        return 0.0\n",
    "    amount_ratio = amount / avg_amount\n",
    "    time_factor = 1 / (1 + time_since_last / 3600.0) # Higher score if time is short (in hours)\n",
    "\n",
    "    # Simple heuristic (replace with actual model/logic)\n",
    "    score = amount_ratio * time_factor\n",
    "    return float(score) if score is not None else 0.0\n",
    "\n",
    "# Register the function as a UDF, specifying the return type\n",
    "risk_score_udf = udf(calculate_risk_score, DoubleType())\n",
    "\n",
    "# Apply the UDF\n",
    "# Note: Ensure columns passed to UDF are not null or handle nulls inside the UDF\n",
    "final_features = sales_with_time_diff.withColumn(\n",
    "    \"CustomRiskScore\",\n",
    "    risk_score_udf(col(\"Amount\"), col(\"UserAvgTxnAmount\"), col(\"TimeSinceLastTxn_sec\"))\n",
    ")\n",
    "\n",
    "print(\"Final Features with Custom UDF Score:\")\n",
    "final_features.select(\"TransactionID\", \"UserID\", \"Amount\", \"UserAvgTxnAmount\", \"TimeSinceLastTxn_sec\", \"CustomRiskScore\").show(truncate=False)\n",
    "\n",
    "\n",
    "# 4. Caching Intermediate Results\n",
    "# If 'final_features' DataFrame is used multiple times later (e.g., for model training AND analysis)\n",
    "final_features.persist() # Or .cache()\n",
    "\n",
    "# Perform actions that use the cached DataFrame\n",
    "count = final_features.count()\n",
    "print(f\"Total transactions processed: {count}\")\n",
    "high_risk = final_features.filter(col(\"CustomRiskScore\") > 1.5).count()\n",
    "print(f\"Transactions with risk score > 1.5: {high_risk}\")\n",
    "\n",
    "# Unpersist when done to free up memory\n",
    "final_features.unpersist()\n",
    "\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **`Window.partitionBy(...).orderBy(...)`**: Defines a window specification. Partitions data by `UserID` and orders rows within each partition by `Timestamp`. `rowsBetween` defines the frame boundaries (e.g., all preceding rows up to the current row).\n",
    "*   **`avg(\"Amount\").over(user_window)`**: Calculates the average `Amount` over the specified window frame for each row.\n",
    "*   **`lag(\"Timestamp\", 1).over(...)`**: Accesses the value of the `Timestamp` column from the previous row within the window partition. `lead()` accesses the next row.\n",
    "*   **`.cast(\"long\")`**: Converts timestamp to Unix epoch seconds (as long integer) for easy subtraction to find time differences.\n",
    "*   **`.fillna(0, ...)`**: Replaces null values (which occur for the first transaction where `lag` finds no previous row) with 0.\n",
    "*   **`join(..., on=\"UserID\", how=\"left\")`**: Performs a left outer join. Keeps all rows from the left DataFrame (`sales_with_time_diff`) and adds matching data from the right (`latest_activity`). If no match is found for a `UserID`, columns from the right DataFrame will be null.\n",
    "*   **`udf(calculate_risk_score, DoubleType())`**: Registers the Python function `calculate_risk_score` as a Spark UDF, specifying that it returns a `DoubleType`.\n",
    "*   **`risk_score_udf(...)`**: Applies the UDF to the specified columns. Spark serializes row data, sends it to a Python process, executes the function, and deserializes the result. *This is often slower than built-in functions.*\n",
    "*   **`.persist()` / `.cache()`**: Caches the DataFrame `final_features` in memory across the cluster's executors. Subsequent actions on `final_features` will read from the cache instead of recomputing the entire lineage, speeding up iterative workloads.\n",
    "*   **`.unpersist()`**: Removes the DataFrame from the cache, freeing up executor memory. It's crucial to unpersist when the cached data is no longer needed.\n",
    "\n",
    "**Performance Considerations:**\n",
    "\n",
    "*   **Joins:** Analyze join strategies using `df.explain()`. Ensure join keys are well-distributed. Consider broadcasting smaller tables (`spark.sql.autoBroadcastJoinThreshold` config or `broadcast()` hint). Ensure data types of join keys match.\n",
    "*   **Window Functions:** Can be expensive, especially with large partitions or complex window frames. Ensure `partitionBy` keys are effective.\n",
    "*   **UDFs:** Avoid if possible. Explore built-in functions first. If necessary, consider Pandas UDFs for vectorized execution, which can be significantly faster for complex numerical or string operations.\n",
    "*   **Caching:** Use `cache()` or `persist()` strategically for DataFrames that are accessed multiple times. Monitor memory usage via the Spark UI. Choose appropriate storage levels (`MEMORY_ONLY`, `MEMORY_AND_DISK`, etc.) based on data size and memory availability. Don't cache everything.\n",
    "\n",
    "**Use Cases:** Real-time fraud detection systems (often combined with streaming), risk modeling, anomaly detection, cybersecurity analytics.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Example 3: Customer Segmentation on E-Commerce Data\n",
    "\n",
    "**Goal:** Group customers into distinct segments based on their purchasing behavior (e.g., frequency, recency, monetary value - RFM analysis) or other attributes. This often involves feature engineering followed by clustering algorithms (like K-Means from MLlib). Here, we focus on the PySpark data preparation part.\n",
    "\n",
    "**PySpark Concepts Illustrated:** Advanced Aggregations, Date/Time Functions, Data Scaling (MLlib), VectorAssembler (MLlib).\n",
    "\n",
    "**Theory: Feature Engineering for Segmentation**\n",
    "\n",
    "Customer segmentation requires creating meaningful features that capture distinct customer behaviors. RFM analysis is a common technique:\n",
    "\n",
    "*   **Recency:** How recently did the customer make a purchase? (e.g., days since last purchase).\n",
    "*   **Frequency:** How often does the customer purchase? (e.g., total number of transactions).\n",
    "*   **Monetary Value:** How much does the customer spend? (e.g., total or average transaction value).\n",
    "\n",
    "These features often need to be scaled before being fed into distance-based clustering algorithms like K-Means. Spark's MLlib library provides tools for this.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, count, sum, datediff, current_date, lit\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assume 'spark' session and 'sales_df' (with UserID, Timestamp, TotalAmount) are available\n",
    "\n",
    "# 1. Calculate RFM Features\n",
    "# Find the most recent date in the dataset (or use current date)\n",
    "# Using current_date() assumes data is up-to-date. If historical, use max timestamp.\n",
    "max_date_df = sales_df.agg(spark_max(\"Timestamp\").alias(\"MaxTimestamp\"))\n",
    "# Extract the actual date value to use in calculations\n",
    "max_date = max_date_df.first()[\"MaxTimestamp\"]\n",
    "# Use lit() to treat max_date as a literal value in DataFrame operations\n",
    "# Alternatively, crossJoin max_date_df if needed in complex scenarios.\n",
    "\n",
    "# Calculate Recency, Frequency, Monetary Value per UserID\n",
    "rfm_df = sales_df.groupBy(\"UserID\").agg(\n",
    "    # Recency: Days since last purchase from the reference date\n",
    "    datediff(lit(max_date), spark_max(\"Timestamp\")).alias(\"Recency\"),\n",
    "    # Frequency: Total number of transactions\n",
    "    count(\"TransactionID\").alias(\"Frequency\"),\n",
    "    # Monetary: Total amount spent by the user\n",
    "    sum(\"TotalAmount\").alias(\"MonetaryValue\")\n",
    ")\n",
    "\n",
    "print(\"RFM Features:\")\n",
    "rfm_df.show(10)\n",
    "\n",
    "# 2. Prepare Features for ML Clustering (using MLlib)\n",
    "# K-Means requires features combined into a single vector column.\n",
    "\n",
    "# Define input columns for the vector\n",
    "feature_cols = [\"Recency\", \"Frequency\", \"MonetaryValue\"]\n",
    "\n",
    "# Use VectorAssembler to combine features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "# Use StandardScaler to scale features (important for distance-based algorithms like K-Means)\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
    "# setWithStd(True): Scale data to unit standard deviation.\n",
    "# setWithMean(True): Center data with mean before scaling (optional, depends on algorithm).\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=False # K-Means is sensitive to scale, less sensitive to mean centering\n",
    ")\n",
    "\n",
    "# Create a Pipeline to chain the assembler and scaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit the pipeline to the data (calculates necessary statistics like mean/std dev)\n",
    "scaler_model = pipeline.fit(rfm_df)\n",
    "\n",
    "# Transform the data to add the scaled features column\n",
    "rfm_scaled_df = scaler_model.transform(rfm_df)\n",
    "\n",
    "print(\"RFM Features Scaled for Clustering:\")\n",
    "rfm_scaled_df.select(\"UserID\", \"Recency\", \"Frequency\", \"MonetaryValue\", \"features_scaled\").show(10, truncate=False)\n",
    "\n",
    "# 3. Next Steps (Conceptual - MLlib K-Means)\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "#\n",
    "# kmeans = KMeans(featuresCol=\"features_scaled\", k=5) # Specify number of clusters (k)\n",
    "# model = kmeans.fit(rfm_scaled_df)\n",
    "# predictions = model.transform(rfm_scaled_df)\n",
    "# predictions.select(\"UserID\", \"prediction\").show() # 'prediction' column holds the cluster ID\n",
    "\n",
    "# 4. Output Prepared Features (Optional)\n",
    "rfm_scaled_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"path/to/output/customer_features_scaled.parquet\")\n",
    "\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **`agg(max(...))`**: Used to find the latest timestamp in the entire dataset, which serves as the reference point for calculating recency.\n",
    "*   **`first()[\"MaxTimestamp\"]`**: Retrieves the maximum timestamp value from the single-row DataFrame created by `agg`.\n",
    "*   **`lit(max_date)`**: Creates a literal column containing the `max_date` value, allowing it to be used in functions like `datediff` across all rows.\n",
    "*   **`datediff(lit(max_date), spark_max(\"Timestamp\"))`**: Calculates the difference in days between the reference date and the user's latest purchase date (`spark_max` within the `groupBy`).\n",
    "*   **`count(\"TransactionID\")`**: Counts transactions per user for Frequency.\n",
    "*   **`sum(\"TotalAmount\")`**: Sums purchase amounts per user for Monetary value.\n",
    "*   **`VectorAssembler`**: An MLlib *Transformer* that combines a list of columns into a single vector column (`features_unscaled`). This is a standard requirement for most MLlib algorithms.\n",
    "*   **`StandardScaler`**: An MLlib *Estimator*. When `fit()` is called, it computes the standard deviation (and optionally mean) for each feature in the input vector column. The resulting *Model* (`scaler_model`) can then `transform()` the data, applying the scaling.\n",
    "*   **`Pipeline`**: Chains multiple MLlib stages (Transformers and Estimators) together. `fit()` calls `fit()` on Estimators and `transform()` on Transformers sequentially. This simplifies the workflow.\n",
    "*   **`scaler_model.transform(rfm_df)`**: Applies the fitted pipeline (assembler then scaler) to the RFM data, producing the `features_scaled` column.\n",
    "*   **`features_scaled` column:** Contains the standardized RFM values in a vector format, ready for clustering algorithms like K-Means.\n",
    "\n",
    "**Use Cases:** Targeted marketing campaigns, personalized recommendations, customer churn prediction, understanding customer lifetime value.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Example 4: Twitter Sentiment Analysis (Streaming)\n",
    "\n",
    "**Goal:** Process a stream of tweets in near real-time, perform sentiment analysis, and aggregate results (e.g., count positive/negative tweets per topic).\n",
    "\n",
    "**PySpark Concepts Illustrated:** Structured Streaming (Sources, Sinks, Transformations), Windowing on Event Time, Watermarking.\n",
    "\n",
    "**Theory: Structured Streaming**\n",
    "\n",
    "Structured Streaming is Spark's scalable and fault-tolerant stream processing engine built on the Spark SQL engine. It treats a live data stream as a continuously growing table. You can apply standard DataFrame/SQL operations on this \"input table\" and output the results to a \"result table\".\n",
    "\n",
    "*   **Input Sources:** Kafka, Kinesis, Event Hubs, files (various formats), sockets (for testing).\n",
    "*   **Output Sinks:** Kafka, Kinesis, files, console (for debugging), foreachBatch (for custom logic, e.g., writing to databases).\n",
    "*   **Model:** Based on micro-batches. Spark processes stream data in small batches, achieving low end-to-end latency and exactly-once fault tolerance.\n",
    "*   **Event Time Processing:** Allows handling data based on timestamps embedded in the data itself (event time), rather than when Spark processes it (processing time). Crucial for handling out-of-order data.\n",
    "*   **Windowing:** Group data into time windows (e.g., 5-minute tumbling windows) for aggregation. Requires an event time column.\n",
    "*   **Watermarking:** A mechanism to handle late-arriving data in event-time processing. Spark tracks the maximum observed event time (watermark) and drops state for windows older than the watermark, limiting the amount of state maintained.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr, window, udf, lower\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Assume 'spark' session is available\n",
    "\n",
    "# 1. Define Schema for incoming JSON data (e.g., from Kafka)\n",
    "# Assuming tweets have 'text' and 'created_at' fields\n",
    "tweet_schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True), # Assuming string format initially\n",
    "    StructField(\"id_str\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "    # Add other fields as needed (e.g., user info)\n",
    "])\n",
    "\n",
    "# 2. Define Input Stream\n",
    "# Read from a Kafka source (replace with actual brokers and topic)\n",
    "# Requires 'spark-sql-kafka-0-10' package:\n",
    "# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.x.x ...\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"your_kafka_broker:9092\") \\\n",
    "    .option(\"subscribe\", \"twitter_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Kafka messages have 'key', 'value', 'topic', 'partition', 'offset', 'timestamp', 'timestampType'\n",
    "# The tweet data is usually in the 'value' column (as bytes)\n",
    "\n",
    "# 3. Process the Stream\n",
    "# Decode the 'value' column from bytes to string, then parse JSON\n",
    "# Select the 'value' column, cast it to STRING\n",
    "# Use from_json to parse the string using the defined schema\n",
    "parsed_stream = streaming_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), tweet_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") # Flatten the struct fields into columns\n",
    "\n",
    "# Convert 'created_at' string to Timestamp and handle potential format issues\n",
    "# Example format: \"Wed Oct 26 10:00:00 +0000 2023\"\n",
    "# Use expr for complex timestamp conversion\n",
    "# Note: Timestamp parsing in streams needs care; ensure format consistency.\n",
    "#       It might be better to pre-process timestamps into a standard format upstream.\n",
    "parsed_stream = parsed_stream.withColumn(\n",
    "    \"event_time\",\n",
    "    expr(\"to_timestamp(created_at, 'EEE MMM dd HH:mm:ss Z yyyy')\")\n",
    ")\n",
    "\n",
    "# Filter out rows where timestamp parsing failed\n",
    "parsed_stream = parsed_stream.filter(col(\"event_time\").isNotNull())\n",
    "\n",
    "# 4. Perform Sentiment Analysis (Illustrative - using a simple UDF)\n",
    "# In a real application, use a pre-trained model or a more robust library.\n",
    "def basic_sentiment(text):\n",
    "    text = text.lower()\n",
    "    if \"happy\" in text or \"good\" in text or \"excellent\" in text:\n",
    "        return \"positive\"\n",
    "    elif \"sad\" in text or \"bad\" in text or \"terrible\" in text:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "sentiment_udf = udf(basic_sentiment, StringType())\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_stream = parsed_stream.withColumn(\"sentiment\", sentiment_udf(lower(col(\"text\"))))\n",
    "\n",
    "# 5. Aggregate results over a time window with Watermarking\n",
    "# Count tweets by sentiment in 5-minute tumbling windows based on 'event_time'\n",
    "# Watermark: Allow data to be up to 10 minutes late before being dropped.\n",
    "windowed_counts = sentiment_stream \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"), # Tumbling window\n",
    "        col(\"sentiment\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "# 6. Define Output Sink and Start the Query\n",
    "# Output to console for debugging (use 'append', 'complete', or 'update' mode)\n",
    "# 'update': Only rows updated since the last trigger will be output.\n",
    "# 'complete': The entire updated result table will be output. Requires aggregation.\n",
    "# 'append': Only new rows added since the last trigger will be output. Not suitable for aggregations without watermarking.\n",
    "query = windowed_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# Keep the query running until terminated\n",
    "query.awaitTermination()\n",
    "\n",
    "# To stop programmatically (e.g., in another thread or based on a condition)\n",
    "# query.stop()\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **`spark.readStream.format(\"kafka\")...load()`**: Sets up the connection to Kafka as a streaming source.\n",
    "*   **`selectExpr(\"CAST(value AS STRING)\")`**: Selects the Kafka message value and casts it from binary to a string.\n",
    "*   **`from_json(col(\"value\"), tweet_schema).alias(\"data\")`**: Parses the JSON string in the `value` column according to `tweet_schema`, creating a struct column named `data`.\n",
    "*   **`.select(\"data.*\")`**: Promotes the fields within the `data` struct to top-level columns (`created_at`, `id_str`, `text`).\n",
    "*   **`expr(\"to_timestamp(...)\").alias(\"event_time\")`**: Parses the string timestamp into a Spark `TimestampType` column, which is crucial for event-time processing. The format string must match the input exactly.\n",
    "*   **`sentiment_udf = udf(...)`**: Defines a simple UDF for sentiment analysis (placeholder for a real model).\n",
    "*   **`.withWatermark(\"event_time\", \"10 minutes\")`**: Specifies the event-time column and the threshold for late data. Spark will keep state for windows until `max(event_time) - 10 minutes`. Data arriving later than this watermark will be dropped. This is essential for bounding state in aggregations.\n",
    "*   **`groupBy(window(...), col(...))`**: Groups data by a time window (here, 5-minute non-overlapping/tumbling windows on `event_time`) and the `sentiment` column.\n",
    "*   **`.count()`**: Aggregates the count within each group (window, sentiment).\n",
    "*   **`writeStream.outputMode(\"update\")...start()`**: Configures the output sink (console) and starts the streaming query execution. `update` mode is suitable for aggregations with watermarking, outputting only the rows whose counts changed in the current micro-batch.\n",
    "*   **`query.awaitTermination()`**: Blocks the main thread, keeping the application alive to process the stream continuously.\n",
    "\n",
    "**Use Cases:** Real-time monitoring dashboards, alerting systems, online machine learning model updates, IoT data processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Concepts: Performance Tuning & Optimization\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Writing efficient PySpark code involves understanding how Spark executes jobs and applying optimization techniques.\n",
    "\n",
    "1.  **Partitioning:**\n",
    "    *   Data in Spark is processed in partitions. The number of partitions affects parallelism. Too few partitions lead to underutilization of cluster resources; too many can cause overhead (task scheduling, small data per task).\n",
    "    *   **`df.rdd.getNumPartitions()`**: Check the current number of partitions.\n",
    "    *   **`repartition(n)` / `repartition(col)`**: Increases or decreases the number of partitions, involving a full data shuffle. Use `repartition(col)` to partition data based on column values, which can optimize subsequent joins or groupBys on that column (co-location of data).\n",
    "    *   **`coalesce(n)`**: Decreases the number of partitions *without* a full shuffle. It merges existing partitions on the same executor, making it more efficient than `repartition` for *reducing* partition count but can lead to uneven data distribution. Ideal before writing output to reduce the number of small files.\n",
    "    *   **Configuration:** `spark.sql.shuffle.partitions` (default 200) controls the number of partitions created after shuffles (joins, groupBys). Tune based on data size and cluster capacity.\n",
    "\n",
    "2.  **Caching/Persistence:** (Covered in Fraud Detection example) Re-use computed DataFrames efficiently. Monitor cache usage in Spark UI. Choose appropriate `StorageLevel` (MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.).\n",
    "\n",
    "3.  **Broadcast Joins:**\n",
    "    *   When joining a large DataFrame with a small one, Spark can *broadcast* the small DataFrame to all executors. This avoids shuffling the large DataFrame.\n",
    "    *   Controlled by `spark.sql.autoBroadcastJoinThreshold` (default 10MB). Spark automatically broadcasts tables smaller than this threshold.\n",
    "    *   Can be forced using `from pyspark.sql.functions import broadcast; large_df.join(broadcast(small_df), on=\"key\")`.\n",
    "    *   Monitor `explain()` output and Spark UI (DAG visualization) to see if broadcast joins are used.\n",
    "\n",
    "4.  **Catalyst Optimizer & Tungsten:**\n",
    "    *   **Catalyst:** Spark's extensible query optimizer. It analyzes DataFrame/SQL queries, applies optimization rules (e.g., predicate pushdown, column pruning, join reordering), and generates multiple logical and physical execution plans, choosing the most efficient one.\n",
    "    *   **Tungsten:** Spark's execution engine. Optimizes Spark jobs by operating directly on binary data (off-heap memory management), reducing garbage collection overhead and improving CPU efficiency through whole-stage code generation.\n",
    "    *   **Leverage Them:** Use DataFrame API and Spark SQL functions whenever possible, as they allow Catalyst and Tungsten to perform extensive optimizations. Avoid UDFs that hide logic from the optimizer.\n",
    "\n",
    "5.  **Data Formats:** Choose efficient storage formats like Parquet or ORC. They offer columnar storage (only read needed columns), predicate pushdown (filter data at the storage layer), and good compression. Avoid text-based formats like CSV or JSON for intermediate or large datasets.\n",
    "\n",
    "6.  **Spark UI:** An essential tool for monitoring and debugging. Access it typically via `http://<driver-node>:4040`. Key sections:\n",
    "    *   **Jobs:** View active and completed jobs triggered by actions.\n",
    "    *   **Stages:** See stages within jobs, identify shuffles, track task progress.\n",
    "    *   **Storage:** Monitor cached/persisted RDDs/DataFrames and memory usage.\n",
    "    *   **Environment:** Check Spark configuration properties.\n",
    "    *   **Executors:** View resource usage (memory, disk, cores) per executor, check for bottlenecks or failures.\n",
    "    *   **SQL:** Inspect query plans (logical, physical), identify bottlenecks like inefficient joins or scans.\n",
    "\n",
    "7.  **Skew Handling:** Data skew (where some partitions have vastly more data than others due to specific key values) can cripple performance, as stages wait for the longest-running tasks. Techniques include:\n",
    "    *   Salting: Add a random prefix/suffix to skewed keys before joining/grouping, then aggregate results afterwards.\n",
    "    *   Splitting skewed keys: Handle highly frequent keys separately.\n",
    "    *   Adaptive Query Execution (AQE, Spark 3.0+): Spark can dynamically optimize query plans at runtime, including handling skew by splitting/merging shuffle partitions (`spark.sql.adaptive.enabled=true`).\n",
    "\n",
    "**Code Example Snippets (Illustrative):**\n",
    "\n",
    "```python\n",
    "# Check partitions\n",
    "print(f\"Number of partitions: {my_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition before a join/groupBy (can help co-location)\n",
    "# Careful: Causes a shuffle\n",
    "partitioned_df = my_df.repartition(200, \"user_id\") # Partition by user_id into 200 partitions\n",
    "\n",
    "# Coalesce before writing to reduce small files (more efficient than repartition for reduction)\n",
    "final_df_coalesced = aggregated_df.coalesce(10)\n",
    "final_df_coalesced.write.parquet(\"path/to/output\")\n",
    "\n",
    "# Force broadcast join hint\n",
    "from pyspark.sql.functions import broadcast\n",
    "joined_df = large_df.join(broadcast(small_lookup_df), large_df[\"key\"] == small_lookup_df[\"id\"], \"left\")\n",
    "\n",
    "# View execution plan\n",
    "large_df.filter(col(\"value\") > 100).groupBy(\"category\").count().explain()\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "*   Use DataFrames and Spark SQL for structured data.\n",
    "*   Define explicit schemas when reading data.\n",
    "*   Use efficient file formats (Parquet, ORC).\n",
    "*   Filter data early (`filter()` before `join()` or `groupBy()`).\n",
    "*   Avoid `collect()` on large DataFrames.\n",
    "*   Use built-in functions over UDFs where possible. Consider Pandas UDFs otherwise.\n",
    "*   Tune `spark.sql.shuffle.partitions` and executor/driver memory.\n",
    "*   Leverage partitioning (`repartition`, `partitionBy` writing).\n",
    "*   Use `cache()`/`persist()` strategically.\n",
    "*   Monitor applications using the Spark UI.\n",
    "*   Enable and understand Adaptive Query Execution (AQE) in Spark 3+.\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive set of notes covers PySpark fundamentals, architecture, core APIs through practical project examples, and advanced optimization techniques, suitable for professional training and reference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
