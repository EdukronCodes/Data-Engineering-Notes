{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bfdbf2",
   "metadata": {},
   "source": [
    "# Lesson 15 - Medallion Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4069a",
   "metadata": {},
   "source": [
    "Okay, let's structure the technical notes for Lesson 15, focusing on the Medallion Architecture, robust pipelines, and data quality using PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Lesson 15 - The Medallion Architecture for Data Lakehouses\n",
    "\n",
    "### Introduction\n",
    "\n",
    "As organizations collect vast amounts of data, structuring and refining this data for reliable analytics and machine learning becomes paramount. The \"Data Lakehouse\" paradigm, combining the flexibility of data lakes with the management features of data warehouses, often employs architectural patterns to manage data flow and quality. The **Medallion Architecture** (often associated with Databricks and Delta Lake) is a popular pattern that organizes data into distinct layers – Bronze, Silver, and Gold – representing progressive stages of refinement. This lesson explores this architecture, principles for building robust PySpark data pipelines to implement it, and techniques for managing data quality throughout the process.\n",
    "\n",
    "### The Medallion Architecture Explained\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "The Medallion Architecture provides a blueprint for logically organizing data within a data lake or lakehouse. It promotes incremental data improvement, auditability, and decoupling of data processing stages. Data flows through successive layers, each serving a different purpose and audience.\n",
    "\n",
    "1.  **Bronze Layer (Raw Data):**\n",
    "    *   **Purpose:** Ingestion point for all source data into the lakehouse. Captures data in its original, unaltered state (\"as-is\"). Think of it as the historical archive.\n",
    "    *   **Characteristics:**\n",
    "        *   Mirrors source system structures (often).\n",
    "        *   Minimal transformations (perhaps only metadata addition like load timestamps, source identifiers).\n",
    "        *   Schema is often inferred or captured as it arrives (schema-on-read).\n",
    "        *   Immutable: Data is typically appended, preserving the raw history. Older records are not updated in place.\n",
    "        *   Long retention periods.\n",
    "    *   **Formats:** Can be diverse (JSON, CSV, Avro, Parquet, raw logs, database CDC streams). Using an efficient storage format like Delta Lake or Parquet even here can offer benefits like schema evolution tracking and time travel, but the *content* remains raw.\n",
    "    *   **Consumers:** Data engineers, data scientists (for exploration or fixing upstream issues). Rarely queried directly by analysts.\n",
    "    *   **Analogy:** The raw, unrefined metal ore.\n",
    "\n",
    "2.  **Silver Layer (Cleansed & Conformed Data):**\n",
    "    *   **Purpose:** Provides a validated, enriched, and more structured view of the data. This is where data quality rules, cleansing, and basic transformations (like joining reference data) occur.\n",
    "    *   **Characteristics:**\n",
    "        *   Data is cleansed (e.g., handling nulls, standardizing formats, type casting).\n",
    "        *   Data is validated against quality rules. Invalid data might be quarantined or flagged.\n",
    "        *   Schema is more defined and enforced (schema-on-write).\n",
    "        *   Data is often joined or conformed (e.g., standardizing codes, aligning data from different sources).\n",
    "        *   Represents a single \"source of truth\" for key business entities (e.g., customers, products).\n",
    "        *   May involve some level of normalization or semi-denormalization.\n",
    "    *   **Formats:** Typically query-optimized columnar formats like Delta Lake or Parquet. Delta Lake is highly recommended here for its ACID properties, time travel (debugging), and schema enforcement/evolution capabilities.\n",
    "    *   **Consumers:** Data engineers, data scientists (for feature engineering), data analysts (for ad-hoc querying).\n",
    "    *   **Analogy:** The refined, shaped, but unpolished silver medallion.\n",
    "\n",
    "3.  **Gold Layer (Curated Business-Level Data):**\n",
    "    *   **Purpose:** Delivers highly refined, aggregated data views optimized for specific business use cases, analytics, and reporting.\n",
    "    *   **Characteristics:**\n",
    "        *   Business-centric: Organized around business dimensions and measures.\n",
    "        *   Often denormalized and aggregated to support specific reporting needs (e.g., star schemas, data marts).\n",
    "        *   Focuses on performance for analytical queries.\n",
    "        *   Contains derived metrics, KPIs, and potentially features for ML models.\n",
    "        *   \"Ready-to-consume\" data.\n",
    "    *   **Formats:** Almost always Delta Lake or Parquet for performance and reliability.\n",
    "    *   **Consumers:** Business analysts, data scientists (consuming features), BI dashboards, reporting tools.\n",
    "    *   **Analogy:** The polished, finished, valuable gold medallion ready for display/use.\n",
    "\n",
    "**Benefits of the Medallion Architecture:**\n",
    "\n",
    "*   **Improved Data Quality:** Incremental validation and cleaning steps.\n",
    "*   **Auditability & Replayability:** Raw data in Bronze allows reprocessing if logic changes or errors are found later. Delta Lake's time travel enhances this.\n",
    "*   **Decoupling:** Changes in source systems primarily affect Bronze -> Silver pipelines. Changes in reporting needs primarily affect Silver -> Gold pipelines.\n",
    "*   **Self-Service:** Different user groups can reliably access data at the appropriate level of refinement.\n",
    "*   **Simplified Debugging:** Easier to trace data issues back through the layers.\n",
    "\n",
    "### Building Robust Data Pipelines with PySpark\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Implementing the Medallion Architecture requires building reliable data pipelines. Robustness means the pipeline is resilient to failures, produces consistent results, is maintainable, and can handle evolving requirements. Key principles include:\n",
    "\n",
    "1.  **Idempotency:** Running the pipeline multiple times with the same input should produce the same output state. This is crucial for recovery from failures. Delta Lake's `MERGE` operation and transactional writes are key enablers.\n",
    "2.  **Modularity:** Breaking down complex pipelines into smaller, reusable functions or stages (e.g., separate PySpark jobs/scripts for Bronze->Silver and Silver->Gold).\n",
    "3.  **Error Handling & Logging:** Implementing `try...except` blocks for I/O operations or complex transformations. Comprehensive logging helps diagnose issues.\n",
    "4.  **Configuration Management:** Externalizing configurations (paths, connection strings, business rules) instead of hardcoding them.\n",
    "5.  **Monitoring & Alerting:** Tracking pipeline execution status, data volumes, and quality metrics. Setting up alerts for failures or anomalies.\n",
    "6.  **Schema Management:** Handling schema changes gracefully (using Delta Lake's schema evolution or explicit schema definitions).\n",
    "7.  **Testing:** Implementing unit and integration tests for transformation logic and data quality rules.\n",
    "\n",
    "**PySpark Implementation Aspects:**\n",
    "\n",
    "*   **Structuring Jobs:** Organize PySpark code into functions or classes for readability and reuse (e.g., `ingest_to_bronze()`, `cleanse_to_silver()`, `aggregate_to_gold()`).\n",
    "*   **Parameterization:** Use command-line arguments or configuration files to pass parameters like input/output paths, dates, etc.\n",
    "*   **Delta Lake:** Leverage Delta Lake (`.format(\"delta\")`) for reading and writing, especially for Silver and Gold layers, to gain ACID transactions, time travel, schema enforcement, and `MERGE` capabilities for idempotency.\n",
    "*   **Partitioning:** Strategically partition data at each layer to optimize read/write performance based on common query patterns.\n",
    "    *   Bronze: Often partitioned by ingestion date (`/bronze/source_a/ingest_date=YYYY-MM-DD/`).\n",
    "    *   Silver: Might be partitioned by event date or key business identifiers (`/silver/events/event_date=YYYY-MM-DD/`, `/silver/customers/country=US/`).\n",
    "    *   Gold: Partitioned based on primary query dimensions (`/gold/sales_summary/region=EU/year=YYYY/`).\n",
    "\n",
    "**Code Example: Conceptual Pipeline Stages**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, input_file_name\n",
    "from delta.tables import DeltaTable\n",
    "import logging # Configure logging appropriately in a real application\n",
    "\n",
    "# Initialize SparkSession with Delta Lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MedallionPipelineExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- Configuration ---\n",
    "raw_source_path = \"/mnt/landing/source_x/*.json\" # Example path\n",
    "bronze_table_path = \"/mnt/lakehouse/bronze/source_x_events\"\n",
    "silver_table_path = \"/mnt/lakehouse/silver/events\"\n",
    "gold_table_path = \"/mnt/lakehouse/gold/daily_event_summary\"\n",
    "quarantine_path = \"/mnt/lakehouse/quarantine/events\"\n",
    "\n",
    "# --- Bronze Layer ---\n",
    "def ingest_to_bronze(source_path: str, bronze_path: str):\n",
    "    \"\"\"Ingests raw data, adds metadata, and appends to Bronze Delta table.\"\"\"\n",
    "    try:\n",
    "        print(f\"Ingesting from {source_path} to {bronze_path}\")\n",
    "        raw_df = spark.read.format(\"json\").load(source_path) # Adjust format as needed\n",
    "\n",
    "        # Add ingestion metadata\n",
    "        bronze_df = raw_df.withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                         .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "        # Append to Bronze table (Delta handles schema evolution if configured)\n",
    "        bronze_df.write.format(\"delta\") \\\n",
    "                 .mode(\"append\") \\\n",
    "                 .option(\"mergeSchema\", \"true\") # Allow schema evolution\n",
    "                 .save(bronze_path)\n",
    "        print(\"Ingestion to Bronze successful.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error ingesting to Bronze: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Silver Layer (including Data Quality) ---\n",
    "# (See Data Quality section below for DQ implementation details)\n",
    "def process_to_silver(bronze_path: str, silver_path: str, quarantine_path: str):\n",
    "    \"\"\"Reads from Bronze, applies cleaning/validation, writes good data to Silver, bad to quarantine.\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing from {bronze_path} to {silver_path}\")\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "        # --- Placeholder for Data Quality Checks ---\n",
    "        # validated_df = apply_quality_checks(bronze_df) # Returns DF with quality flags/results\n",
    "        # good_df = validated_df.filter(col(\"quality_status\") == \"PASS\")\n",
    "        # bad_df = validated_df.filter(col(\"quality_status\") == \"FAIL\")\n",
    "        # For simplicity here, let's assume basic filtering is the 'quality check'\n",
    "        \n",
    "        # Example: Assume 'event_id' and 'event_timestamp' are critical\n",
    "        validated_df = bronze_df.filter(col(\"event_id\").isNotNull() & col(\"event_timestamp\").isNotNull())\n",
    "        \n",
    "        # Simple type casting and selection for Silver\n",
    "        silver_df = validated_df.select(\n",
    "            col(\"event_id\").cast(\"string\"),\n",
    "            col(\"event_timestamp\").cast(\"timestamp\"),\n",
    "            col(\"payload.user_id\").alias(\"user_id\").cast(\"long\"), # Example transformation\n",
    "            col(\"payload.value\").alias(\"event_value\").cast(\"double\"),\n",
    "            \"source_file\", # Keep some provenance\n",
    "            \"ingest_timestamp\"\n",
    "        )\n",
    "\n",
    "        # Use MERGE for Idempotency (example assumes event_id is a unique key)\n",
    "        if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "            delta_table = DeltaTable.forPath(spark, silver_path)\n",
    "            delta_table.alias(\"target\") \\\n",
    "                .merge(silver_df.alias(\"source\"), \"target.event_id = source.event_id\") \\\n",
    "                .whenMatchedUpdateAll() \\\n",
    "                .whenNotMatchedInsertAll() \\\n",
    "                .execute()\n",
    "            print(\"Merge into Silver successful.\")\n",
    "        else:\n",
    "            silver_df.write.format(\"delta\") \\\n",
    "                     .mode(\"overwrite\") \\\n",
    "                     .partitionBy(\"ingest_timestamp\") # Example partitioning\n",
    "                     .save(silver_path)\n",
    "            print(\"Initial write to Silver successful.\")\n",
    "\n",
    "        # --- Handle Bad Data (Example) ---\n",
    "        # bad_df = bronze_df.filter(~(col(\"event_id\").isNotNull() & col(\"event_timestamp\").isNotNull()))\n",
    "        # if bad_df.count() > 0:\n",
    "        #    print(f\"Writing {bad_df.count()} records to quarantine: {quarantine_path}\")\n",
    "        #    bad_df.withColumn(\"quarantine_reason\", lit(\"Missing critical fields\")) \\\n",
    "        #          .write.format(\"delta\").mode(\"append\").save(quarantine_path)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing to Silver: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Gold Layer ---\n",
    "def aggregate_to_gold(silver_path: str, gold_path: str):\n",
    "    \"\"\"Reads from Silver, performs business aggregation, writes to Gold.\"\"\"\n",
    "    try:\n",
    "        print(f\"Aggregating from {silver_path} to {gold_path}\")\n",
    "        silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "        # Example: Daily event count per user\n",
    "        gold_df = silver_df.groupBy(F.to_date(\"event_timestamp\").alias(\"event_date\"), \"user_id\") \\\n",
    "                           .agg(F.count(\"*\").alias(\"daily_event_count\"),\n",
    "                                F.sum(\"event_value\").alias(\"total_daily_value\"))\n",
    "\n",
    "        # Overwrite or merge into Gold table\n",
    "        gold_df.write.format(\"delta\") \\\n",
    "               .mode(\"overwrite\") \\\n",
    "               .partitionBy(\"event_date\") # Optimize for queries filtering by date\n",
    "               .option(\"overwriteSchema\", \"true\") # Allow schema changes in Gold aggregations\n",
    "               .save(gold_path)\n",
    "        print(\"Aggregation to Gold successful.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error aggregating to Gold: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Pipeline Execution ---\n",
    "if ingest_to_bronze(raw_source_path, bronze_table_path):\n",
    "    if process_to_silver(bronze_table_path, silver_table_path, quarantine_path):\n",
    "        aggregate_to_gold(silver_table_path, gold_table_path)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Initialization:** Sets up SparkSession with Delta Lake extensions.\n",
    "2.  **Configuration:** Defines paths for different layers and quarantine zone. In production, use a proper config framework.\n",
    "3.  **`ingest_to_bronze`:**\n",
    "    *   Reads raw data (here, JSON).\n",
    "    *   Adds metadata columns (`ingest_timestamp`, `source_file`) for provenance.\n",
    "    *   Appends data to the Bronze Delta table using `.mode(\"append\")`. `mergeSchema=true` allows adding new columns found in source data without failing the job.\n",
    "4.  **`process_to_silver`:**\n",
    "    *   Reads from the Bronze Delta table.\n",
    "    *   **Placeholder for DQ:** Comments indicate where comprehensive DQ checks (`apply_quality_checks`) would fit. The example shows basic filtering (`isNotNull`).\n",
    "    *   Performs transformations: selecting columns, aliasing (`alias`), casting types (`cast`).\n",
    "    *   **Idempotency with MERGE:** Checks if the Silver table exists. If yes, uses `DeltaTable.merge()` to upsert data based on a key (`event_id`). This ensures that re-running the job for the same Bronze data doesn't create duplicates and can update existing records if needed. If the table doesn't exist, it performs an initial write.\n",
    "    *   **Partitioning:** Example shows partitioning Silver by `ingest_timestamp`. Choose partitions based on query patterns.\n",
    "    *   **Quarantine Handling:** Comments show how filtered-out `bad_df` could be written to a separate quarantine location with a reason.\n",
    "5.  **`aggregate_to_gold`:**\n",
    "    *   Reads from the validated Silver table.\n",
    "    *   Performs business aggregation (`groupBy`, `agg` with `count`, `sum`).\n",
    "    *   Writes to the Gold Delta table, often using `.mode(\"overwrite\")` for aggregate tables that represent a snapshot (like daily summaries). Partitioning (`event_date`) is crucial for query performance. `overwriteSchema=true` allows the schema of the aggregate table to change if the aggregation logic changes.\n",
    "6.  **Pipeline Execution:** Calls the functions sequentially. In a real system, use a workflow orchestrator (like Airflow, Databricks Workflows, Azure Data Factory). Basic error checking is shown.\n",
    "7.  **Error Handling:** Basic `try...except` blocks log errors (should be more detailed in production).\n",
    "\n",
    "### Managing Data Quality\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Data quality (DQ) is integral to the Medallion Architecture, primarily enforced during the Bronze -> Silver transition. It involves defining rules, measuring data against those rules, and deciding how to handle non-compliant data.\n",
    "\n",
    "**Common DQ Dimensions:**\n",
    "\n",
    "*   **Completeness:** Are required fields populated? (e.g., `isNotNull`)\n",
    "*   **Uniqueness:** Are identifiers unique? (e.g., `groupBy().count()` check)\n",
    "*   **Validity/Accuracy:** Do values conform to expected formats or ranges? (e.g., regex checks for emails, range checks for numbers, valid enum values)\n",
    "*   **Consistency:** Do related data points align across records or tables? (e.g., state/zip code consistency)\n",
    "*   **Timeliness:** Is the data arriving within the expected timeframe?\n",
    "\n",
    "**Implementing DQ Checks in PySpark:**\n",
    "\n",
    "*   **Built-in Functions:** Leverage `pyspark.sql.functions` extensively:\n",
    "    *   `isNull()`, `isNotNull()`\n",
    "    *   `when().otherwise()` for conditional logic/flagging\n",
    "    *   `length()`, `substring()` for string checks\n",
    "    *   `rlike()` for regex pattern matching\n",
    "    *   `cast()` for type validation (can fail or return null)\n",
    "    *   `assertNotNull()` (though often better to filter/flag than fail the job)\n",
    "*   **User-Defined Functions (UDFs):** For highly complex or custom validation logic not covered by built-ins (use sparingly due to performance implications).\n",
    "*   **Dedicated DQ Libraries:** Frameworks like `Deequ` (on Scala, can be used via Spark) or Python libraries integrated via UDFs/Pandas UDFs can provide more structured DQ definition and reporting. Databricks also offers built-in expectations with Delta Live Tables.\n",
    "\n",
    "**Handling Bad Records:**\n",
    "\n",
    "*   **Filtering:** Simply drop invalid records (simplest, but data is lost).\n",
    "*   **Flagging:** Add DQ metadata columns (e.g., `quality_status`, `validation_errors`) to records in the Silver layer. Allows downstream users to decide whether to use flagged records.\n",
    "*   **Quarantining:** Move invalid records to a separate \"quarantine\" table or location, often with metadata about why they failed. Allows for later investigation and potential reprocessing. This is often the preferred approach.\n",
    "\n",
    "**Code Example: Data Quality Checks (Conceptual Integration into `process_to_silver`)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, when, length, isnull, rlike, array, struct, lit\n",
    "\n",
    "def apply_quality_checks(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Applies various data quality checks and returns DataFrame with results.\"\"\"\n",
    "    \n",
    "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "\n",
    "    # Define checks as conditions\n",
    "    check_event_id_null = isnull(col(\"event_id\"))\n",
    "    check_timestamp_null = isnull(col(\"event_timestamp\"))\n",
    "    check_user_id_negative = col(\"payload.user_id\") < 0\n",
    "    check_email_format = ~col(\"payload.user_email\").rlike(email_regex) # Assuming email in payload\n",
    "\n",
    "    # Create flags or error messages for each check\n",
    "    df_with_checks = df.withColumn(\"dq_errors\", array(\n",
    "        when(check_event_id_null, lit(\"event_id is null\")).otherwise(lit(None)),\n",
    "        when(check_timestamp_null, lit(\"event_timestamp is null\")).otherwise(lit(None)),\n",
    "        when(check_user_id_negative, lit(\"user_id is negative\")).otherwise(lit(None)),\n",
    "        when(check_email_format, lit(\"email format invalid\")).otherwise(lit(None))\n",
    "    ))\n",
    "\n",
    "    # Filter out nulls from the error array\n",
    "    df_with_filtered_errors = df_with_checks.withColumn(\n",
    "        \"dq_errors\", F.expr(\"filter(dq_errors, x -> x is not null)\")\n",
    "    )\n",
    "    \n",
    "    # Determine overall status\n",
    "    df_with_status = df_with_filtered_errors.withColumn(\n",
    "        \"quality_status\",\n",
    "        when(F.size(col(\"dq_errors\")) == 0, \"PASS\").otherwise(\"FAIL\")\n",
    "    )\n",
    "    \n",
    "    return df_with_status\n",
    "\n",
    "# --- Integrating into process_to_silver ---\n",
    "# Inside the try block of process_to_silver:\n",
    "# ... read bronze_df ...\n",
    "validated_df = apply_quality_checks(bronze_df)\n",
    "validated_df.cache() # Cache if filtering multiple times\n",
    "\n",
    "good_df = validated_df.filter(col(\"quality_status\") == \"PASS\").drop(\"dq_errors\", \"quality_status\")\n",
    "bad_df = validated_df.filter(col(\"quality_status\") == \"FAIL\")\n",
    "\n",
    "# --- Process good_df to Silver (using MERGE etc. as before) ---\n",
    "# Select and transform columns from good_df\n",
    "silver_ready_df = good_df.select(...) \n",
    "# ... merge silver_ready_df into silver_table_path ...\n",
    "\n",
    "# --- Write bad_df to Quarantine ---\n",
    "if bad_df.count() > 0: # Check if bad_df is not empty before writing\n",
    "    print(f\"Writing {bad_df.count()} records to quarantine: {quarantine_path}\")\n",
    "    bad_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingest_timestamp\").save(quarantine_path)\n",
    "\n",
    "validated_df.unpersist()\n",
    "# ... rest of the function ...\n",
    "```\n",
    "\n",
    "**Code Explanation (DQ):**\n",
    "\n",
    "1.  **`apply_quality_checks` Function:** Encapsulates DQ logic.\n",
    "2.  **Define Checks:** Boolean Spark SQL expressions define conditions for failure (e.g., `isnull`, `<`, `~rlike`).\n",
    "3.  **Generate Error Messages:** Uses `when` to create string messages if a check fails, otherwise `null`. Collects these into an array column `dq_errors`.\n",
    "4.  **Filter Nulls:** Removes the `null` entries from the `dq_errors` array using `F.expr(\"filter(...)\")`.\n",
    "5.  **Determine Status:** Checks the size of the filtered `dq_errors` array. If empty, status is \"PASS\"; otherwise, \"FAIL\".\n",
    "6.  **Integration:**\n",
    "    *   Calls `apply_quality_checks` on the Bronze DataFrame.\n",
    "    *   Caches the result as it's used multiple times (for filtering good/bad).\n",
    "    *   Filters into `good_df` and `bad_df` based on `quality_status`.\n",
    "    *   Processes `good_df` for the Silver layer (selecting needed columns, dropping DQ columns).\n",
    "    *   Writes `bad_df` (including the `dq_errors` column) to the quarantine Delta table.\n",
    "    *   Unpersists the cached DataFrame.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Medallion Architecture provides a structured, scalable approach to building reliable data lakehouses. By progressively refining data through Bronze, Silver, and Gold layers using robust PySpark pipelines, organizations can improve data quality, ensure auditability, and deliver trustworthy data for analytics and AI. Implementing comprehensive data quality checks, leveraging Delta Lake features for reliability and idempotency, and adopting sound pipeline engineering principles (modularity, logging, configuration) are crucial for the success of this architecture. PySpark provides the powerful tools and APIs necessary to build and manage these sophisticated data processing workflows effectively.\n",
    "\n",
    "---\n",
    "**End of Lesson 15 Notes**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
