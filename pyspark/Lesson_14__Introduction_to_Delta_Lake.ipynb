{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267a96fc",
   "metadata": {},
   "source": [
    "# Lesson 14 - Introduction to Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedace0",
   "metadata": {},
   "source": [
    "Okay, here are detailed technical notes on Introduction to Delta Lake, suitable for professional learners using PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Introduction to Delta Lake\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Traditional data lakes, often built on top of distributed file systems like HDFS or cloud storage like AWS S3, Azure Data Lake Storage (ADLS), or Google Cloud Storage (GCS) using formats like Apache Parquet or ORC, provide massive scalability and cost-effectiveness for storing vast amounts of diverse data. However, they historically lacked critical features found in traditional data warehouses, leading to challenges around data reliability, consistency, and manageability. Issues like failed production jobs leaving data in corrupt states, inability to enforce data quality standards (schema), and difficulty performing updates or deletes were common, often leading to unreliable \"data swamps.\"\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions, scalable metadata handling, schema enforcement, and time travel capabilities directly to data lakes. It runs on top of your existing storage (S3, ADLS, GCS, HDFS) and is fully compatible with Apache Spark APIs. By using Delta Lake with PySpark, organizations can build reliable, high-performance data pipelines and enable data warehousing capabilities directly on their data lake, creating what is often referred to as a \"Lakehouse\" architecture.\n",
    "\n",
    "These notes explore the core concepts of Delta Lake and how to leverage its features using PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Delta Lake? Addressing Data Lake Challenges\n",
    "\n",
    "**Theory**\n",
    "\n",
    "Standard data lake storage formats like Parquet are immutable. Once a Parquet file is written, it cannot be easily updated row by row. This poses several challenges that Delta Lake aims to solve:\n",
    "\n",
    "1.  **Lack of ACID Transactions:** If a Spark job writing multiple Parquet files fails mid-way, the data lake is left in an inconsistent state with partial writes. There's no atomicity â€“ either the whole operation succeeds or fails cleanly. Concurrent reads during writes can see inconsistent data. Concurrent writes can corrupt data.\n",
    "2.  **Schema Enforcement Issues:** Data lakes traditionally follow a \"schema-on-read\" approach. While flexible, this can lead to data quality issues if data with incorrect schemas (wrong data types, missing/extra columns) is ingested. Detecting and fixing these issues downstream is complex and costly.\n",
    "3.  **Difficulty with Updates and Deletes:** Performing record-level updates or deletes on Parquet data is inefficient. It typically requires rewriting entire partitions or datasets, which is slow and computationally expensive. This makes use cases like Change Data Capture (CDC) or GDPR/CCPA compliance (right to be forgotten) difficult to implement.\n",
    "4.  **Scalability of Metadata:** Listing large numbers of files (common in partitioned datasets) in cloud storage can be slow and become a bottleneck, especially for tables with millions of small files.\n",
    "5.  **Lack of Versioning:** There's no built-in way to easily query the state of the data as it was at a specific point in time, making auditing, rollbacks, or reproducing experiments difficult.\n",
    "\n",
    "Delta Lake addresses these by introducing a transaction log (`_delta_log`) alongside the data files (which are typically stored in Parquet format). This log is the single source of truth, recording every transaction that modifies the data or table metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ACID Transactions\n",
    "\n",
    "**Theory**\n",
    "\n",
    "ACID properties guarantee data reliability during transactions (read/write operations):\n",
    "\n",
    "*   **Atomicity:** Ensures that all changes within a transaction are performed successfully, or none are. If a job writing to a Delta table fails, the transaction is aborted, and the table remains in its previous consistent state.\n",
    "*   **Consistency:** Guarantees that data is always in a valid state. Schema enforcement (discussed next) plays a key role here. Transactions bring the data from one valid state to another.\n",
    "*   **Isolation:** Ensures that concurrent transactions do not interfere with each other. Delta Lake uses optimistic concurrency control. When a transaction starts, it checks the table's current version. Before committing, it checks if the table has been modified by another transaction since it started reading. If so, the commit fails (optimistic concurrency exception), and the operation typically needs to be retried. This ensures that reads are always consistent and concurrent writes don't corrupt the table. Snapshot isolation ensures a reader always sees a consistent snapshot of the table from a specific version.\n",
    "*   **Durability:** Ensures that once a transaction is committed, its changes are permanent and survive system failures. This is achieved by writing the transaction log entries reliably to the underlying distributed storage.\n",
    "\n",
    "**How Delta Achieves ACID:**\n",
    "\n",
    "The core is the `_delta_log` directory within the Delta table's root directory.\n",
    "\n",
    "1.  When a transaction occurs (e.g., writing a DataFrame), Spark stages the new data files (Parquet).\n",
    "2.  It records the changes (files added, files removed) in a JSON file within the `_delta_log` directory (e.g., `00000000000000000001.json`).\n",
    "3.  Committing the transaction involves atomically writing this JSON file. Because writing a single file to most distributed file systems is atomic, this ensures the entire transaction is atomic.\n",
    "4.  Readers consult the log first to determine the current valid version of the table and which data files constitute that version. They ignore any staged files from incomplete transactions.\n",
    "\n",
    "**Code Example (Illustrating Atomic Appends/Overwrites)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import * # Import Delta Lake helper functions\n",
    "\n",
    "# --- Setup Spark Session with Delta Lake support ---\n",
    "# Note: This configuration is typically needed when running PySpark locally or on clusters\n",
    "# where Delta Lake is not pre-configured (like standard Spark distributions).\n",
    "# Databricks Runtime includes Delta Lake.\n",
    "builder = SparkSession.builder.appName(\"DeltaACIDDemo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define data and paths\n",
    "data1 = [(1, \"Apple\"), (2, \"Banana\")]\n",
    "columns = [\"id\", \"fruit\"]\n",
    "delta_path = \"/tmp/delta_acid_table\" # Use a suitable path (local or cloud storage)\n",
    "\n",
    "# --- Initial Write (Transaction 1) ---\n",
    "print(\"Performing initial write...\")\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"Data written to Delta table at: {delta_path}\")\n",
    "\n",
    "# Read the initial state\n",
    "print(\"\\nReading initial state:\")\n",
    "df_read1 = spark.read.format(\"delta\").load(delta_path)\n",
    "df_read1.show()\n",
    "\n",
    "# --- Append Data (Transaction 2 - Atomic) ---\n",
    "print(\"\\nPerforming atomic append...\")\n",
    "data2 = [(3, \"Cherry\")]\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "df2.write.format(\"delta\").mode(\"append\").save(delta_path) # Append is atomic\n",
    "\n",
    "# Read the state after append\n",
    "print(\"\\nReading state after append:\")\n",
    "df_read2 = spark.read.format(\"delta\").load(delta_path)\n",
    "df_read2.show()\n",
    "\n",
    "# --- Overwrite Data (Transaction 3 - Atomic) ---\n",
    "print(\"\\nPerforming atomic overwrite...\")\n",
    "data3 = [(4, \"Date\"), (5, \"Elderberry\")]\n",
    "df3 = spark.createDataFrame(data3, columns)\n",
    "# Overwrite replaces the entire table atomically\n",
    "df3.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Read the final state\n",
    "print(\"\\nReading state after overwrite:\")\n",
    "df_read3 = spark.read.format(\"delta\").load(delta_path)\n",
    "df_read3.show()\n",
    "\n",
    "# Simulate a FAILED write (conceptual - hard to force failure easily in simple script)\n",
    "# Imagine a large job writing data4 that crashes halfway.\n",
    "# Because Delta uses atomic commits to the log, the table would remain in the\n",
    "# state after Transaction 3 (data3), not partially written with data4.\n",
    "# The staged Parquet files from the failed job would exist but wouldn't be referenced\n",
    "# by the transaction log, so readers would ignore them.\n",
    "print(\"\\n(Conceptual) If a subsequent write failed, the table remains consistent.\")\n",
    "\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `from delta import *`: Imports necessary functions from the Delta Lake library, like `configure_spark_with_delta_pip`.\n",
    "2.  `builder = SparkSession.builder...`: Standard SparkSession builder setup.\n",
    "3.  `.config(\"spark.sql.extensions\", ...)` and `.config(\"spark.sql.catalog.spark_catalog\", ...)`: These configurations integrate Delta Lake's SQL parser and catalog capabilities into Spark.\n",
    "4.  `spark = configure_spark_with_delta_pip(builder).getOrCreate()`: A helper function from the `delta-spark` package that ensures necessary JARs are added and configurations applied.\n",
    "5.  `df1.write.format(\"delta\").mode(\"overwrite\").save(delta_path)`: Writes the DataFrame `df1` to the specified path in Delta format. `mode(\"overwrite\")` ensures any existing data at the path is replaced. This is the first transaction.\n",
    "6.  `spark.read.format(\"delta\").load(delta_path)`: Reads the data back from the Delta table. It automatically finds the latest valid version by consulting the `_delta_log`.\n",
    "7.  `df2.write.format(\"delta\").mode(\"append\").save(delta_path)`: Appends data from `df2` to the existing Delta table. This is Transaction 2, also atomic.\n",
    "8.  `df3.write.format(\"delta\").mode(\"overwrite\").save(delta_path)`: Overwrites the *entire* table contents with `df3`. This is Transaction 3, atomic. Even though it replaces all data, it's done as a single transactional unit.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "\n",
    "*   **Reliable ETL Pipelines:** Ensures that data pipelines either complete fully or leave the target table untouched, preventing data corruption from partial writes.\n",
    "*   **Streaming Data Ingestion:** Delta Lake is a popular sink for Spark Structured Streaming, providing exactly-once write semantics and allowing concurrent batch reads while streaming continues.\n",
    "*   **Concurrent Operations:** Allows multiple users or jobs to safely read and write to the same table concurrently (with optimistic concurrency handling potential conflicts).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Schema Enforcement\n",
    "\n",
    "**Theory**\n",
    "\n",
    "Schema enforcement prevents writing data to a Delta table that does not conform to the table's predefined schema. By default, Delta Lake enforces schema on write:\n",
    "\n",
    "*   If the DataFrame being written has columns that are *not* present in the target Delta table's schema, the write operation will fail.\n",
    "*   If the DataFrame being written has different data types for existing columns compared to the target table, the write will fail.\n",
    "*   If the DataFrame is *missing* columns that exist in the target table (and are not nullable), the write will fail unless those target columns are nullable (in which case `null` will be written).\n",
    "\n",
    "This strictness ensures data quality and consistency, preventing accidental corruption of the table structure.\n",
    "\n",
    "**Schema Evolution:** While enforcement is the default, Delta Lake also supports explicit schema evolution. If you intend to change the table schema (e.g., add new columns), you can use the `mergeSchema` option during the write:\n",
    "\n",
    "*   `df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(path)`: Allows adding new columns present in the DataFrame but not in the target table. Existing columns must still match data types.\n",
    "*   `df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(path)`: Allows completely replacing the table schema and data with the schema and data of the DataFrame being written (use with caution!).\n",
    "\n",
    "**Code Example**\n",
    "\n",
    "```python\n",
    "# Continuing from the previous SparkSession and delta_path setup\n",
    "\n",
    "# Current Schema (after overwrite with df3):\n",
    "print(\"Current table schema:\")\n",
    "current_df = spark.read.format(\"delta\").load(delta_path)\n",
    "current_df.printSchema()\n",
    "current_df.show()\n",
    "\n",
    "# --- Attempt 1: Write with Incompatible Schema (Extra Column) ---\n",
    "print(\"\\nAttempting to write data with an extra column (will fail by default)...\")\n",
    "data_extra_col = [(6, \"Fig\", \"Green\")]\n",
    "df_extra_col = spark.createDataFrame(data_extra_col, [\"id\", \"fruit\", \"color\"]) # Added 'color'\n",
    "\n",
    "try:\n",
    "    df_extra_col.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "except Exception as e:\n",
    "    print(f\"Write failed as expected: {e}\")\n",
    "\n",
    "# --- Attempt 2: Write with Incompatible Schema (Wrong Data Type) ---\n",
    "print(\"\\nAttempting to write data with wrong data type for 'id' (will fail by default)...\")\n",
    "data_wrong_type = [(\"7\", \"Grape\")] # 'id' as string instead of integer\n",
    "df_wrong_type = spark.createDataFrame(data_wrong_type, [\"id\", \"fruit\"])\n",
    "\n",
    "try:\n",
    "    df_wrong_type.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "except Exception as e:\n",
    "    print(f\"Write failed as expected: {e}\")\n",
    "\n",
    "# --- Attempt 3: Write with Schema Evolution (Adding a new column) ---\n",
    "print(\"\\nAppending data with a new column using schema evolution ('mergeSchema')...\")\n",
    "df_extra_col.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(delta_path)\n",
    "print(\"Write successful with schema evolution.\")\n",
    "\n",
    "# Read the table with the evolved schema\n",
    "print(\"\\nReading table after schema evolution:\")\n",
    "df_evolved = spark.read.format(\"delta\").load(delta_path)\n",
    "df_evolved.printSchema() # Note the new 'color' column\n",
    "df_evolved.show()\n",
    "# Explanation: Rows written before the schema change will have null for the new 'color' column.\n",
    "\n",
    "# --- Attempt 4: Overwriting Schema ---\n",
    "print(\"\\nOverwriting table and schema using 'overwriteSchema'...\")\n",
    "data_new_schema = [(\"A\", 100.5), (\"B\", 200.0)]\n",
    "df_new_schema = spark.createDataFrame(data_new_schema, [\"item_code\", \"value\"])\n",
    "\n",
    "df_new_schema.write.format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "print(\"Table and schema overwritten.\")\n",
    "\n",
    "# Read the completely new table structure\n",
    "print(\"\\nReading table after schema overwrite:\")\n",
    "df_overwritten = spark.read.format(\"delta\").load(delta_path)\n",
    "df_overwritten.printSchema()\n",
    "df_overwritten.show()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `current_df.printSchema()`: Displays the schema of the Delta table before attempting modifications.\n",
    "2.  `df_extra_col = ...`: Creates a DataFrame with an additional `color` column not present in the target table.\n",
    "3.  `try...except`: Catches the exception thrown by Spark/Delta when attempting to write `df_extra_col` without schema evolution, demonstrating default enforcement.\n",
    "4.  `df_wrong_type = ...`: Creates a DataFrame where the `id` column is a String, mismatching the IntegerType in the target table.\n",
    "5.  The second `try...except` block catches the error due to the data type mismatch.\n",
    "6.  `df_extra_col.write...option(\"mergeSchema\", \"true\")...save(delta_path)`: Performs the write again, but this time includes the `mergeSchema` option. Delta Lake adds the new `color` column to the table's schema definition in the transaction log.\n",
    "7.  `df_evolved.printSchema()`: Shows the updated schema including the nullable `color` column.\n",
    "8.  `df_new_schema = ...`: Creates a DataFrame with a completely different structure.\n",
    "9.  `df_new_schema.write...option(\"overwriteSchema\", \"true\")...save(delta_path)`: Uses `overwriteSchema` along with `mode(\"overwrite\")`. This replaces not only the data but also the schema of the target table entirely.\n",
    "10. `df_overwritten.printSchema()`: Shows the final, completely changed schema.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "\n",
    "*   **Data Quality:** Prevents accidental corruption of tables by ensuring incoming data adheres to expected structures and types.\n",
    "*   **Pipeline Robustness:** Makes ETL pipelines more resilient to unexpected changes in source data schemas.\n",
    "*   **Controlled Evolution:** Allows deliberate, controlled changes to table schemas over time as requirements evolve, without breaking the table.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Time Travel & Versioning\n",
    "\n",
    "**Theory**\n",
    "\n",
    "Every operation (write, update, delete, merge, schema change) that modifies a Delta table creates a new version. Delta Lake retains the transaction log entries and, through them, the history of the table. This allows users to query previous versions of the table data, a feature known as \"Time Travel.\"\n",
    "\n",
    "You can query older versions using either:\n",
    "\n",
    "1.  **Version Number:** Querying the table `AS OF VERSION <version_number>`. Version numbers are sequential integers starting from 0.\n",
    "2.  **Timestamp:** Querying the table `AS OF TIMESTAMP <timestamp_string>`. Delta finds the latest version committed at or before the specified timestamp. The timestamp string should be in a format Spark can parse (e.g., `'yyyy-MM-dd HH:mm:ss'`).\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "When you query a specific version or timestamp, Delta consults the transaction log to identify the set of data files (Parquet files) that constituted the table at that specific point in time. It then instructs Spark to read only those specific files.\n",
    "\n",
    "**Code Example**\n",
    "\n",
    "```python\n",
    "# Continuing from the previous SparkSession and delta_path setup\n",
    "# Let's see the history of our table first\n",
    "\n",
    "from delta.tables import * # Import DeltaTable for history and DML\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "print(\"\\nShowing table history:\")\n",
    "history_df = delta_table.history()\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# Assuming the 'overwriteSchema' write was version 4 (check history output)\n",
    "target_version = history_df.selectExpr(\"max(version)\").first()[0] - 1 # Get previous version\n",
    "if target_version < 0: target_version = 0\n",
    "\n",
    "print(f\"\\n--- Time Travelling ---\")\n",
    "\n",
    "# --- Query using Version Number ---\n",
    "print(f\"\\nReading table AS OF VERSION {target_version}:\")\n",
    "try:\n",
    "    df_version = spark.read.format(\"delta\").option(\"versionAsOf\", target_version).load(delta_path)\n",
    "    df_version.show()\n",
    "    df_version.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Could not read version {target_version}: {e}\") # Might fail if version doesn't exist or was VACUUMed\n",
    "\n",
    "# --- Query using Timestamp ---\n",
    "# Get timestamp of the target version from history\n",
    "target_timestamp = history_df.filter(f\"version = {target_version}\").select(\"timestamp\").first()[0]\n",
    "# Format timestamp for the option (adjust precision if needed)\n",
    "timestamp_str = target_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] # Example format\n",
    "\n",
    "print(f\"\\nReading table AS OF TIMESTAMP '{timestamp_str}' (corresponds to version {target_version}):\")\n",
    "try:\n",
    "    df_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_str).load(delta_path)\n",
    "    df_timestamp.show()\n",
    "    df_timestamp.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Could not read timestamp '{timestamp_str}': {e}\")\n",
    "\n",
    "# Query the latest version (standard read)\n",
    "print(\"\\nReading latest version (default):\")\n",
    "df_latest = spark.read.format(\"delta\").load(delta_path)\n",
    "df_latest.show()\n",
    "df_latest.printSchema()\n",
    "```\n",
    "\n",
    "**Line-by-Line Explanation:**\n",
    "\n",
    "1.  `from delta.tables import *`: Imports the `DeltaTable` class, which provides methods like `history()`, `update()`, `delete()`, `merge()`.\n",
    "2.  `delta_table = DeltaTable.forPath(spark, delta_path)`: Creates a `DeltaTable` object representing the table at the specified path.\n",
    "3.  `history_df = delta_table.history()`: Retrieves the transaction history of the table as a DataFrame. Each row represents a committed transaction (version).\n",
    "4.  `history_df.select(...).show()`: Displays key information about each transaction, like the version number, timestamp, operation type (WRITE, MERGE, etc.), and parameters.\n",
    "5.  `target_version = ...`: Determines a previous version number to query (e.g., the second to last version).\n",
    "6.  `spark.read.format(\"delta\").option(\"versionAsOf\", target_version).load(delta_path)`: Reads the Delta table, specifically requesting the data as it existed at `target_version`.\n",
    "7.  `target_timestamp = ...`: Retrieves the timestamp associated with the `target_version` from the history DataFrame.\n",
    "8.  `timestamp_str = ...`: Formats the timestamp into a string suitable for the `timestampAsOf` option.\n",
    "9.  `spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_str).load(delta_path)`: Reads the Delta table as it existed at the specified timestamp.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "\n",
    "*   **Auditing & Governance:** Track changes to data over time, identify when specific records were modified or deleted.\n",
    "*   **Debugging Data Pipelines:** If a pipeline introduces bad data, time travel allows inspecting the table state *before* the problematic job ran.\n",
    "*   **Rollbacks:** Easily revert the table to a previous known-good state if a recent transaction caused issues (often done by reading the old version and overwriting the table with it).\n",
    "*   **Reproducing ML Experiments:** Query the exact version of the dataset used for training a model to ensure reproducibility.\n",
    "\n",
    "**Important Note on VACUUM:** Delta Lake retains historical data files and log entries indefinitely by default. However, the `VACUUM` command can be used to physically remove data files no longer referenced by recent versions within a specified retention period (default is 7 days). Once files are vacuumed, you can no longer time travel back beyond the retention period for those files. Use `VACUUM` carefully to manage storage costs while preserving necessary history.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Advanced Considerations & Best Practices\n",
    "\n",
    "*   **Updates, Deletes, and Merges:** Delta Lake supports standard DML operations (`UPDATE`, `DELETE`, `MERGE INTO`) using the `DeltaTable` API or SQL. These operations also create new table versions and are ACID compliant. The `MERGE INTO` (upsert) operation is particularly powerful for efficiently applying changes from a source dataset to a target Delta table.\n",
    "    ```python\n",
    "    # Example MERGE (Upsert)\n",
    "    # Assume delta_table is the target DeltaTable object\n",
    "    # source_df contains new data/updates with columns 'id', 'fruit', 'color'\n",
    "\n",
    "    # delta_table.alias(\"target\").merge(\n",
    "    #     source=source_df.alias(\"source\"),\n",
    "    #     condition=\"target.id = source.id\"  # Join condition\n",
    "    # ).whenMatchedUpdate(set={          # Action if match found\n",
    "    #     \"fruit\": col(\"source.fruit\"),\n",
    "    #     \"color\": col(\"source.color\")\n",
    "    # }).whenNotMatchedInsert(values={    # Action if no match found\n",
    "    #     \"id\": col(\"source.id\"),\n",
    "    #     \"fruit\": col(\"source.fruit\"),\n",
    "    #     \"color\": col(\"source.color\")\n",
    "    # }).execute()\n",
    "    ```\n",
    "*   **Partitioning:** Just like with Parquet, partitioning Delta tables (e.g., by date) using `partitionBy()` during writes significantly improves query performance when filtering on partition columns, as Spark can skip reading irrelevant partitions (data skipping). Delta Lake stores partition information in the transaction log, making partition discovery faster than traditional file listing.\n",
    "*   **OPTIMIZE Command:** Over time, many small files can accumulate in a Delta table due to frequent appends, merges, or streaming updates. This \"small file problem\" hurts read performance. The `OPTIMIZE` command compacts small files into larger ones, improving read throughput.\n",
    "    ```python\n",
    "    # delta_table.optimize().executeCompaction() # Basic compaction\n",
    "    # spark.sql(f\"OPTIMIZE '{delta_path}'\")      # SQL equivalent\n",
    "    ```\n",
    "*   **Z-Ordering (Multi-dimensional Clustering):** An enhancement to `OPTIMIZE`. `OPTIMIZE ZORDER BY (col1, col2)` co-locates related data within the compacted files based on the specified Z-Order columns. If queries frequently filter on these columns (especially non-partition columns), Z-Ordering can significantly improve data skipping and query speed. Choose low-cardinality columns frequently used in filters for Z-Ordering.\n",
    "    ```python\n",
    "    # delta_table.optimize().executeZOrderBy(\"filter_col1\", \"filter_col2\")\n",
    "    # spark.sql(f\"OPTIMIZE '{delta_path}' ZORDER BY (filter_col1, filter_col2)\")\n",
    "    ```\n",
    "*   **Performance Tuning:** Besides partitioning and optimization, standard Spark tuning practices apply (e.g., cluster sizing, shuffle partitions, memory management). Delta Lake's transaction log processing adds a small overhead but enables features that often lead to better overall pipeline reliability and performance compared to managing raw Parquet files manually.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Delta Lake significantly enhances data lakes by bringing reliability, data quality guarantees, and powerful DML/versioning features previously associated with data warehouses. By leveraging ACID transactions, schema enforcement, and time travel through its transaction log mechanism, Delta Lake allows organizations to build robust, scalable, and trustworthy \"Lakehouse\" architectures directly on cost-effective cloud storage. Its seamless integration with the PySpark API makes it a powerful tool for data engineers and data scientists working with large-scale datasets in the Spark ecosystem. Understanding and utilizing these features effectively is key to building modern, reliable data platforms.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
