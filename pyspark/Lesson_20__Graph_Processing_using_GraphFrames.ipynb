{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62ff4f3",
   "metadata": {},
   "source": [
    "# Lesson 20 - Graph Processing using GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b05c0",
   "metadata": {},
   "source": [
    "Okay, let's craft the detailed technical notes on Graph Processing using GraphFrames in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Lesson 20 - Graph Processing using GraphFrames\n",
    "\n",
    "**Objective:** This document provides a comprehensive guide to performing graph processing and analysis using the GraphFrames library within the PySpark environment. It covers the fundamentals of GraphFrames, core graph algorithms, application examples like recommendation systems, and advanced considerations for performance optimization.\n",
    "\n",
    "**Target Audience:** Data scientists, engineers, and analysts familiar with PySpark DataFrames who need to perform scalable graph analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to GraphFrames\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Graphs are fundamental data structures used to model relationships between entities. Examples include social networks (users and friendships), web graphs (pages and hyperlinks), transportation networks (locations and routes), and knowledge graphs. Analyzing these relationships can reveal valuable insights, such as influential entities, community structures, and shortest paths.\n",
    "\n",
    "Apache Spark provides two primary libraries for graph processing:\n",
    "\n",
    "1.  **GraphX:** The original RDD-based API, primarily available in Scala. It offers fine-grained control and potentially higher performance for certain highly iterative graph algorithms due to lower-level optimizations.\n",
    "2.  **GraphFrames:** A newer library built on top of Spark DataFrames. It offers several advantages:\n",
    "    *   **Unified API:** Seamless integration with Spark DataFrames and Spark SQL, allowing graph operations to be easily combined with structured data processing.\n",
    "    *   **Language Support:** First-class support for Python (PySpark), Scala, and Java.\n",
    "    *   **Query Optimization:** Leverages the Catalyst optimizer and Tungsten execution engine inherited from DataFrames.\n",
    "    *   **Rich Query Capabilities:** Supports declarative graph queries using motif finding, similar to Cypher (Neo4j) or SPARQL.\n",
    "\n",
    "GraphFrames represent graphs using two DataFrames:\n",
    "\n",
    "*   **Vertices DataFrame:** Contains the nodes (vertices) of the graph. Must have a special column named `id` uniquely identifying each vertex. Can contain any other columns representing vertex attributes (e.g., name, age, properties).\n",
    "*   **Edges DataFrame:** Contains the connections (edges) between vertices. Must have two special columns: `src` (source vertex ID) and `dst` (destination vertex ID). Can contain other columns representing edge attributes (e.g., relationship type, weight, timestamp).\n",
    "\n",
    "**Note:** GraphFrames is not part of the standard PySpark distribution. You typically need to install it as a package using `--packages graphframes:graphframes:0.8.2-spark3.2-s_2.12` (adjust version numbers according to your Spark and Scala versions) when launching `pyspark` or configuring your SparkSession.\n",
    "\n",
    "**Code Example: Creating a GraphFrame**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Required for GraphFrames - make sure the package is added during session creation\n",
    "# Example: spark-submit --packages graphframes:graphframes:0.8.2-spark3.2-s_2.12 your_script.py\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"GraphFramesIntroduction\").getOrCreate()\n",
    "# Set checkpoint directory - Recommended for iterative algorithms\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\") # Use a persistent DFS path in production\n",
    "\n",
    "# 1. Define Vertices DataFrame\n",
    "# Must contain an 'id' column\n",
    "vertices_data = [\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "    (\"d\", \"David\", 29),\n",
    "    (\"e\", \"Esther\", 32),\n",
    "    (\"f\", \"Fanny\", 38),\n",
    "    (\"g\", \"Gabby\", 60) # An isolated vertex\n",
    "]\n",
    "vertices_schema = [\"id\", \"name\", \"age\"]\n",
    "vertices = spark.createDataFrame(vertices_data, vertices_schema)\n",
    "\n",
    "# 2. Define Edges DataFrame\n",
    "# Must contain 'src' and 'dst' columns, referencing vertex IDs\n",
    "edges_data = [\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"b\", \"c\", \"follow\"),\n",
    "    (\"c\", \"b\", \"follow\"),\n",
    "    (\"f\", \"c\", \"follow\"),\n",
    "    (\"e\", \"f\", \"friend\"),\n",
    "    (\"e\", \"d\", \"friend\"),\n",
    "    (\"d\", \"a\", \"friend\"),\n",
    "    (\"a\", \"e\", \"friend\") # Added edge to create a cycle\n",
    "]\n",
    "edges_schema = [\"src\", \"dst\", \"relationship\"]\n",
    "edges = spark.createDataFrame(edges_data, edges_schema)\n",
    "\n",
    "# 3. Create the GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "\n",
    "# Display the GraphFrame components\n",
    "print(\"Vertices DataFrame:\")\n",
    "g.vertices.show()\n",
    "\n",
    "print(\"\\nEdges DataFrame:\")\n",
    "g.edges.show()\n",
    "\n",
    "# Basic Graph Attributes\n",
    "print(f\"\\nNumber of vertices: {g.vertices.count()}\")\n",
    "print(f\"Number of edges: {g.edges.count()}\")\n",
    "\n",
    "# Display vertex degrees\n",
    "print(\"\\nVertex Degrees (total):\")\n",
    "g.degrees.show()\n",
    "\n",
    "print(\"\\nVertex In-Degrees:\")\n",
    "g.inDegrees.show()\n",
    "\n",
    "print(\"\\nVertex Out-Degrees:\")\n",
    "g.outDegrees.show()\n",
    "\n",
    "# Stop Spark Session\n",
    "# spark.stop() # Usually at the end of the script\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Import necessary libraries:** `SparkSession` for Spark context, `col` for DataFrame operations, and `GraphFrame` from the `graphframes` library.\n",
    "2.  **Initialize SparkSession:** Standard SparkSession setup. Setting a checkpoint directory is good practice, especially for iterative algorithms covered later.\n",
    "3.  **Define Vertices:** Create a list of tuples representing vertex data. Each tuple contains the `id` and other attributes (`name`, `age`). Create a PySpark DataFrame `vertices` using `spark.createDataFrame`.\n",
    "4.  **Define Edges:** Create a list of tuples representing edge data. Each tuple contains the `src` ID, `dst` ID, and other edge attributes (`relationship`). Create a PySpark DataFrame `edges`.\n",
    "5.  **Create GraphFrame:** Instantiate the `GraphFrame` object using the `vertices` and `edges` DataFrames.\n",
    "6.  **Display Components:** Show the underlying `vertices` and `edges` DataFrames using `.show()`.\n",
    "7.  **Basic Attributes:** Use `.count()` on the vertex/edge DataFrames to get graph size.\n",
    "8.  **Vertex Degrees:** Access pre-computed degree information:\n",
    "    *   `g.degrees`: DataFrame with `id` and `degree` (total number of edges connected to the vertex).\n",
    "    *   `g.inDegrees`: DataFrame with `id` and `inDegree` (number of incoming edges).\n",
    "    *   `g.outDegrees`: DataFrame with `id` and `outDegree` (number of outgoing edges).\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   Representing social networks, organizational structures, network topologies.\n",
    "*   Initial data exploration of graph structures.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Graph Queries with Motif Finding\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "GraphFrames allow for powerful declarative queries using **motif finding**. A motif is a structural pattern you want to find within the graph. You specify the pattern using a syntax similar to Cypher (used in Neo4j):\n",
    "\n",
    "*   Nodes are represented by parentheses: `(a)`, `(b)`.\n",
    "*   Edges are represented by square brackets: `-[e]->`. The hyphen indicates an undirected connection, while the arrow `->` indicates direction (`src` to `dst`). You can also use `<-`.\n",
    "*   Variables (`a`, `b`, `e`) are assigned to nodes and edges in the pattern.\n",
    "*   Patterns are chained using semicolons: `(a)-[e1]->(b); (b)-[e2]->(c)`.\n",
    "\n",
    "The `g.find(motif)` method returns a DataFrame where each row corresponds to a match of the motif in the graph. The columns of the resulting DataFrame correspond to the variables assigned in the motif pattern (e.g., columns `a`, `e1`, `b`, `e2`, `c` for the pattern above). Each column contains the vertex or edge data (as a StructType) that matched that part of the pattern.\n",
    "\n",
    "**Code Example: Motif Finding**\n",
    "\n",
    "```python\n",
    "# Assuming 'g' GraphFrame from the previous example is available\n",
    "\n",
    "# Motif 1: Find pairs of users who are friends with each other (mutual relationship)\n",
    "# (a)-[e1]->(b) where relationship is 'friend'\n",
    "# (b)-[e2]->(a) where relationship is 'friend'\n",
    "mutual_friends_motif = g.find(\"(a)-[e1]->(b); (b)-[e2]->(a)\") \\\n",
    "    .filter((col(\"e1.relationship\") == \"friend\") & (col(\"e2.relationship\") == \"friend\")) \\\n",
    "    .filter(col(\"a.id\") < col(\"b.id\")) # Avoid duplicates like (a,b) and (b,a)\n",
    "\n",
    "print(\"\\nMotif: Mutual Friends\")\n",
    "mutual_friends_motif.select(\"a.id\", \"a.name\", \"b.id\", \"b.name\").show()\n",
    "\n",
    "# Motif 2: Find users (a) who follow someone (b), and that someone (b) follows them back (c == a).\n",
    "# Equivalent to the mutual friends motif but demonstrates referencing the same node\n",
    "follow_back_motif = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\") \\\n",
    "    .filter((col(\"e1.relationship\") == \"follow\") & (col(\"e2.relationship\") == \"follow\")) \\\n",
    "    .filter(col(\"a.id\") == col(\"c.id\")) # Ensure c is the same person as a\n",
    "\n",
    "print(\"\\nMotif: Mutual Followers\")\n",
    "# Select distinct pairs to avoid showing the same pair twice if multiple edge types exist\n",
    "follow_back_motif.select(\"a.id\", \"a.name\", \"b.id\", \"b.name\").distinct().show()\n",
    "\n",
    "\n",
    "# Motif 3: Find \"structural holes\" or \"triangles\": A follows B, B follows C, but A does not follow C\n",
    "# Find all paths A->B->C\n",
    "paths = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\")\n",
    "\n",
    "# Find all direct edges A->C\n",
    "direct_edges = g.edges.filter(col(\"relationship\") == \"follow\").select(col(\"src\").alias(\"a_id\"), col(\"dst\").alias(\"c_id\"))\n",
    "\n",
    "# Join paths with direct edges to find where A->C exists\n",
    "# We are looking for paths where this join *fails* (left_anti join)\n",
    "potential_triangles = paths.select(col(\"a.id\").alias(\"a_id\"), col(\"a.name\").alias(\"a_name\"),\n",
    "                                 col(\"b.id\").alias(\"b_id\"), col(\"b.name\").alias(\"b_name\"),\n",
    "                                 col(\"c.id\").alias(\"c_id\"), col(\"c.name\").alias(\"c_name\")) \\\n",
    "                           .filter(col(\"e1.relationship\") == \"follow\") \\\n",
    "                           .filter(col(\"e2.relationship\") == \"follow\") \\\n",
    "                           .filter(col(\"a_id\") != col(\"c_id\")) # Ensure A and C are different people\n",
    "\n",
    "\n",
    "triangles_where_a_follows_c = potential_triangles.join(\n",
    "    direct_edges,\n",
    "    (potential_triangles.a_id == direct_edges.a_id) & (potential_triangles.c_id == direct_edges.c_id)\n",
    ")\n",
    "\n",
    "# This requires more complex logic, potentially using subgraph queries or multiple finds.\n",
    "# A simpler interpretation: Find chains of length 2 (A->B->C)\n",
    "print(\"\\nMotif: Chains of two follows (A->B->C)\")\n",
    "potential_triangles.show()\n",
    "\n",
    "# Stop Spark Session\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Motif 1 (Mutual Friends):**\n",
    "    *   `g.find(\"(a)-[e1]->(b); (b)-[e2]->(a)\")`: Defines a motif looking for two nodes `a` and `b` connected by edges `e1` (a to b) and `e2` (b to a).\n",
    "    *   `.filter(...)`: Filters the results to only include matches where both relationships are 'friend'. We access edge attributes using `col(\"e1.relationship\")`.\n",
    "    *   `.filter(col(\"a.id\") < col(\"b.id\"))`: Filters out duplicate pairs (e.g., if (Alice, Bob) is found, don't also show (Bob, Alice)).\n",
    "    *   `.select(...)`: Selects specific columns (node IDs and names) for clearer output.\n",
    "\n",
    "2.  **Motif 2 (Mutual Followers):**\n",
    "    *   `g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\")`: Finds paths of length two.\n",
    "    *   `.filter(...)`: Filters for 'follow' relationships on both edges.\n",
    "    *   `.filter(col(\"a.id\") == col(\"c.id\"))`: Crucially, this ensures the end node `c` is the same as the start node `a`, effectively finding mutual follows (A follows B, B follows A).\n",
    "    *   `.distinct()`: Ensures each pair is shown only once.\n",
    "\n",
    "3.  **Motif 3 (Chains):**\n",
    "    *   This example demonstrates finding simple paths of length two (`A -> B -> C`) where the relationship is 'follow'.\n",
    "    *   Finding \"structural holes\" (where A->B, B->C but *not* A->C) is more complex and often involves anti-joins or more sophisticated graph operations beyond simple motif finding. The example shows finding the A->B->C paths.\n",
    "    *   `.filter(col(\"a_id\") != col(\"c_id\"))`: Prevents considering paths like A->B->A as part of the chain.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   Identifying specific structural patterns in networks (e.g., fraud detection rings, protein interactions).\n",
    "*   Social network analysis (finding mutual connections, common friends).\n",
    "*   Knowledge graph querying.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Graph Algorithms\n",
    "\n",
    "GraphFrames provides implementations of standard graph algorithms that operate directly on the GraphFrame object.\n",
    "\n",
    "#### 3.1 PageRank\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "PageRank is an algorithm originally used by Google to rank web pages based on their importance in the hyperlink structure. It assigns a numerical weight (PageRank score) to each node in a directed graph. The core idea is that a node is considered important if it is linked to by other important nodes. It's an iterative algorithm where nodes \"vote\" for others by distributing their own PageRank score among the nodes they link to. A damping factor (usually ~0.85, corresponding to `resetProbability`=0.15) is used to simulate a user randomly jumping to any page, preventing scores from getting trapped in small cycles.\n",
    "\n",
    "**PySpark Implementation:** `graphframe.pageRank()`\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "*   `resetProbability` (float): Probability of resetting to a random vertex. Corresponds to `1 - dampingFactor`. Default is 0.15.\n",
    "*   `maxIter` (int): Maximum number of iterations to run. Default is 10.\n",
    "*   `tol` (float): Convergence tolerance. If the PageRank values change by less than this amount between iterations, the algorithm stops early. Default 0.01.\n",
    "*   `sourceId` (str): Optional vertex ID for personalized PageRank, biasing the random jumps towards this specific node.\n",
    "\n",
    "**Output:** Returns a GraphFrame where the `vertices` DataFrame has an additional `pagerank` column, and the `edges` DataFrame has an additional `weight` column (representing the contribution of that edge to the destination's PageRank in an iteration).\n",
    "\n",
    "**Code Example: PageRank**\n",
    "\n",
    "```python\n",
    "# Assuming 'g' GraphFrame is available\n",
    "\n",
    "print(\"\\nCalculating PageRank...\")\n",
    "# Run PageRank algorithm\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# Display PageRank scores for vertices\n",
    "print(\"\\nPageRank Scores:\")\n",
    "pr_results.vertices.select(\"id\", \"name\", \"pagerank\").orderBy(col(\"pagerank\").desc()).show()\n",
    "\n",
    "# Display edge weights (less commonly used, shows transitional probability)\n",
    "# print(\"\\nPageRank Edge Weights:\")\n",
    "# pr_results.edges.select(\"src\", \"dst\", \"weight\").show()\n",
    "\n",
    "# Example: Run Personalized PageRank for node 'a' (Alice)\n",
    "# print(\"\\nCalculating Personalized PageRank for Alice (a)...\")\n",
    "# ppr_results_a = g.pageRank(resetProbability=0.15, maxIter=10, sourceId=\"a\")\n",
    "# print(\"\\nPersonalized PageRank Scores (source='a'):\")\n",
    "# ppr_results_a.vertices.select(\"id\", \"name\", \"pagerank\").orderBy(col(\"pagerank\").desc()).show()\n",
    "\n",
    "# Stop Spark Session\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Call `pageRank()`:** Invoke the `g.pageRank()` method on the GraphFrame.\n",
    "    *   `resetProbability=0.15`: Standard damping factor setting.\n",
    "    *   `maxIter=10`: Limits the computation time. Convergence might occur earlier if `tol` is met.\n",
    "2.  **Access Results:** The method returns a *new* GraphFrame (`pr_results`).\n",
    "    *   `pr_results.vertices`: Contains the original vertex data plus the calculated `pagerank` score.\n",
    "    *   `pr_results.edges`: Contains the original edge data plus a `weight` column (often related to `1 / outDegree(src)`).\n",
    "3.  **Display Scores:** Select the relevant columns (`id`, `name`, `pagerank`) from the result vertices DataFrame and display them, ordered by PageRank score.\n",
    "4.  **Personalized PageRank (Optional):** The commented-out section shows how to use the `sourceId` parameter to calculate PageRank biased towards a specific starting node ('a' in this case). This identifies nodes strongly connected *to* or influenced *by* the source node.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   Ranking web pages (original use case).\n",
    "*   Identifying influential users in social networks.\n",
    "*   Detecting important proteins in biological networks.\n",
    "*   Recommender systems (ranking items based on user navigation patterns).\n",
    "\n",
    "#### 3.2 Connected Components\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Connected Components algorithms find disjoint subgraphs (sets of nodes) where every node within a set is reachable from every other node in the same set (following edges, potentially ignoring direction depending on the algorithm variant). In simpler terms, it finds the \"islands\" or \"clusters\" within a graph. GraphFrames typically implements the Weakly Connected Components (WCC) algorithm, which treats edges as undirected.\n",
    "\n",
    "**PySpark Implementation:** `graphframe.connectedComponents()` or `graphframe.stronglyConnectedComponents()`\n",
    "\n",
    "*   `connectedComponents()`: Finds Weakly Connected Components (WCC). Treats edges as undirected. Faster and more common.\n",
    "*   `stronglyConnectedComponents()`: Finds Strongly Connected Components (SCC). Considers edge direction – requires a path from A to B *and* B to A (within the component). Computationally more expensive.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "*   `maxIter` (int): (For SCC) Maximum number of iterations.\n",
    "*   `checkpointInterval` (int): How often to checkpoint the intermediate results during iterative computation (useful for very large graphs or long-running jobs). Setting this requires `spark.sparkContext.setCheckpointDir()` to be configured. Default -1 (no checkpointing within the algorithm call).\n",
    "\n",
    "**Output:** Returns a DataFrame (not a GraphFrame) with the vertex schema plus an additional column named `component`. All vertices belonging to the same component will have the same `component` ID (a unique long integer, usually the lowest vertex ID within that component).\n",
    "\n",
    "**Code Example: Connected Components**\n",
    "\n",
    "```python\n",
    "# Assuming 'g' GraphFrame is available\n",
    "\n",
    "print(\"\\nCalculating Weakly Connected Components (WCC)...\")\n",
    "# Use checkpointing for iterative algorithms\n",
    "# Ensure spark.sparkContext.setCheckpointDir() was called earlier\n",
    "wcc_results = g.connectedComponents(checkpointInterval=2) # Checkpoint every 2 iterations\n",
    "\n",
    "print(\"\\nWeakly Connected Components:\")\n",
    "wcc_results.select(\"id\", \"name\", \"age\", \"component\").orderBy(\"component\", \"id\").show()\n",
    "\n",
    "# Example: Calculate Strongly Connected Components (SCC)\n",
    "# print(\"\\nCalculating Strongly Connected Components (SCC)...\")\n",
    "# scc_results = g.stronglyConnectedComponents(maxIter=10, checkpointInterval=2)\n",
    "# print(\"\\nStrongly Connected Components:\")\n",
    "# scc_results.select(\"id\", \"name\", \"age\", \"component\").orderBy(\"component\", \"id\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "# spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Call `connectedComponents()`:** Invokes the WCC algorithm. The `checkpointInterval=2` parameter tells Spark to save the state every 2 iterations, which helps with fault tolerance and potentially reduces lineage complexity.\n",
    "2.  **Access Results:** The method returns a DataFrame (`wcc_results`) containing all original vertex attributes plus the `component` ID assigned to each vertex.\n",
    "3.  **Display Components:** Show the resulting DataFrame, typically ordered by `component` to group vertices belonging to the same cluster. Note that the specific component ID assigned is arbitrary but consistent within a component (often the minimum vertex ID in that component).\n",
    "4.  **Strongly Connected Components (Optional):** The commented-out section shows the call to `stronglyConnectedComponents()`. Note that SCC often results in more, smaller components than WCC because of the directionality requirement. For our example graph, SCC might break down components found by WCC.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   Community detection in social networks.\n",
    "*   Network segmentation (finding isolated parts of a network).\n",
    "*   Data cleaning (identifying potentially disconnected entities that should be linked).\n",
    "*   Understanding network resilience.\n",
    "\n",
    "#### 3.3 Breadth-First Search (BFS)\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Breadth-First Search (BFS) is an algorithm for traversing or searching a graph. Starting from one or more source nodes, it explores the immediate neighbors first, then their neighbors, and so on, layer by layer. It's often used to find the shortest path (in terms of number of edges or \"hops\") between nodes.\n",
    "\n",
    "**PySpark Implementation:** `graphframe.bfs()`\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "*   `fromExpr` (str): Spark SQL expression specifying the starting vertices for the search.\n",
    "*   `toExpr` (str): Spark SQL expression specifying the target vertices. The search stops when it reaches any vertex satisfying this condition.\n",
    "*   `maxPathLength` (int): Maximum number of edges allowed in the path.\n",
    "*   `edgeFilter` (str): Optional Spark SQL expression to filter which edges can be traversed.\n",
    "\n",
    "**Output:** Returns a DataFrame representing the shortest paths found from any `fromExpr` vertex to any `toExpr` vertex, within the `maxPathLength`. The DataFrame schema typically includes columns like `from` (start vertex), `e0` (first edge), `v1` (first intermediate vertex), `e1` (second edge), ..., `to` (end vertex).\n",
    "\n",
    "**Code Example: Breadth-First Search (BFS)**\n",
    "\n",
    "```python\n",
    "# Assuming 'g' GraphFrame is available\n",
    "\n",
    "print(\"\\nPerforming Breadth-First Search (BFS)...\")\n",
    "\n",
    "# Find shortest paths from 'Alice' (a) to 'Fanny' (f)\n",
    "paths_a_to_f = g.bfs(\n",
    "    fromExpr = \"id = 'a'\",\n",
    "    toExpr = \"id = 'f'\",\n",
    "    maxPathLength = 4\n",
    ")\n",
    "\n",
    "print(\"\\nShortest paths from Alice (a) to Fanny (f):\")\n",
    "if paths_a_to_f.count() > 0:\n",
    "    paths_a_to_f.show(truncate=False)\n",
    "else:\n",
    "    print(\"No path found within maxPathLength.\")\n",
    "\n",
    "\n",
    "# Find shortest paths from anyone aged < 35 to anyone aged > 35\n",
    "paths_young_to_old = g.bfs(\n",
    "    fromExpr = \"age < 35\",\n",
    "    toExpr = \"age > 35\",\n",
    "    maxPathLength = 3,\n",
    "    edgeFilter = \"relationship = 'friend'\" # Only traverse 'friend' edges\n",
    ")\n",
    "\n",
    "print(\"\\nShortest paths (<=3 hops, friends only) from age < 35 to age > 35:\")\n",
    "if paths_young_to_old.count() > 0:\n",
    "    paths_young_to_old.show(truncate=False)\n",
    "else:\n",
    "    print(\"No such path found.\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop() # Clean up the session at the end\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Call `bfs()` (Example 1):**\n",
    "    *   `fromExpr = \"id = 'a'\"`: Starts the search from the vertex where the `id` is 'a' (Alice).\n",
    "    *   `toExpr = \"id = 'f'\"`: Stops the search if it reaches the vertex where `id` is 'f' (Fanny).\n",
    "    *   `maxPathLength = 4`: Limits the search depth to 4 hops.\n",
    "2.  **Call `bfs()` (Example 2):**\n",
    "    *   `fromExpr = \"age < 35\"`: Starts the search from *any* vertex matching this condition.\n",
    "    *   `toExpr = \"age > 35\"`: Stops when *any* vertex matching this condition is reached.\n",
    "    *   `maxPathLength = 3`: Limits path length.\n",
    "    *   `edgeFilter = \"relationship = 'friend'\"`: Specifies that only edges where the `relationship` attribute is 'friend' should be considered during traversal.\n",
    "3.  **Access Results:** The method returns a DataFrame (`paths_a_to_f`, `paths_young_to_old`). Each row represents a path found. The columns depend on the path length (e.g., `from`, `e0`, `v1`, `e1`, `to` for a path of length 2). The vertex/edge columns contain the full vertex/edge data as StructTypes.\n",
    "4.  **Display Paths:** Show the resulting DataFrame. `truncate=False` is useful to see the full content of the struct columns. Check `.count()` first to avoid errors if no path exists.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   Finding shortest paths in networks (e.g., degrees of separation in social networks, routing).\n",
    "*   Reachability analysis (can node A reach node B?).\n",
    "*   Web crawling (discovering pages layer by layer).\n",
    "*   Dependency analysis (finding dependencies in software projects or task workflows).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Application: Building Recommendation Systems\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "GraphFrames can be applied to build recommendation systems, particularly collaborative filtering approaches. The core idea is to represent users, items, and their interactions as a graph.\n",
    "\n",
    "*   **Nodes:** Users and Items.\n",
    "*   **Edges:** Interactions like 'viewed', 'purchased', 'rated'. Edge properties can include ratings or timestamps.\n",
    "\n",
    "Recommendations can be generated using various graph techniques:\n",
    "\n",
    "1.  **Motif Finding:** Find patterns like \"Users who bought item X also bought item Y\".\n",
    "    *   Motif: `(user1)-[buy1]->(itemX)<-[buy2]-(user2)` finds pairs of users who bought the same item.\n",
    "    *   Motif: `(user)-[buy1]->(itemX); (user)-[buy2]->(itemY)` finds items frequently bought together by the same user.\n",
    "2.  **Path Finding (BFS):** Find items liked/purchased by \"friends of friends\" or users with similar purchasing history.\n",
    "    *   Start BFS from a target user, traverse user-item edges, then item-user edges to find similar users, then user-item edges again to find items those similar users liked.\n",
    "3.  **PageRank/Personalized PageRank:** Run PageRank on the user-item graph. High PageRank items might be globally popular. Personalized PageRank starting from a user can rank items highly relevant to that specific user based on their connections and the connections of items they interacted with.\n",
    "4.  **Connected Components:** Identify communities of users with similar tastes.\n",
    "\n",
    "**Code Example: Recommendation based on Common Purchases (Motif Finding)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"GraphFramesRecSys\").getOrCreate()\n",
    "\n",
    "# Sample Data: Users, Items, Purchases\n",
    "users_data = [(\"u1\", \"Alice\"), (\"u2\", \"Bob\"), (\"u3\", \"Charlie\"), (\"u4\", \"David\")]\n",
    "items_data = [(\"i1\", \"Laptop\"), (\"i2\", \"Mouse\"), (\"i3\", \"Keyboard\"), (\"i4\", \"Monitor\")]\n",
    "purchases_data = [\n",
    "    (\"u1\", \"i1\"), (\"u1\", \"i2\"), # Alice bought Laptop, Mouse\n",
    "    (\"u2\", \"i1\"), (\"u2\", \"i3\"), # Bob bought Laptop, Keyboard\n",
    "    (\"u3\", \"i2\"), (\"u3\", \"i3\"), # Charlie bought Mouse, Keyboard\n",
    "    (\"u4\", \"i4\"),               # David bought Monitor\n",
    "    (\"u1\", \"i3\")  # Alice also bought Keyboard\n",
    "]\n",
    "\n",
    "# Create Vertices (Users and Items combined)\n",
    "# Add a 'type' column to distinguish users from items\n",
    "vertices = spark.createDataFrame([(id, name, \"user\") for id, name in users_data] + \\\n",
    "                                 [(id, name, \"item\") for id, name in items_data],\n",
    "                                [\"id\", \"name\", \"type\"])\n",
    "\n",
    "# Create Edges (Purchases)\n",
    "edges = spark.createDataFrame(purchases_data, [\"src\", \"dst\"]) \\\n",
    "            .withColumn(\"interaction\", col(\"src\")) # Add interaction column just for demo if needed\n",
    "\n",
    "# Create GraphFrame\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "print(\"User-Item Graph Vertices:\")\n",
    "graph.vertices.show()\n",
    "print(\"\\nUser-Item Graph Edges (Purchases):\")\n",
    "graph.edges.show()\n",
    "\n",
    "# --- Recommendation Logic using Motifs ---\n",
    "\n",
    "# Find pairs of items frequently purchased together by the same user\n",
    "# Motif: (user)-[buy1]->(item1); (user)-[buy2]->(item2)\n",
    "item_pairs_motif = graph.find(\"(u)-[e1]->(i1); (u)-[e2]->(i2)\") \\\n",
    "    .filter(col(\"i1.id\") < col(\"i2.id\")) # Avoid duplicates (i1,i2) vs (i2,i1) and self-pairs\n",
    "\n",
    "print(\"\\nItem pairs purchased together:\")\n",
    "item_pairs_motif.select(\"i1.name\", \"i2.name\", \"u.name\").show()\n",
    "\n",
    "# Aggregate to find most frequent co-purchased pairs\n",
    "co_purchased_counts = item_pairs_motif.groupBy(\"i1.id\", \"i1.name\", \"i2.id\", \"i2.name\") \\\n",
    "    .agg(count(\"*\").alias(\"frequency\")) \\\n",
    "    .orderBy(desc(\"frequency\"))\n",
    "\n",
    "print(\"\\nFrequently Co-Purchased Item Pairs:\")\n",
    "co_purchased_counts.show()\n",
    "\n",
    "# --- Generate Recommendations for a specific user (e.g., Alice 'u1') ---\n",
    "\n",
    "# 1. Find items Alice already purchased\n",
    "alice_items = graph.edges.filter(col(\"src\") == \"u1\").select(col(\"dst\").alias(\"id\"))\n",
    "print(\"\\nItems Alice purchased:\")\n",
    "alice_items.join(graph.vertices.filter(col(\"type\") == \"item\"), \"id\").select(\"id\", \"name\").show()\n",
    "alice_item_ids = set(row.id for row in alice_items.collect())\n",
    "\n",
    "# 2. Use the co-purchased pairs to recommend items\n",
    "# Find pairs where one item was bought by Alice, recommend the other\n",
    "recommendations = co_purchased_counts \\\n",
    "    .filter((col(\"i1.id\").isin(alice_item_ids)) | (col(\"i2.id\").isin(alice_item_ids)))\n",
    "\n",
    "# Filter out items Alice already owns and format recommendations\n",
    "final_recs = recommendations.rdd \\\n",
    "    .flatMap(lambda row: [(row['i1.id'], row['i1.name'], row.frequency) if row['i2.id'] in alice_item_ids else \\\n",
    "                           (row['i2.id'], row['i2.name'], row.frequency)]) \\\n",
    "    .toDF([\"rec_id\", \"rec_name\", \"strength\"]) \\\n",
    "    .filter(~col(\"rec_id\").isin(alice_item_ids)) \\\n",
    "    .groupBy(\"rec_id\", \"rec_name\") \\\n",
    "    .agg({\"strength\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(strength)\", \"recommendation_score\") \\\n",
    "    .orderBy(desc(\"recommendation_score\"))\n",
    "\n",
    "\n",
    "print(\"\\nRecommendations for Alice (u1) based on co-purchases:\")\n",
    "if final_recs.count() > 0:\n",
    "    final_recs.show()\n",
    "else:\n",
    "    print(\"No recommendations found based on this logic.\")\n",
    "\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Prepare Data:** Define sample user, item, and purchase data.\n",
    "2.  **Create Vertices:** Combine users and items into a single DataFrame. Add a `type` column to differentiate them. Ensure unique IDs across users and items (e.g., prefix item IDs if necessary in real data).\n",
    "3.  **Create Edges:** Create the edge DataFrame representing purchases (`src`=user ID, `dst`=item ID).\n",
    "4.  **Create GraphFrame:** Build the `GraphFrame`.\n",
    "5.  **Find Co-purchased Items:**\n",
    "    *   Use `graph.find(\"(u)-[e1]->(i1); (u)-[e2]->(i2)\")` to find users who bought two items (`i1`, `i2`).\n",
    "    *   Filter `col(\"i1.id\") < col(\"i2.id\")` to avoid duplicates and self-pairs.\n",
    "    *   Group by the item pair (`i1.id`, `i2.id`) and count the frequency using `agg(count(\"*\").alias(\"frequency\"))`.\n",
    "    *   Order by frequency to find the most commonly co-purchased pairs.\n",
    "6.  **Generate Recommendations for Alice ('u1'):**\n",
    "    *   Identify items Alice already bought (`alice_item_ids`).\n",
    "    *   Filter the `co_purchased_counts` DataFrame to include only pairs where at least one item was bought by Alice.\n",
    "    *   Use `flatMap` (via RDD conversion) to extract the *other* item from each pair as a potential recommendation, along with the frequency (strength).\n",
    "    *   Convert back to a DataFrame (`final_recs`).\n",
    "    *   Filter out items Alice already owns using the `alice_item_ids` set.\n",
    "    *   Group by the recommended item ID and name, summing the 'strength' (in case an item appears in multiple pairs) to get a final `recommendation_score`.\n",
    "    *   Order by score to rank recommendations.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*   \"Users who bought this also bought...\" recommendations.\n",
    "*   \"People you may know\" based on common connections.\n",
    "*   Content recommendation based on user navigation paths or topic graphs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Advanced Concepts and Considerations\n",
    "\n",
    "#### 5.1 Performance Tuning and Optimization\n",
    "\n",
    "Graph algorithms, especially iterative ones like PageRank and Connected Components, can be computationally intensive. Since GraphFrames builds on DataFrames, standard Spark optimization techniques apply, plus some graph-specific ones:\n",
    "\n",
    "1.  **Checkpointing:**\n",
    "    *   **Why:** Iterative algorithms build up a long RDD/DataFrame lineage. If a task fails mid-computation, Spark might have to recompute a large portion of the work. Checkpointing saves the state of a DataFrame to reliable storage (like HDFS or S3), truncating the lineage.\n",
    "    *   **How:**\n",
    "        *   Set a checkpoint directory: `spark.sparkContext.setCheckpointDir(\"hdfs:///path/to/checkpoints\")`. Use a reliable distributed file system path.\n",
    "        *   Use the `checkpointInterval` parameter in iterative algorithms (`connectedComponents`, `stronglyConnectedComponents`, potentially within custom iterative loops). For algorithms like `pageRank` that don't expose it directly, you might manually checkpoint the GraphFrame's vertices/edges periodically if running a custom iterative process.\n",
    "    *   **Trade-off:** Checkpointing has I/O overhead. Setting the interval too low (e.g., 1) can slow down computation due to frequent writes. A value between 5-10 is often a reasonable starting point.\n",
    "\n",
    "2.  **Caching / Persistence:**\n",
    "    *   **Why:** If you reuse the base `vertices` or `edges` DataFrames, or intermediate results (like the output of one algorithm feeding into another), caching them in memory can significantly speed up subsequent operations.\n",
    "    *   **How:** Use `.cache()` (shortcut for `persist(StorageLevel.MEMORY_AND_DISK)`) or `.persist(StorageLevel.LEVEL)` on the DataFrames or GraphFrame's components (`g.vertices.persist()`, `g.edges.persist()`).\n",
    "    *   **Storage Levels:** Choose appropriate storage levels (e.g., `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `DISK_ONLY`) based on available memory and resilience requirements. `MEMORY_AND_DISK` is often a safe default. Remember to `.unpersist()` when done to free up memory.\n",
    "\n",
    "3.  **Data Partitioning:**\n",
    "    *   **Why:** Graph operations often involve joins between vertices and edges, or shuffling data based on vertex IDs. Proper partitioning of the underlying `vertices` and `edges` DataFrames can minimize data shuffling across the network.\n",
    "    *   **How:** While GraphFrames doesn't offer explicit graph partitioning algorithms like GraphX's `partitionBy`, you can influence partitioning at the DataFrame level *before* creating the GraphFrame or between operations:\n",
    "        *   `df.repartition(N)`: Can help distribute data more evenly if skewed, but involves a full shuffle.\n",
    "        *   `df.repartition(col(\"id\"))` or `df.repartition(col(\"src\"), col(\"dst\"))`: Partition based on relevant ID columns. This might help co-locate related vertices and edges, although Spark's join optimizations often handle this.\n",
    "        *   `df.coalesce(N)`: Reduces the number of partitions without a full shuffle (efficient for decreasing parallelism).\n",
    "    *   **Note:** Optimal partitioning is data-dependent and requires experimentation. Default Spark partitioning might be sufficient for moderately sized graphs.\n",
    "\n",
    "4.  **Motif Finding Optimization:** Complex motifs can lead to intricate query plans. Check the Spark UI's SQL tab to understand the execution plan. Sometimes, breaking down a very complex motif into smaller finds and joining the results can be more efficient.\n",
    "\n",
    "5.  **Resource Allocation:** Ensure your Spark cluster has sufficient memory (executor memory, driver memory) and CPU cores allocated. Graph processing can be memory-intensive. Monitor Garbage Collection (GC) time in the Spark UI.\n",
    "\n",
    "#### 5.2 GraphFrames vs. GraphX\n",
    "\n",
    "| Feature           | GraphFrames                             | GraphX (Scala)                           |\n",
    "| :---------------- | :-------------------------------------- | :--------------------------------------- |\n",
    "| **Primary API**   | DataFrame-based                         | RDD-based                                |\n",
    "| **Languages**     | Python, Scala, Java, SQL                | Primarily Scala (Java limited, no Python) |\n",
    "| **Integration**   | High (Spark SQL, MLlib DataFrames)      | Lower (RDD focus)                       |\n",
    "| **Querying**      | Motif Finding (declarative)             | Pregel API, Graph Ops (programmatic)     |\n",
    "| **Optimization**  | Catalyst, Tungsten                      | RDD optimizations, Graph-specific opts   |\n",
    "| **Performance**   | Generally good, benefits from DataFrame engine | Potentially faster for highly iterative, low-level graph ops (e.g., Pregel) |\n",
    "| **Ease of Use**   | High for DataFrame users                | Steeper learning curve                   |\n",
    "| **Graph Mutations**| Immutable (create new graph)            | Immutable (create new graph)            |\n",
    "| **Partitioning**  | Implicit via DataFrames                 | Explicit Graph Partitioning Strategies   |\n",
    "\n",
    "**When to Choose:**\n",
    "\n",
    "*   **GraphFrames:** If you primarily work in Python or SQL, need seamless integration with other DataFrame operations, prefer declarative queries (motifs), and your performance needs are met by the DataFrame engine. This is the recommended choice for most PySpark users.\n",
    "*   **GraphX:** If you work primarily in Scala, need fine-grained control over graph algorithms (e.g., implementing custom Pregel iterations), require specific graph partitioning strategies, or find GraphFrames performance insufficient for highly specialized, iterative graph computations.\n",
    "\n",
    "#### 5.3 Integration with Spark MLlib\n",
    "\n",
    "Features derived from graph analysis using GraphFrames are readily available as columns in DataFrames. These can be directly incorporated into Spark MLlib pipelines:\n",
    "\n",
    "*   **Node Features:** PageRank scores, component IDs, node degrees can be used as input features for node classification or regression models.\n",
    "*   **Edge Features:** Edge weights or properties derived from BFS/motifs can be used in link prediction models.\n",
    "*   **Graph Embeddings:** While GraphFrames doesn't have built-in embedding algorithms (like Node2Vec), you could potentially implement sampling strategies (like random walks using BFS or motifs) and feed the results into MLlib's Word2Vec or other embedding techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Conclusion\n",
    "\n",
    "GraphFrames provides a powerful and user-friendly interface for scalable graph processing within the PySpark ecosystem. By leveraging the familiar DataFrame API and Spark SQL, it allows users to seamlessly combine graph analysis with structured data processing. Key strengths include its expressive motif finding capabilities and implementations of standard graph algorithms like PageRank, Connected Components, and BFS. Understanding the underlying DataFrame representation and applying performance tuning techniques like checkpointing and caching are crucial for handling large-scale graphs effectively. GraphFrames is an essential tool for uncovering insights hidden within relational data across various domains, from social networks to recommendation systems and beyond.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
