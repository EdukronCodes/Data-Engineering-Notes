# Prasad - Data Engineer Resume

## Header
**Prasad**  
üìû +91-98765-43210 | ‚úâÔ∏è prasad.email@example.com | üåç Bangalore, India | üîó linkedin.com/in/prasad-dataengineer | üíª github.com/prasad-data

## Career Objective / Summary
Dynamic Data Engineer with proven experience in Azure cloud technologies and ETL pipeline development. Demonstrated expertise in building robust data solutions using Azure Data Factory, Azure Databricks, and Power BI. Seeking challenging opportunities to leverage technical skills in building enterprise-scale data platforms while contributing to data-driven business transformations.

## Key Responsibilities
1. Designed and developed ETL pipelines using Azure Data Factory for data ingestion from multiple sources
2. Built data processing workflows using Azure Databricks with PySpark for data transformation and cleansing
3. Implemented data storage solutions using ADLS Gen2 with proper partitioning and optimization strategies
4. Created dimensional data models using star schema for business intelligence and reporting purposes
5. Developed automated data quality checks and validation processes for ensuring data accuracy
6. Built interactive dashboards and reports using Power BI for business stakeholders and executives
7. Optimized SQL queries and data processing performance to improve system efficiency
8. Collaborated with business analysts and stakeholders to understand data requirements and deliver solutions
9. Maintained and monitored data pipelines ensuring high availability and reliability
10. Documented data processes, workflows, and technical specifications for knowledge sharing

## Technical Skills

| Category | Skills/Tools |
|----------|--------------|
| Azure Services | Azure Data Factory, Azure Databricks, ADLS Gen2, Azure SQL Database |
| Data Processing | PySpark, Spark SQL, Delta Lake |
| Data Modeling | Star Schema, SCD Type 1 & 2, Dimensional Modeling |
| Programming | Python, SQL |
| CI/CD & Tools | Azure DevOps, GitHub |
| Visualization | Power BI, Power Query |

## Professional Experience

**Junior Data Engineer | TechCorp Solutions | 6 months | Bangalore**

**Data Engineering Intern | CloudData Systems | 6 months | Bangalore**

## Projects

### Project 1: Retail Sales Analytics Pipeline

**Objective:** To develop an automated data pipeline for processing retail sales data and generating daily analytics reports for business stakeholders.

**Skills:** Azure Data Factory, Azure Databricks (PySpark), ADLS Gen2, SQL, Power BI

**Responsibilities:**
- Built ETL pipelines in Azure Data Factory for daily sales data ingestion from CSV and SQL sources
- Implemented basic data validation and cleansing using PySpark transformations in Databricks
- Created bronze and silver layers for raw and processed data storage in ADLS Gen2
- Developed SQL queries for aggregating sales metrics and KPIs
- Designed Power BI dashboard to visualize daily sales trends and product performance

### Project 2: Customer Data Management System

**Objective:** To create a centralized system for managing customer information with data quality checks and basic analytics capabilities.

**Skills:** Azure Data Factory, Azure SQL Database, Python (Pandas), Power BI

**Responsibilities:**
- Designed data ingestion pipelines for customer data from multiple sources using ADF
- Implemented data deduplication and validation logic using Python and SQL
- Created customer master data tables with proper normalization in Azure SQL Database
- Built automated data quality monitoring with basic alerting mechanisms
- Developed customer segmentation dashboard in Power BI for marketing insights

### Project 3: Banking Transaction Processing Pipeline

**Objective:** To build a secure data pipeline for processing banking transaction data with compliance and monitoring features.

**Skills:** Azure Data Factory, Databricks, Azure SQL, Python, Event Hubs (Basic)

**Responsibilities:**
- Developed secure data ingestion pipelines for transaction data using Azure Data Factory
- Implemented data encryption and masking for sensitive financial information
- Created automated reconciliation processes for transaction validation
- Built monitoring dashboards for transaction volume and processing times
- Designed basic fraud detection alerts using transaction pattern analysis
