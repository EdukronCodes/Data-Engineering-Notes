# Sales Data Consolidation and Analysis Platform

As a **Data Engineer**, I was responsible for designing and implementing a **Sales Data Consolidation and Analysis Platform**, enabling seamless migration from **on-premise SQL Server** to **Azure Data Lake Storage Gen2 (ADLS Gen2)** using the **Medallion Architecture (Bronze, Silver, Gold layers)**. The objective was to build a scalable data pipeline that could process, transform, and analyze sales data across multiple regions in real-time. To achieve this, I developed **Azure Data Factory (ADF) pipelines** to extract and ingest raw transactional data, including **customer orders, inventory levels, and revenue transactions**, into the **Bronze layer** of ADLS Gen2. This ensured that all historical and real-time sales data were available for further processing while maintaining **data lineage and traceability**.

Once the raw data was ingested, I implemented **PySpark jobs in Azure Databricks** to process and clean the data within the **Silver layer**. This involved **removing duplicate transactions, handling missing values, standardizing date formats, and applying currency conversions** for global sales data. Additionally, I integrated **external data sources** to enrich missing customer location details and product information, improving data completeness. To enhance performance, I optimized **data partitioning strategies**, significantly reducing query execution time and enabling **faster data retrieval for analytics and reporting**.

In the **Gold layer**, I focused on **aggregating and structuring the data** for advanced analytics and business intelligence. I integrated **Power BI dashboards**, enabling real-time insights into **regional sales performance, top-selling products, and customer purchasing patterns**. To ensure data reliability, I developed **automated data quality checks and implemented monitoring alerts** for pipeline failures. This project not only streamlined the **end-to-end data consolidation process** but also empowered stakeholders with **real-time sales analysis and forecasting capabilities**, allowing the business to make **data-driven decisions with confidence**. By leveraging **Azure Data Factory, Azure Databricks, and PySpark**, I successfully built a **scalable and automated data pipeline**, transforming raw transactional data into meaningful business insights.


# Real-Time Customer Interaction Tracking System

As a **Data Engineer**, I worked on designing and implementing a **Real-Time Customer Interaction Tracking System** for a telecom company. The goal was to capture, process, and analyze real-time customer interactions to improve customer experience and optimize service offerings. The system ingested **customer call logs, chat interactions, network usage patterns, and service requests** from multiple sources and processed them in real time. To achieve this, I developed **Azure Data Factory (ADF) pipelines** to extract and stream customer interaction data from various sources into **Azure Data Lake Storage Gen2 (ADLS Gen2)**, storing raw, unstructured data in the **Bronze layer** for historical tracking and auditing.

In the **Silver layer**, I implemented **PySpark jobs in Azure Databricks** to clean and enrich the raw data, ensuring consistency and reliability. This included **removing duplicate records, normalizing timestamps, tagging customer sentiment from chat logs using NLP, and performing session-based aggregations**. Additionally, I used **Python scripts** to perform **data enrichment** by integrating external datasets such as customer demographics, billing information, and historical service issues. This enriched dataset was then structured and optimized for further analysis, reducing data redundancy and improving query performance.

For the **Gold layer**, I designed aggregated datasets that allowed **real-time monitoring and reporting** of customer interactions. I integrated **Power BI dashboards** with the Gold layer to provide **business stakeholders with actionable insights into customer engagement, service performance, and churn prediction trends**. The dashboards displayed **customer satisfaction scores, response time analytics, and service performance metrics**, enabling the telecom company to enhance customer retention strategies. To ensure **low-latency data processing**, I conducted **performance tuning of PySpark jobs**, optimizing **data partitioning and indexing strategies**. This project successfully automated the **real-time tracking of customer interactions**, empowering the business to **deliver personalized customer support, reduce response times, and improve overall service quality**.
