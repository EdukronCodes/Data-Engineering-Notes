# üìÑ Resume ‚Äì Santhosh | Data Engineer (3+ YOE)

**Santhosh Reddy**
üìû Phone: +91-98765-43211 | ‚úâÔ∏è Email: santhosh.reddy@email.com
üåê LinkedIn: linkedin.com/in/santhosh-reddy-data | üìç Location: Hyderabad, Telangana

## Professional Summary

Data Engineer with 3+ years of experience in designing and implementing scalable data solutions using modern big data technologies. Expertise in building ETL/ELT pipelines, data warehousing, and real-time data processing. Proficient in Python, SQL, and cloud platforms with strong focus on data quality, performance optimization, and automation.

## Technical Skills

**Big Data Technologies:** Apache Spark, PySpark, Azure Event Hubs, Azure Data Factory, Azure HDInsight, Azure Stream Analytics

**Cloud Platforms:** Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Data Lake (Gen2), Azure SQL Database

**Databases:** Azure SQL Database, Azure Cosmos DB, Azure Database for PostgreSQL, Azure Database for MySQL, Azure Cache for Redis

**Programming:** Python, SQL, Java, Scala

**Data Processing:** ETL/ELT, Data Streaming, Batch Processing, Real-time Analytics

**Tools & Frameworks:** Docker, Azure Container Instances, Azure Kubernetes Service, Git, Azure DevOps, Azure Resource Manager

**Visualization:** Power BI, Azure Analysis Services, Azure Monitor

## Professional Experience

### Senior Data Engineer
**[Company Name] ‚Äì [Location]**
üìÖ Mar 2022 ‚Äì Present

- Architected and implemented end-to-end data pipelines processing 2TB+ daily data volume from multiple retail sources.
- Built real-time streaming pipelines using Kafka and Spark Streaming for inventory management and customer behavior tracking.
- Designed and optimized data warehouse schemas (Star/Snowflake) for retail analytics and reporting.
- Implemented data quality frameworks and monitoring systems reducing data issues by 60%.
- Developed automated data validation and reconciliation processes for financial and inventory data.
- Collaborated with business stakeholders to define data requirements and deliver actionable insights.

### Data Engineer
**[Previous Company Name] ‚Äì [Location]**
üìÖ Jun 2020 ‚Äì Feb 2022

- Built ETL pipelines using Python and Apache Airflow for processing e-commerce transaction data.
- Implemented data lake architecture on Azure Data Lake Gen2 with automated data partitioning and compression.
- Created data models and views for business intelligence and reporting requirements.
- Optimized SQL queries and Spark jobs resulting in 50% performance improvement.

## Education

üéì **Master of Technology (M.Tech) ‚Äì Data Science**
[University Name], [Year of Graduation]

## Projects (Retail-Focused)

### Omnichannel Inventory Management System
- **Description:** Developed a comprehensive real-time inventory tracking system that synchronizes stock levels across all retail channels including online stores, mobile apps, physical stores, and warehouses. The system ensures accurate inventory visibility and prevents overselling while optimizing stock allocation based on demand patterns.
- **Technology Stack:** Kafka, Spark Streaming, PostgreSQL, Redis, Airflow
- **Key Achievements:**
  - Processed 100K+ inventory updates per minute
  - Reduced stock discrepancies by 75%
  - Implemented automated reorder point calculations

### Customer Behavior Analytics Platform
- **Description:** Built advanced customer analytics platform that tracks and analyzes customer interactions across all touchpoints including website visits, mobile app usage, in-store purchases, and customer service interactions. The platform provides deep insights into customer journey patterns and preferences for targeted marketing strategies.
- **Technology Stack:** Python, Spark, Elasticsearch, Grafana
- **Key Achievements:**
  - Tracked customer interactions across 50+ touchpoints
  - Built predictive models for customer lifetime value
  - Created real-time dashboards for marketing team

### Supply Chain Data Integration Hub
- **Description:** Created a centralized data integration platform that consolidates information from multiple supplier systems, logistics partners, and internal databases. The system provides real-time visibility into supply chain operations and enables data-driven decision making for procurement and inventory management.
- **Technology Stack:** Azure Data Factory, Azure Synapse Analytics, Azure Functions, Azure Data Lake Gen2
- **Key Achievements:**
  - Automated data ingestion from various file formats (CSV, JSON, XML)
  - Implemented data quality checks and anomaly detection
  - Reduced data processing time from 8 hours to 2 hours

### Dynamic Pricing Data Pipeline
- **Description:** Developed a real-time pricing intelligence system that monitors competitor prices, market trends, and internal factors to enable dynamic pricing strategies. The system processes pricing data from multiple sources and provides recommendations for optimal pricing decisions to maximize revenue and market competitiveness.
- **Technology Stack:** Python, Kafka, ClickHouse, Airflow
- **Key Achievements:**
  - Processed competitor pricing data from 100+ sources
  - Implemented real-time price change notifications
  - Achieved 99.9% data accuracy through automated validation
