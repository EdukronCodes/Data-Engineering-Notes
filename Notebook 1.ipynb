{"cells":[{"cell_type":"code","metadata":{"cell_id":"a19553c5f6ab42fcaf3e63c9694117c2","deepnote_cell_type":"code"},"source":"","block_group":"a19553c5f6ab42fcaf3e63c9694117c2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"color":"purple","cell_id":"984df5f358ca41b9b0acb305cc4aa97d","deepnote_cell_type":"text-cell-callout"},"source":"> RAK Bank Data Engineering project , it is migrating on premise sql server and other sources to azure cloud write some theory about the project and objectives and business impact","block_group":"41dd29db8483409db090079d142ec99e"},{"cell_type":"code","metadata":{"source_hash":"6439ac8e","execution_start":1726634686767,"execution_millis":98,"deepnote_to_be_reexecuted":false,"cell_id":"1d9efedf34d247ac9fb51bfbd3d61ab1","deepnote_cell_type":"code"},"source":"project_theory = \"\"\"\nThe RAK Bank Data Engineering project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. This migration is a strategic initiative aimed at leveraging the benefits of cloud computing to enhance the bank's data management capabilities.\n\nObjectives:\n1. Scalability: By moving to Azure, RAK Bank can easily scale its data infrastructure up or down based on demand, ensuring optimal performance without the need for significant upfront investments in hardware.\n2. Cost Efficiency: Cloud platforms like Azure offer a pay-as-you-go model, which can lead to cost savings compared to maintaining on-premise infrastructure.\n3. Enhanced Security: Azure provides advanced security features and compliance certifications, ensuring that the bank's data is protected against threats and meets regulatory requirements.\n4. Improved Data Accessibility: Migrating to the cloud allows for better data accessibility and collaboration, enabling employees to access data from anywhere, at any time.\n5. Disaster Recovery: Azure offers robust disaster recovery solutions, ensuring business continuity in case of data loss or system failures.\n\nBusiness Impact:\n1. Operational Efficiency: The migration will streamline data management processes, reduce downtime, and improve overall operational efficiency.\n2. Competitive Advantage: By leveraging advanced analytics and AI capabilities available on Azure, RAK Bank can gain insights that drive better decision-making and offer innovative services to customers.\n3. Customer Experience: Improved data accessibility and analytics can lead to enhanced customer experiences through personalized services and faster response times.\n4. Cost Savings: Reduced infrastructure and maintenance costs can lead to significant savings, which can be reinvested into other strategic initiatives.\n5. Compliance and Security: Enhanced security and compliance features ensure that the bank meets regulatory requirements, reducing the risk of fines and reputational damage.\n\nOverall, the migration to Azure is expected to transform RAK Bank's data infrastructure, providing a solid foundation for future growth and innovation.\n\"\"\"\n\nproject_theory","block_group":"cc2fd02364a44c0cbfe88539e8393cf6","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"\"\\nThe RAK Bank Data Engineering project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. This migration is a strategic initiative aimed at leveraging the benefits of cloud computing to enhance the bank's data management capabilities.\\n\\nObjectives:\\n1. Scalability: By moving to Azure, RAK Bank can easily scale its data infrastructure up or down based on demand, ensuring optimal performance without the need for significant upfront investments in hardware.\\n2. Cost Efficiency: Cloud platforms like Azure offer a pay-as-you-go model, which can lead to cost savings compared to maintaining on-premise infrastructure.\\n3. Enhanced Security: Azure provides advanced security features and compliance certifications, ensuring that the bank's data is protected against threats and meets regulatory requirements.\\n4. Improved Data Accessibility: Migrating to the cloud allows for better data accessibility and collaboration, enabling employees to access data from anywhere, at any time.\\n5. Disaster Recovery: Azure offers robust disaster recovery solutions, ensuring business continuity in case of data loss or system failures.\\n\\nBusiness Impact:\\n1. Operational Efficiency: The migration will streamline data management processes, reduce downtime, and improve overall operational efficiency.\\n2. Competitive Advantage: By leveraging advanced analytics and AI capabilities available on Azure, RAK Bank can gain insights that drive better decision-making and offer innovative services to customers.\\n3. Customer Experience: Improved data accessibility and analytics can lead to enhanced customer experiences through personalized services and faster response times.\\n4. Cost Savings: Reduced infrastructure and maintenance costs can lead to significant savings, which can be reinvested into other strategic initiatives.\\n5. Compliance and Security: Enhanced security and compliance features ensure that the bank meets regulatory requirements, reducing the risk of fines and reputational damage.\\n\\nOverall, the migration to Azure is expected to transform RAK Bank's data infrastructure, providing a solid foundation for future growth and innovation.\\n\""},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/a9ad8115-5345-4004-8d82-90c2c6b9c1af","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"95f8f0e5f5dc4c4387f510e60f0b3648","deepnote_cell_type":"text-cell-p"},"source":"The RAK Bank Data Engineering project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. This migration is a strategic initiative aimed at leveraging the benefits of cloud computing to enhance the bank's data management capabilities.\n\nObjectives:\n1. Scalability: By moving to Azure, RAK Bank can easily scale its data infrastructure up or down based on demand, ensuring optimal performance without the need for significant upfront investments in hardware.\n2. Cost Efficiency: Cloud platforms like Azure offer a pay-as-you-go model, which can lead to cost savings compared to maintaining on-premise infrastructure.\n3. Enhanced Security: Azure provides advanced security features and compliance certifications, ensuring that the bank's data is protected against threats and meets regulatory requirements.\n4. Improved Data Accessibility: Migrating to the cloud allows for better data accessibility and collaboration, enabling employees to access data from anywhere, at any time.\n5. Disaster Recovery: Azure offers robust disaster recovery solutions, ensuring business continuity in case of data loss or system failures.\n\nBusiness Impact:\n1. Operational Efficiency: The migration will streamline data management processes, reduce downtime, and improve overall operational efficiency.\n2. Competitive Advantage: By leveraging advanced analytics and AI capabilities available on Azure, RAK Bank can gain insights that drive better decision-making and offer innovative services to customers.\n3. Customer Experience: Improved data accessibility and analytics can lead to enhanced customer experiences through personalized services and faster response times.\n4. Cost Savings: Reduced infrastructure and maintenance costs can lead to significant savings, which can be reinvested into other strategic initiatives.\n5. Compliance and Security: Enhanced security and compliance features ensure that the bank meets regulatory requirements, reducing the risk of fines and reputational damage.\n\nOverall, the migration to Azure is expected to transform RAK Bank's data infrastructure, providing a solid foundation for future growth and innovation.","block_group":"6a62c0485f3447619a7d7b83742a35da"},{"cell_type":"markdown","metadata":{"color":"purple","cell_id":"880a916d814d49a0acfc72d08c612a46","deepnote_cell_type":"text-cell-callout"},"source":"> write azure adf pipeline indetailed for this and expand each section with examples add medallian architecutre and make it more detailed and add transformation as well \n# RAK Bank Azure Data Migration Project - Schema Diagram\n\n## High-Level Schema\n\nThe RAK Bank Azure Data Migration project contains the following key entities:\n\n1. **Customers**\n2. **Accounts**\n3. **Transactions**\n4. **Loans**\n5. **Cards**\n6. **Branches**\n7. **Employees**\n8. **Audit**\n\nThese entities are interconnected through various relationships such as foreign keys (FKs) to represent the logical flow of data within the bank's system.\n\n---\n\n### Customers Table\n\n| Column Name          | Data Type      | Description                                    |\n|----------------------|----------------|------------------------------------------------|\n| `customer_id`         | INT (PK)       | Unique identifier for the customer             |\n| `first_name`          | VARCHAR(100)   | First name of the customer                     |\n| `last_name`           | VARCHAR(100)   | Last name of the customer                      |\n| `email`               | VARCHAR(255)   | Email address of the customer                  |\n| `phone_number`        | VARCHAR(20)    | Phone number of the customer                   |\n| `date_of_birth`       | DATE           | Customer's date of birth                       |\n| `address`             | VARCHAR(255)   | Address of the customer                        |\n| `city`                | VARCHAR(100)   | City of the customer                           |\n| `country`             | VARCHAR(100)   | Country of the customer                        |\n| `national_id`         | VARCHAR(50)    | National ID number                             |\n| `customer_since`      | DATE           | The date the customer joined                   |\n| `customer_status`     | VARCHAR(50)    | Active/Inactive status                         |\n\n---\n\n### Accounts Table\n\n| Column Name          | Data Type      | Description                                    |\n|----------------------|----------------|------------------------------------------------|\n| `account_id`          | INT (PK)       | Unique identifier for the account              |\n| `customer_id`         | INT (FK)       | Reference to the customer                      |\n| `account_type`        | VARCHAR(50)    | Type of the account (e.g., Savings, Current)   |\n| `balance`             | DECIMAL(15,2)  | Current balance of the account                 |\n| `currency`            | VARCHAR(10)    | Currency of the account (e.g., AED, USD)       |\n| `branch_id`           | INT (FK)       | Branch where the account is held               |\n| `account_status`      | VARCHAR(50)    | Active/Inactive status                         |\n| `date_opened`         | DATE           | Date the account was opened                    |\n\n---\n\n### Transactions Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `transaction_id`       | INT (PK)       | Unique identifier for the transaction          |\n| `account_id`           | INT (FK)       | Reference to the account                       |\n| `transaction_type`     | VARCHAR(50)    | Type of transaction (e.g., Credit, Debit)      |\n| `amount`               | DECIMAL(15,2)  | Transaction amount                             |\n| `transaction_date`     | DATE           | Date of the transaction                        |\n| `description`          | VARCHAR(255)   | Transaction description                        |\n| `balance_after_txn`    | DECIMAL(15,2)  | Account balance after the transaction          |\n| `branch_id`            | INT (FK)       | Branch where the transaction occurred          |\n\n---\n\n### Loans Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `loan_id`              | INT (PK)       | Unique identifier for the loan                 |\n| `customer_id`          | INT (FK)       | Reference to the customer                      |\n| `loan_type`            | VARCHAR(50)    | Type of loan (e.g., Personal, Home, Auto)      |\n| `loan_amount`          | DECIMAL(15,2)  | Total loan amount                              |\n| `interest_rate`        | DECIMAL(5,2)   | Interest rate on the loan                      |\n| `loan_status`          | VARCHAR(50)    | Loan status (e.g., Approved, Pending, Closed)  |\n| `loan_start_date`      | DATE           | Date the loan was issued                       |\n| `loan_end_date`        | DATE           | Date the loan is to be repaid                  |\n\n---\n\n### Cards Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `card_id`              | INT (PK)       | Unique identifier for the card                 |\n| `customer_id`          | INT (FK)       | Reference to the customer                      |\n| `card_type`            | VARCHAR(50)    | Type of card (e.g., Credit, Debit)             |\n| `card_number`          | VARCHAR(16)    | Card number (masked for security)              |\n| `card_status`          | VARCHAR(50)    | Card status (e.g., Active, Blocked)            |\n| `expiry_date`          | DATE           | Expiry date of the card                        |\n| `issue_date`           | DATE           | Date the card was issued                       |\n| `credit_limit`         | DECIMAL(15,2)  | Credit limit (if applicable)                   |\n\n---\n\n### Branches Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `branch_id`            | INT (PK)       | Unique identifier for the branch               |\n| `branch_name`          | VARCHAR(100)   | Name of the branch                             |\n| `branch_code`          | VARCHAR(10)    | Branch code for transactions                   |\n| `city`                 | VARCHAR(100)   | City where the branch is located               |\n| `country`              | VARCHAR(100)   | Country where the branch is located            |\n| `phone_number`         | VARCHAR(20)    | Contact number of the branch                   |\n\n---\n\n### Employees Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `employee_id`          | INT (PK)       | Unique identifier for the employee             |\n| `first_name`           | VARCHAR(100)   | Employee's first name                          |\n| `last_name`            | VARCHAR(100)   | Employee's last name                           |\n| `position`             | VARCHAR(50)    | Job title of the employee                      |\n| `branch_id`            | INT (FK)       | Branch where the employee works                |\n| `email`                | VARCHAR(255)   | Contact email of the employee                  |\n| `phone_number`         | VARCHAR(20)    | Employee's contact number                      |\n| `hire_date`            | DATE           | Date the employee was hired                    |\n| `salary`               | DECIMAL(10,2)  | Employee's salary                              |\n\n---\n\n### Audit Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `audit_id`             | INT (PK)       | Unique identifier for the audit                |\n| `table_name`           | VARCHAR(100)   | Name of the table being audited                |\n| `operation_type`       | VARCHAR(50)    | Type of operation (Insert, Update, Delete)     |\n| `performed_by`         | VARCHAR(100)   | Name or ID of the user who performed the operation |\n| `operation_time`       | TIMESTAMP      | Time the operation was performed               |\n| `details`              | JSON           | Detailed log of changes                        |\n\n---\n\n## Extended Tables for 200 Columns\n\nTo complete the schema with 200 columns, the following additional tables can be included:\n\n### Customer Contacts Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `contact_id`           | INT (PK)       | Unique identifier for the contact              |\n| `customer_id`          | INT (FK)       | Reference to the customer                      |\n| `emergency_contact_name` | VARCHAR(100) | Name of the emergency contact                  |\n| `emergency_contact_number` | VARCHAR(20) | Emergency contact number                      |\n\n---\n\n### Loan Payments Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `payment_id`           | INT (PK)       | Unique identifier for the payment              |\n| `loan_id`              | INT (FK)       | Reference to the loan                          |\n| `payment_amount`       | DECIMAL(15,2)  | Amount of the loan payment                     |\n| `payment_date`         | DATE           | Date of the payment                            |\n| `payment_status`       | VARCHAR(50)    | Status of the payment (e.g., Completed, Pending) |\n\n---\n\n### Transaction Fees Table\n\n| Column Name           | Data Type      | Description                                    |\n|-----------------------|----------------|------------------------------------------------|\n| `fee_id`               | INT (PK)       | Unique identifier for the fee                  |\n| `transaction_id`       | INT (FK)       | Reference to the transaction                   |\n| `fee_amount`           | DECIMAL(15,2)  | Fee amount for the transaction                 |\n| `fee_type`             | VARCHAR(50)    | Type of fee (e.g., Service Fee, Penalty)       |\n| `description`          | VARCHAR(255)   | Description of the fee                         |\n\n---\n this is the full schema","block_group":"f91cb6ee583542438c5d42f2cdda4789"},{"cell_type":"markdown","metadata":{"cell_id":"c021b50afc6f44d591e7b846003b2226","deepnote_cell_type":"text-cell-p"},"source":"# RAK Bank Azure Data Migration Project - Azure Data Factory (ADF) Pipeline\n\n## Overview\n\nThe RAK Bank Azure Data Migration project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. The migration process will be managed using Azure Data Factory (ADF), which will orchestrate the data movement and transformation tasks. The pipeline will follow the Medallion Architecture, which consists of three layers: Bronze, Silver, and Gold.\n\n### Medallion Architecture\n\n1. **Bronze Layer**: Raw data ingestion from various sources.\n2. **Silver Layer**: Cleaned and enriched data.\n3. **Gold Layer**: Aggregated and business-ready data.\n\n## Azure Data Factory Pipeline\n\n### 1. Bronze Layer - Raw Data Ingestion\n\n#### Data Sources\n\n- On-premise SQL Server\n- Other data sources (e.g., CSV files, APIs)\n\n#### Activities\n\n1. **Copy Data Activity**: Ingest raw data from on-premise SQL Server to Azure Data Lake Storage (ADLS) Gen2.\n2. **Copy Data Activity**: Ingest raw data from other sources (e.g., CSV files) to ADLS Gen2.\n\n#### Example\n\n```json\n{\n  \"name\": \"CopyDataFromSQLServerToADLS\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"name\": \"SQLServerDataset\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"ADLSRawDataset\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"SqlSource\",\n      \"sqlReaderQuery\": \"SELECT * FROM Customers\"\n    },\n    \"sink\": {\n      \"type\": \"AzureBlobFSink\",\n      \"blobPath\": \"bronze/customers/\"\n    }\n  }\n}\n```\n\n### 2. Silver Layer - Data Cleaning and Enrichment\n\n#### Activities\n\n1. **Data Flow Activity**: Clean and transform raw data.\n2. **Mapping Data Flow**: Define transformations (e.g., removing duplicates, handling null values).\n\n#### Example\n\n```json\n{\n  \"name\": \"CleanAndTransformData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"CleanCustomersDataFlow\"\n    }\n  }\n}\n```\n\n### 3. Gold Layer - Aggregated and Business-Ready Data\n\n#### Activities\n\n1. **Data Flow Activity**: Aggregate and prepare data for business use.\n2. **Mapping Data Flow**: Define business logic and aggregations.\n\n#### Example\n\n```json\n{\n  \"name\": \"AggregateAndPrepareData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"AggregateCustomersDataFlow\"\n    }\n  }\n}\n```\n\n## Detailed Pipeline Steps\n\n### Step 1: Ingest Raw Data (Bronze Layer)\n\n1. **Create Linked Services**: Define connections to on-premise SQL Server and ADLS Gen2.\n2. **Create Datasets**: Define datasets for source and destination.\n3. **Copy Data Activity**: Ingest data from SQL Server to ADLS Gen2.\n\n### Step 2: Clean and Enrich Data (Silver Layer)\n\n1. **Create Data Flow**: Define transformations to clean and enrich data.\n2. **Data Flow Activity**: Execute the data flow to transform data.\n\n### Step 3: Aggregate and Prepare Data (Gold Layer)\n\n1. **Create Data Flow**: Define business logic and aggregations.\n2. **Data Flow Activity**: Execute the data flow to prepare business-ready data.\n\n## Example Transformations\n\n### Removing Duplicates\n\n```json\n{\n  \"name\": \"RemoveDuplicates\",\n  \"type\": \"AlterRow\",\n  \"typeProperties\": {\n    \"condition\": \"unique(customer_id)\"\n  }\n}\n```\n\n### Handling Null Values\n\n```json\n{\n  \"name\": \"HandleNullValues\",\n  \"type\": \"DerivedColumn\",\n  \"typeProperties\": {\n    \"expressions\": {\n      \"email\": \"iif(isNull(email), 'unknown@example.com', email)\"\n    }\n  }\n}\n```\n\n### Aggregating Data\n\n```json\n{\n  \"name\": \"AggregateData\",\n  \"type\": \"Aggregate\",\n  \"typeProperties\": {\n    \"groupBy\": [\n      \"customer_id\"\n    ],\n    \"aggregates\": {\n      \"total_balance\": \"sum(balance)\"\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Azure Data Factory pipeline for the RAK Bank Azure Data Migration project follows the Medallion Architecture, ensuring a structured and efficient data migration process. The pipeline includes raw data ingestion, data cleaning and enrichment, and aggregation to prepare business-ready data. This approach ensures data quality, scalability, and accessibility, providing a solid foundation for future growth and innovation.","block_group":"5d5f7155ef5049279bed9997f0e52d30"},{"cell_type":"markdown","metadata":{"color":"purple","cell_id":"bf8b3a17e8044d3c9bde1ba607bd7f2a","deepnote_cell_type":"text-cell-callout"},"source":"# RAK Bank Azure Data Migration Project - Azure Data Factory (ADF) Pipeline\n\n## Overview\n\nThe RAK Bank Azure Data Migration project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. The migration process will be managed using Azure Data Factory (ADF), which will orchestrate the data movement and transformation tasks. The pipeline will follow the Medallion Architecture, which consists of three layers: Bronze, Silver, and Gold.\n\n### Medallion Architecture\n\n1. **Bronze Layer**: Raw data ingestion from various sources.\n2. **Silver Layer**: Cleaned and enriched data.\n3. **Gold Layer**: Aggregated and business-ready data.\n\n## Azure Data Factory Pipeline\n\n### 1. Bronze Layer - Raw Data Ingestion\n\n#### Data Sources\n\n- On-premise SQL Server\n- Other data sources (e.g., CSV files, APIs)\n\n#### Activities\n\n1. **Copy Data Activity**: Ingest raw data from on-premise SQL Server to Azure Data Lake Storage (ADLS) Gen2.\n2. **Copy Data Activity**: Ingest raw data from other sources (e.g., CSV files) to ADLS Gen2.\n\n#### Example\n\n```json\n{\n  \"name\": \"CopyDataFromSQLServerToADLS\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"name\": \"SQLServerDataset\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"ADLSRawDataset\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"SqlSource\",\n      \"sqlReaderQuery\": \"SELECT * FROM Customers\"\n    },\n    \"sink\": {\n      \"type\": \"AzureBlobFSink\",\n      \"blobPath\": \"bronze/customers/\"\n    }\n  }\n}\n```\n\n### 2. Silver Layer - Data Cleaning and Enrichment\n\n#### Activities\n\n1. **Data Flow Activity**: Clean and transform raw data.\n2. **Mapping Data Flow**: Define transformations (e.g., removing duplicates, handling null values).\n\n#### Example\n\n```json\n{\n  \"name\": \"CleanAndTransformData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"CleanCustomersDataFlow\"\n    }\n  }\n}\n```\n\n### 3. Gold Layer - Aggregated and Business-Ready Data\n\n#### Activities\n\n1. **Data Flow Activity**: Aggregate and prepare data for business use.\n2. **Mapping Data Flow**: Define business logic and aggregations.\n\n#### Example\n\n```json\n{\n  \"name\": \"AggregateAndPrepareData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"AggregateCustomersDataFlow\"\n    }\n  }\n}\n```\n\n## Detailed Pipeline Steps\n\n### Step 1: Ingest Raw Data (Bronze Layer)\n\n1. **Create Linked Services**: Define connections to on-premise SQL Server and ADLS Gen2.\n2. **Create Datasets**: Define datasets for source and destination.\n3. **Copy Data Activity**: Ingest data from SQL Server to ADLS Gen2.\n\n### Step 2: Clean and Enrich Data (Silver Layer)\n\n1. **Create Data Flow**: Define transformations to clean and enrich data.\n2. **Data Flow Activity**: Execute the data flow to transform data.\n\n### Step 3: Aggregate and Prepare Data (Gold Layer)\n\n1. **Create Data Flow**: Define business logic and aggregations.\n2. **Data Flow Activity**: Execute the data flow to prepare business-ready data.\n\n## Example Transformations\n\n### Removing Duplicates\n\n```json\n{\n  \"name\": \"RemoveDuplicates\",\n  \"type\": \"AlterRow\",\n  \"typeProperties\": {\n    \"condition\": \"unique(customer_id)\"\n  }\n}\n```\n\n### Handling Null Values\n\n```json\n{\n  \"name\": \"HandleNullValues\",\n  \"type\": \"DerivedColumn\",\n  \"typeProperties\": {\n    \"expressions\": {\n      \"email\": \"iif(isNull(email), 'unknown@example.com', email)\"\n    }\n  }\n}\n```\n\n### Aggregating Data\n\n```json\n{\n  \"name\": \"AggregateData\",\n  \"type\": \"Aggregate\",\n  \"typeProperties\": {\n    \"groupBy\": [\n      \"customer_id\"\n    ],\n    \"aggregates\": {\n      \"total_balance\": \"sum(balance)\"\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Azure Data Factory pipeline for the RAK Bank Azure Data Migration project follows the Medallion Architecture, ensuring a structured and efficient data migration process. The pipeline includes raw data ingestion, data cleaning and enrichment, and aggregation to prepare business-ready data. This approach ensures data quality, scalability, and accessibility, providing a solid foundation for future growth and innovation. write this in markd docw code","block_group":"fcd5f7a567dd46a9bb6ccc927d17719e"},{"cell_type":"code","metadata":{"source_hash":"b28ae24d","execution_start":1726634813949,"execution_millis":195,"deepnote_to_be_reexecuted":false,"cell_id":"94d79a6282f24429b074d4f07b8439bb","deepnote_cell_type":"code"},"source":"from markdown import markdown\n\n# The provided text\ntext = \"\"\"\n# RAK Bank Azure Data Migration Project - Azure Data Factory (ADF) Pipeline\n\n## Overview\n\nThe RAK Bank Azure Data Migration project involves migrating the bank's on-premise SQL Server and other data sources to the Azure cloud platform. The migration process will be managed using Azure Data Factory (ADF), which will orchestrate the data movement and transformation tasks. The pipeline will follow the Medallion Architecture, which consists of three layers: Bronze, Silver, and Gold.\n\n### Medallion Architecture\n\n1. **Bronze Layer**: Raw data ingestion from various sources.\n2. **Silver Layer**: Cleaned and enriched data.\n3. **Gold Layer**: Aggregated and business-ready data.\n\n## Azure Data Factory Pipeline\n\n### 1. Bronze Layer - Raw Data Ingestion\n\n#### Data Sources\n\n- On-premise SQL Server\n- Other data sources (e.g., CSV files, APIs)\n\n#### Activities\n\n1. **Copy Data Activity**: Ingest raw data from on-premise SQL Server to Azure Data Lake Storage (ADLS) Gen2.\n2. **Copy Data Activity**: Ingest raw data from other sources (e.g., CSV files) to ADLS Gen2.\n\n#### Example\n\n```json\n{\n  \"name\": \"CopyDataFromSQLServerToADLS\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"name\": \"SQLServerDataset\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"ADLSRawDataset\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"SqlSource\",\n      \"sqlReaderQuery\": \"SELECT * FROM Customers\"\n    },\n    \"sink\": {\n      \"type\": \"AzureBlobFSink\",\n      \"blobPath\": \"bronze/customers/\"\n    }\n  }\n}\n```\n\n### 2. Silver Layer - Data Cleaning and Enrichment\n\n#### Activities\n\n1. **Data Flow Activity**: Clean and transform raw data.\n2. **Mapping Data Flow**: Define transformations (e.g., removing duplicates, handling null values).\n\n#### Example\n\n```json\n{\n  \"name\": \"CleanAndTransformData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"CleanCustomersDataFlow\"\n    }\n  }\n}\n```\n\n### 3. Gold Layer - Aggregated and Business-Ready Data\n\n#### Activities\n\n1. **Data Flow Activity**: Aggregate and prepare data for business use.\n2. **Mapping Data Flow**: Define business logic and aggregations.\n\n#### Example\n\n```json\n{\n  \"name\": \"AggregateAndPrepareData\",\n  \"type\": \"DataFlow\",\n  \"typeProperties\": {\n    \"dataFlow\": {\n      \"name\": \"AggregateCustomersDataFlow\"\n    }\n  }\n}\n```\n\n## Detailed Pipeline Steps\n\n### Step 1: Ingest Raw Data (Bronze Layer)\n\n1. **Create Linked Services**: Define connections to on-premise SQL Server and ADLS Gen2.\n2. **Create Datasets**: Define datasets for source and destination.\n3. **Copy Data Activity**: Ingest data from SQL Server to ADLS Gen2.\n\n### Step 2: Clean and Enrich Data (Silver Layer)\n\n1. **Create Data Flow**: Define transformations to clean and enrich data.\n2. **Data Flow Activity**: Execute the data flow to transform data.\n\n### Step 3: Aggregate and Prepare Data (Gold Layer)\n\n1. **Create Data Flow**: Define business logic and aggregations.\n2. **Data Flow Activity**: Execute the data flow to prepare business-ready data.\n\n## Example Transformations\n\n### Removing Duplicates\n\n```json\n{\n  \"name\": \"RemoveDuplicates\",\n  \"type\": \"AlterRow\",\n  \"typeProperties\": {\n    \"condition\": \"unique(customer_id)\"\n  }\n}\n```\n\n### Handling Null Values\n\n```json\n{\n  \"name\": \"HandleNullValues\",\n  \"type\": \"DerivedColumn\",\n  \"typeProperties\": {\n    \"expressions\": {\n      \"email\": \"iif(isNull(email), 'unknown@example.com', email)\"\n    }\n  }\n}\n```\n\n### Aggregating Data\n\n```json\n{\n  \"name\": \"AggregateData\",\n  \"type\": \"Aggregate\",\n  \"typeProperties\": {\n    \"groupBy\": [\n      \"customer_id\"\n    ],\n    \"aggregates\": {\n      \"total_balance\": \"sum(balance)\"\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Azure Data Factory pipeline for the RAK Bank Azure Data Migration project follows the Medallion Architecture, ensuring a structured and efficient data migration process. The pipeline includes raw data ingestion, data cleaning and enrichment, and aggregation to prepare business-ready data. This approach ensures data quality, scalability, and accessibility, providing a solid foundation for future growth and innovation.\n\"\"\"\n\n# Convert the text to markdown\nmarkdown_text = markdown(text)\n\n# Save the markdown text to a file\nwith open(\"RAK_Bank_Azure_Data_Migration_Project.md\", \"w\") as file:\n    file.write(markdown_text)\n\nmarkdown_text","block_group":"d21bbe8feabb4ff8a94dd1d9bc5821d6","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'<h1>RAK Bank Azure Data Migration Project - Azure Data Factory (ADF) Pipeline</h1>\\n<h2>Overview</h2>\\n<p>The RAK Bank Azure Data Migration project involves migrating the bank\\'s on-premise SQL Server and other data sources to the Azure cloud platform. The migration process will be managed using Azure Data Factory (ADF), which will orchestrate the data movement and transformation tasks. The pipeline will follow the Medallion Architecture, which consists of three layers: Bronze, Silver, and Gold.</p>\\n<h3>Medallion Architecture</h3>\\n<ol>\\n<li><strong>Bronze Layer</strong>: Raw data ingestion from various sources.</li>\\n<li><strong>Silver Layer</strong>: Cleaned and enriched data.</li>\\n<li><strong>Gold Layer</strong>: Aggregated and business-ready data.</li>\\n</ol>\\n<h2>Azure Data Factory Pipeline</h2>\\n<h3>1. Bronze Layer - Raw Data Ingestion</h3>\\n<h4>Data Sources</h4>\\n<ul>\\n<li>On-premise SQL Server</li>\\n<li>Other data sources (e.g., CSV files, APIs)</li>\\n</ul>\\n<h4>Activities</h4>\\n<ol>\\n<li><strong>Copy Data Activity</strong>: Ingest raw data from on-premise SQL Server to Azure Data Lake Storage (ADLS) Gen2.</li>\\n<li><strong>Copy Data Activity</strong>: Ingest raw data from other sources (e.g., CSV files) to ADLS Gen2.</li>\\n</ol>\\n<h4>Example</h4>\\n<p><code>json\\n{\\n  \"name\": \"CopyDataFromSQLServerToADLS\",\\n  \"type\": \"Copy\",\\n  \"inputs\": [\\n    {\\n      \"name\": \"SQLServerDataset\"\\n    }\\n  ],\\n  \"outputs\": [\\n    {\\n      \"name\": \"ADLSRawDataset\"\\n    }\\n  ],\\n  \"typeProperties\": {\\n    \"source\": {\\n      \"type\": \"SqlSource\",\\n      \"sqlReaderQuery\": \"SELECT * FROM Customers\"\\n    },\\n    \"sink\": {\\n      \"type\": \"AzureBlobFSink\",\\n      \"blobPath\": \"bronze/customers/\"\\n    }\\n  }\\n}</code></p>\\n<h3>2. Silver Layer - Data Cleaning and Enrichment</h3>\\n<h4>Activities</h4>\\n<ol>\\n<li><strong>Data Flow Activity</strong>: Clean and transform raw data.</li>\\n<li><strong>Mapping Data Flow</strong>: Define transformations (e.g., removing duplicates, handling null values).</li>\\n</ol>\\n<h4>Example</h4>\\n<p><code>json\\n{\\n  \"name\": \"CleanAndTransformData\",\\n  \"type\": \"DataFlow\",\\n  \"typeProperties\": {\\n    \"dataFlow\": {\\n      \"name\": \"CleanCustomersDataFlow\"\\n    }\\n  }\\n}</code></p>\\n<h3>3. Gold Layer - Aggregated and Business-Ready Data</h3>\\n<h4>Activities</h4>\\n<ol>\\n<li><strong>Data Flow Activity</strong>: Aggregate and prepare data for business use.</li>\\n<li><strong>Mapping Data Flow</strong>: Define business logic and aggregations.</li>\\n</ol>\\n<h4>Example</h4>\\n<p><code>json\\n{\\n  \"name\": \"AggregateAndPrepareData\",\\n  \"type\": \"DataFlow\",\\n  \"typeProperties\": {\\n    \"dataFlow\": {\\n      \"name\": \"AggregateCustomersDataFlow\"\\n    }\\n  }\\n}</code></p>\\n<h2>Detailed Pipeline Steps</h2>\\n<h3>Step 1: Ingest Raw Data (Bronze Layer)</h3>\\n<ol>\\n<li><strong>Create Linked Services</strong>: Define connections to on-premise SQL Server and ADLS Gen2.</li>\\n<li><strong>Create Datasets</strong>: Define datasets for source and destination.</li>\\n<li><strong>Copy Data Activity</strong>: Ingest data from SQL Server to ADLS Gen2.</li>\\n</ol>\\n<h3>Step 2: Clean and Enrich Data (Silver Layer)</h3>\\n<ol>\\n<li><strong>Create Data Flow</strong>: Define transformations to clean and enrich data.</li>\\n<li><strong>Data Flow Activity</strong>: Execute the data flow to transform data.</li>\\n</ol>\\n<h3>Step 3: Aggregate and Prepare Data (Gold Layer)</h3>\\n<ol>\\n<li><strong>Create Data Flow</strong>: Define business logic and aggregations.</li>\\n<li><strong>Data Flow Activity</strong>: Execute the data flow to prepare business-ready data.</li>\\n</ol>\\n<h2>Example Transformations</h2>\\n<h3>Removing Duplicates</h3>\\n<p><code>json\\n{\\n  \"name\": \"RemoveDuplicates\",\\n  \"type\": \"AlterRow\",\\n  \"typeProperties\": {\\n    \"condition\": \"unique(customer_id)\"\\n  }\\n}</code></p>\\n<h3>Handling Null Values</h3>\\n<p><code>json\\n{\\n  \"name\": \"HandleNullValues\",\\n  \"type\": \"DerivedColumn\",\\n  \"typeProperties\": {\\n    \"expressions\": {\\n      \"email\": \"iif(isNull(email), \\'unknown@example.com\\', email)\"\\n    }\\n  }\\n}</code></p>\\n<h3>Aggregating Data</h3>\\n<p><code>json\\n{\\n  \"name\": \"AggregateData\",\\n  \"type\": \"Aggregate\",\\n  \"typeProperties\": {\\n    \"groupBy\": [\\n      \"customer_id\"\\n    ],\\n    \"aggregates\": {\\n      \"total_balance\": \"sum(balance)\"\\n    }\\n  }\\n}</code></p>\\n<h2>Conclusion</h2>\\n<p>The Azure Data Factory pipeline for the RAK Bank Azure Data Migration project follows the Medallion Architecture, ensuring a structured and efficient data migration process. The pipeline includes raw data ingestion, data cleaning and enrichment, and aggregation to prepare business-ready data. This approach ensures data quality, scalability, and accessibility, providing a solid foundation for future growth and innovation.</p>'"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/a9a0e30f-1f11-4649-ba14-c57f3ece8e11","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"d2bd7528ffb745ea80974b1b552bd398","deepnote_cell_type":"text-cell-p"},"source":"The markdown content has been successfully converted and saved to the file `RAK_Bank_Azure_Data_Migration_Project.md`.","block_group":"da1a0659969545bcaa32a82246c6e96b"},{"cell_type":"markdown","metadata":{"color":"purple","cell_id":"f6427278b9f24457a84b39d4e62ed19f","deepnote_cell_type":"text-cell-callout"},"source":"> write bottlnecks adn issues as this large data and write some optimization techniues","block_group":"161e929f11b145a7a7c15971df1aa061"},{"cell_type":"code","metadata":{"source_hash":"432bc6eb","execution_start":1726634871504,"execution_millis":182,"deepnote_to_be_reexecuted":false,"cell_id":"1dc128796a3543918abdee010e40dcac","deepnote_cell_type":"code"},"source":"bottlenecks_and_optimizations = \"\"\"\n# Bottlenecks and Issues in Large Data Migration\n\n## Bottlenecks and Issues\n\n1. **Data Volume**: Migrating large volumes of data can lead to extended transfer times and increased costs.\n2. **Network Bandwidth**: Limited network bandwidth can slow down data transfer rates, causing delays in the migration process.\n3. **Data Quality**: Inconsistent or poor-quality data can lead to errors during migration and require additional cleaning and validation steps.\n4. **Schema Changes**: Differences in data schemas between the source and destination can complicate the migration process.\n5. **Downtime**: Ensuring minimal downtime during migration is critical to avoid disruptions to business operations.\n6. **Security and Compliance**: Ensuring data security and compliance with regulatory requirements during migration can be challenging.\n7. **Resource Management**: Efficiently managing cloud resources to avoid over-provisioning or under-provisioning can be difficult.\n\n## Optimization Techniques\n\n1. **Incremental Data Transfer**: Instead of migrating all data at once, use incremental data transfer to move data in smaller batches. This reduces the load on the network and minimizes downtime.\n2. **Data Compression**: Compress data before transfer to reduce the volume of data being moved, which can speed up the migration process and reduce costs.\n3. **Parallel Processing**: Use parallel processing to perform data migration tasks concurrently, which can significantly reduce the overall migration time.\n4. **Data Partitioning**: Partition large datasets into smaller chunks to enable more efficient data transfer and processing.\n5. **Pre-Migration Data Cleaning**: Clean and validate data before migration to reduce errors and ensure data quality in the destination system.\n6. **Schema Mapping and Transformation**: Use schema mapping and transformation tools to handle differences in data schemas between the source and destination.\n7. **Network Optimization**: Optimize network settings and use dedicated network connections (e.g., Azure ExpressRoute) to improve data transfer rates.\n8. **Resource Scaling**: Use auto-scaling features in the cloud to dynamically adjust resources based on the workload, ensuring optimal performance without over-provisioning.\n9. **Monitoring and Logging**: Implement monitoring and logging to track the progress of the migration, identify bottlenecks, and troubleshoot issues in real-time.\n10. **Security Measures**: Use encryption and secure transfer protocols to protect data during migration and ensure compliance with regulatory requirements.\n\"\"\"\n\nwith open(\"RAK_Bank_Azure_Data_Migration_Project_Bottlenecks_and_Optimizations.md\", \"w\") as file:\n    file.write(bottlenecks_and_optimizations)\n\nbottlenecks_and_optimizations","block_group":"a8b981dbdc1248308af2ea2e32be752e","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'\\n# Bottlenecks and Issues in Large Data Migration\\n\\n## Bottlenecks and Issues\\n\\n1. **Data Volume**: Migrating large volumes of data can lead to extended transfer times and increased costs.\\n2. **Network Bandwidth**: Limited network bandwidth can slow down data transfer rates, causing delays in the migration process.\\n3. **Data Quality**: Inconsistent or poor-quality data can lead to errors during migration and require additional cleaning and validation steps.\\n4. **Schema Changes**: Differences in data schemas between the source and destination can complicate the migration process.\\n5. **Downtime**: Ensuring minimal downtime during migration is critical to avoid disruptions to business operations.\\n6. **Security and Compliance**: Ensuring data security and compliance with regulatory requirements during migration can be challenging.\\n7. **Resource Management**: Efficiently managing cloud resources to avoid over-provisioning or under-provisioning can be difficult.\\n\\n## Optimization Techniques\\n\\n1. **Incremental Data Transfer**: Instead of migrating all data at once, use incremental data transfer to move data in smaller batches. This reduces the load on the network and minimizes downtime.\\n2. **Data Compression**: Compress data before transfer to reduce the volume of data being moved, which can speed up the migration process and reduce costs.\\n3. **Parallel Processing**: Use parallel processing to perform data migration tasks concurrently, which can significantly reduce the overall migration time.\\n4. **Data Partitioning**: Partition large datasets into smaller chunks to enable more efficient data transfer and processing.\\n5. **Pre-Migration Data Cleaning**: Clean and validate data before migration to reduce errors and ensure data quality in the destination system.\\n6. **Schema Mapping and Transformation**: Use schema mapping and transformation tools to handle differences in data schemas between the source and destination.\\n7. **Network Optimization**: Optimize network settings and use dedicated network connections (e.g., Azure ExpressRoute) to improve data transfer rates.\\n8. **Resource Scaling**: Use auto-scaling features in the cloud to dynamically adjust resources based on the workload, ensuring optimal performance without over-provisioning.\\n9. **Monitoring and Logging**: Implement monitoring and logging to track the progress of the migration, identify bottlenecks, and troubleshoot issues in real-time.\\n10. **Security Measures**: Use encryption and secure transfer protocols to protect data during migration and ensure compliance with regulatory requirements.\\n'"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/1cd1282f-a915-4b6a-8378-1d8b9822876e","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"500e8f5f9ae64b67973e1b9b6e63437c","deepnote_cell_type":"text-cell-p"},"source":"The bottlenecks and optimization techniques for large data migration have been successfully written and saved to the file `RAK_Bank_Azure_Data_Migration_Project_Bottlenecks_and_Optimizations.md`.","block_group":"59660d4e70d94f648a39b653e4674a9b"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d25df7c2-1e2c-4932-9d50-2fa328125b7b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"a890bbf92dcf4725ac6436164c1cf992","deepnote_execution_queue":[]}}